[
    {
        "_id": "64897102d97326f51535995a",
        "username": "psf",
        "repository": "black",
        "issues": [
            {
                "id": 2254,
                "created_at": "2021-05-24T17:46:11Z",
                "closed_at": "2021-06-08T21:37:37Z",
                "title": "fmt:skip fails with internal error",
                "labels": "T: bug, C: crash, F: fmtskip",
                "commits": [
                    {
                        "hash": "40fae18134916b8499bd992d8bef4ae23bcd2986",
                        "commit_date": "2021-06-08T21:37:34Z",
                        "parents": "c1c2418368cfcaa4f49edd7ec599fa45cce2d47d",
                        "stat": {
                            "total": 1,
                            "additions": 20,
                            "deletions": 19,
                            "files": [
                                {
                                    "sha": "2d2b3b4cf49303d1c33cf1f8e09b35525327034f",
                                    "filename": "CHANGES.md",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/40fae18134916b8499bd992d8bef4ae23bcd2986/CHANGES.md",
                                    "raw_url": "https://github.com/psf/black/raw/40fae18134916b8499bd992d8bef4ae23bcd2986/CHANGES.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/CHANGES.md?ref=40fae18134916b8499bd992d8bef4ae23bcd2986",
                                    "patch": "@@ -4,6 +4,7 @@\n \n ### _Black_\n \n+- Fix failure caused by `fmt: skip` and indentation (#2281)\n - Account for += assignment when deciding whether to split string (#2312)\n - Correct max string length calculation when there are string operators (#2292)\n - Fixed option usage when using the `--code` flag (#2259)"
                                },
                                {
                                    "sha": "c7513c21ef5693cb5e5800eecbbc94a3fab88e7c",
                                    "filename": "src/black/comments.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 1,
                                    "changes": 5,
                                    "blob_url": "https://github.com/psf/black/blob/40fae18134916b8499bd992d8bef4ae23bcd2986/src%2Fblack%2Fcomments.py",
                                    "raw_url": "https://github.com/psf/black/raw/40fae18134916b8499bd992d8bef4ae23bcd2986/src%2Fblack%2Fcomments.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2Fcomments.py?ref=40fae18134916b8499bd992d8bef4ae23bcd2986",
                                    "patch": "@@ -159,7 +159,10 @@ def convert_one_fmt_off_pair(node: Node) -> bool:\n             first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n             parent = first.parent\n             prefix = first.prefix\n-            first.prefix = prefix[comment.consumed :]\n+            if comment.value in FMT_OFF:\n+                first.prefix = prefix[comment.consumed :]\n+            if comment.value in FMT_SKIP:\n+                first.prefix = \"\"\n             hidden_value = \"\".join(str(n) for n in ignored_nodes)\n             if comment.value in FMT_OFF:\n                 hidden_value = comment.value + \"\\n\" + hidden_value"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "0a779fcee00f470b69eec84c52ee1f7073fb9eb1",
                                    "filename": "tests/data/fmtskip6.py",
                                    "status": "added",
                                    "additions": 13,
                                    "deletions": 0,
                                    "changes": 13,
                                    "blob_url": "https://github.com/psf/black/blob/40fae18134916b8499bd992d8bef4ae23bcd2986/tests%2Fdata%2Ffmtskip6.py",
                                    "raw_url": "https://github.com/psf/black/raw/40fae18134916b8499bd992d8bef4ae23bcd2986/tests%2Fdata%2Ffmtskip6.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtskip6.py?ref=40fae18134916b8499bd992d8bef4ae23bcd2986",
                                    "patch": "@@ -0,0 +1,13 @@\n+class A:\n+    def f(self):\n+        for line in range(10):\n+            if True:\n+                pass  # fmt: skip\n+\n+# output\n+\n+class A:\n+    def f(self):\n+        for line in range(10):\n+            if True:\n+                pass  # fmt: skip"
                                },
                                {
                                    "sha": "fc9678ad27cda3c0e6791f54c0bcf6858c24e5c6",
                                    "filename": "tests/test_format.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/40fae18134916b8499bd992d8bef4ae23bcd2986/tests%2Ftest_format.py",
                                    "raw_url": "https://github.com/psf/black/raw/40fae18134916b8499bd992d8bef4ae23bcd2986/tests%2Ftest_format.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_format.py?ref=40fae18134916b8499bd992d8bef4ae23bcd2986",
                                    "patch": "@@ -41,6 +41,7 @@\n     \"fmtskip3\",\n     \"fmtskip4\",\n     \"fmtskip5\",\n+    \"fmtskip6\",\n     \"fstring\",\n     \"function\",\n     \"function2\","
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testStepsFull": "pipenv run pytest",
                "testSteps": "pipenv run pytest -- tests/test_format.py"
            },
            {
                "id": 1812,
                "created_at": "2020-11-10T20:00:54Z",
                "closed_at": "2021-04-22T15:23:43Z",
                "title": "Black does not accuse unnecessary spaces in the docstring",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                        "commit_date": "2021-04-22T15:23:41Z",
                        "parents": "7b4ca4bd90c148d72548048f5109537a555aaf77",
                        "stat": {
                            "total": 20,
                            "additions": 91,
                            "deletions": 71,
                            "files": [
                                {
                                    "sha": "f911fdf77ae949beeaef44c43d34b6902d3eb161",
                                    "filename": "CHANGES.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/CHANGES.md",
                                    "raw_url": "https://github.com/psf/black/raw/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/CHANGES.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/CHANGES.md?ref=1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                                    "patch": "@@ -4,6 +4,9 @@\n \n #### _Black_\n \n+- `Black` now processes one-line docstrings by stripping leading and trailing spaces,\n+  and adding a padding space when needed to break up \"\"\"\". (#1740)\n+\n - `Black` now cleans up leading non-breaking spaces in comments (#2092)\n \n - `Black` now respects `--skip-string-normalization` when normalizing multiline"
                                },
                                {
                                    "sha": "efa82f41160ac12f1665052fc7ccfc459d71cdc0",
                                    "filename": "src/black/__init__.py",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 13,
                                    "changes": 40,
                                    "blob_url": "https://github.com/psf/black/blob/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/src%2Fblack%2F__init__.py",
                                    "raw_url": "https://github.com/psf/black/raw/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/src%2Fblack%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2F__init__.py?ref=1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                                    "patch": "@@ -2153,16 +2153,35 @@ def visit_STRING(self, leaf: Leaf) -> Iterator[Line]:\n             # We're ignoring docstrings with backslash newline escapes because changing\n             # indentation of those changes the AST representation of the code.\n             prefix = get_string_prefix(leaf.value)\n-            lead_len = len(prefix) + 3\n-            tail_len = -3\n-            indent = \" \" * 4 * self.current_line.depth\n-            docstring = fix_docstring(leaf.value[lead_len:tail_len], indent)\n+            docstring = leaf.value[len(prefix) :]  # Remove the prefix\n+            quote_char = docstring[0]\n+            # A natural way to remove the outer quotes is to do:\n+            #   docstring = docstring.strip(quote_char)\n+            # but that breaks on \"\"\"\"\"x\"\"\" (which is '\"\"x').\n+            # So we actually need to remove the first character and the next two\n+            # characters but only if they are the same as the first.\n+            quote_len = 1 if docstring[1] != quote_char else 3\n+            docstring = docstring[quote_len:-quote_len]\n+\n+            if is_multiline_string(leaf):\n+                indent = \" \" * 4 * self.current_line.depth\n+                docstring = fix_docstring(docstring, indent)\n+            else:\n+                docstring = docstring.strip()\n+\n             if docstring:\n-                if leaf.value[lead_len - 1] == docstring[0]:\n+                # Add some padding if the docstring starts / ends with a quote mark.\n+                if docstring[0] == quote_char:\n                     docstring = \" \" + docstring\n-                if leaf.value[tail_len + 1] == docstring[-1]:\n+                if docstring[-1] == quote_char:\n                     docstring = docstring + \" \"\n-            leaf.value = leaf.value[0:lead_len] + docstring + leaf.value[tail_len:]\n+            else:\n+                # Add some padding if the docstring is empty.\n+                docstring = \" \"\n+\n+            # We could enforce triple quotes at this point.\n+            quote = quote_char * quote_len\n+            leaf.value = prefix + quote + docstring + quote\n \n         yield from self.visit_default(leaf)\n \n@@ -6113,7 +6132,7 @@ def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:\n \n @lru_cache()\n def get_gitignore(root: Path) -> PathSpec:\n-    \"\"\" Return a PathSpec matching gitignore content if present.\"\"\"\n+    \"\"\"Return a PathSpec matching gitignore content if present.\"\"\"\n     gitignore = root / \".gitignore\"\n     lines: List[str] = []\n     if gitignore.is_file():\n@@ -6953,11 +6972,6 @@ def patched_main() -> None:\n \n \n def is_docstring(leaf: Leaf) -> bool:\n-    if not is_multiline_string(leaf):\n-        # For the purposes of docstring re-indentation, we don't need to do anything\n-        # with single-line docstrings.\n-        return False\n-\n     if prev_siblings_are(\n         leaf.parent, [None, token.NEWLINE, token.INDENT, syms.simple_stmt]\n     ):"
                                },
                                {
                                    "sha": "3a1fae090a01b45e593c2fc75d25f5974ea7c6d3",
                                    "filename": "src/black_primer/primer.json",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 5,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/src%2Fblack_primer%2Fprimer.json",
                                    "raw_url": "https://github.com/psf/black/raw/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/src%2Fblack_primer%2Fprimer.json",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack_primer%2Fprimer.json?ref=1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                                    "patch": "@@ -3,7 +3,7 @@\n   \"projects\": {\n     \"aioexabgp\": {\n       \"cli_arguments\": [],\n-      \"expect_formatting_changes\": false,\n+      \"expect_formatting_changes\": true,\n       \"git_clone_url\": \"https://github.com/cooperlees/aioexabgp.git\",\n       \"long_checkout\": false,\n       \"py_versions\": [\"all\"]\n@@ -17,7 +17,7 @@\n     },\n     \"bandersnatch\": {\n       \"cli_arguments\": [],\n-      \"expect_formatting_changes\": false,\n+      \"expect_formatting_changes\": true,\n       \"git_clone_url\": \"https://github.com/pypa/bandersnatch.git\",\n       \"long_checkout\": false,\n       \"py_versions\": [\"all\"]\n@@ -84,7 +84,7 @@\n     },\n     \"ptr\": {\n       \"cli_arguments\": [],\n-      \"expect_formatting_changes\": false,\n+      \"expect_formatting_changes\": true,\n       \"git_clone_url\": \"https://github.com/facebookincubator/ptr.git\",\n       \"long_checkout\": false,\n       \"py_versions\": [\"all\"]\n@@ -98,14 +98,14 @@\n     },\n     \"sqlalchemy\": {\n       \"cli_arguments\": [],\n-      \"expect_formatting_changes\": false,\n+      \"expect_formatting_changes\": true,\n       \"git_clone_url\": \"https://github.com/sqlalchemy/sqlalchemy.git\",\n       \"long_checkout\": false,\n       \"py_versions\": [\"all\"]\n     },\n     \"tox\": {\n       \"cli_arguments\": [],\n-      \"expect_formatting_changes\": false,\n+      \"expect_formatting_changes\": true,\n       \"git_clone_url\": \"https://github.com/tox-dev/tox.git\",\n       \"long_checkout\": false,\n       \"py_versions\": [\"all\"]"
                                },
                                {
                                    "sha": "71b02e920c7a23d575130f47d9f242f7d40a4bb9",
                                    "filename": "tox.ini",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/tox.ini",
                                    "raw_url": "https://github.com/psf/black/raw/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/tox.ini",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tox.ini?ref=1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                                    "patch": "@@ -23,4 +23,4 @@ commands =\n     pip install -e .[d]\n     coverage erase\n     coverage run fuzz.py\n-    coverage report\n\\ No newline at end of file\n+    coverage report"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "74532b2b91de8c19e483c6f4605f7774a169a83c",
                                    "filename": "tests/data/docstring.py",
                                    "status": "modified",
                                    "additions": 35,
                                    "deletions": 1,
                                    "changes": 36,
                                    "blob_url": "https://github.com/psf/black/blob/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/tests%2Fdata%2Fdocstring.py",
                                    "raw_url": "https://github.com/psf/black/raw/1fc3215e8c8856094b20d497e4e0e3e547ed38eb/tests%2Fdata%2Fdocstring.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fdocstring.py?ref=1fc3215e8c8856094b20d497e4e0e3e547ed38eb",
                                    "patch": "@@ -102,6 +102,23 @@ def and_this():\n   \"hey yah\"'''\n \n \n+def empty():\n+    '''\n+    \n+    \n+    \n+    \n+    '''\n+\n+\n+def oneline_empty():\n+    '''      '''\n+\n+\n+def single_quotes():\n+    'testing'\n+\n+\n def believe_it_or_not_this_is_in_the_py_stdlib(): ''' \n \"hey yah\"'''\n \n@@ -110,6 +127,8 @@ def ignored_docstring():\n     \"\"\"a => \\\n b\"\"\"  \n \n+def single_line_docstring_with_whitespace():\n+    \"\"\"   This should be stripped \"\"\"\n \n def docstring_with_inline_tabs_and_space_indentation():\n     \"\"\"hey\n@@ -134,7 +153,6 @@ def docstring_with_inline_tabs_and_tab_indentation():\n \tline ends with some tabs\t\t\n \t\"\"\"\n \tpass\n-        \n \n # output\n \n@@ -241,6 +259,18 @@ def and_this():\n     \"hey yah\"'''\n \n \n+def empty():\n+    \"\"\" \"\"\"\n+\n+\n+def oneline_empty():\n+    \"\"\" \"\"\"\n+\n+\n+def single_quotes():\n+    \"testing\"\n+\n+\n def believe_it_or_not_this_is_in_the_py_stdlib():\n     '''\n     \"hey yah\"'''\n@@ -251,6 +281,10 @@ def ignored_docstring():\n b\"\"\"\n \n \n+def single_line_docstring_with_whitespace():\n+    \"\"\"This should be stripped\"\"\"\n+\n+\n def docstring_with_inline_tabs_and_space_indentation():\n     \"\"\"hey\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_format.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 1632,
                "created_at": "2020-08-26T18:11:36Z",
                "closed_at": "2022-05-08T04:34:33Z",
                "title": "Docstrings reformatted to be too long",
                "labels": "T: bug, F: strings, F: linetoolong",
                "commits": [
                    {
                        "hash": "20d8ccb54253f8a66321f6708d53e2a05a54079b",
                        "commit_date": "2022-05-08T04:34:28Z",
                        "parents": "62c2b167bcf22683fc11add2f24a132d36e8fd19",
                        "stat": {
                            "total": 2,
                            "additions": 160,
                            "deletions": 158,
                            "files": [
                                {
                                    "sha": "8f43431c8421c23d9986697a1f1295fd848f06d4",
                                    "filename": "CHANGES.md",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/CHANGES.md",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/CHANGES.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/CHANGES.md?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -17,6 +17,7 @@\n \n <!-- Changes that affect Black's preview style -->\n \n+- Fixed bug where docstrings with triple quotes could exceed max line length (#3044)\n - Remove redundant parentheses around awaited objects (#2991)\n - Parentheses around return annotations are now managed (#2990)\n - Remove unnecessary parentheses from `with` statements (#2926)"
                                },
                                {
                                    "sha": "ff54e05c4e6299d1bd2a456473a169839a03b43a",
                                    "filename": "src/black/linegen.py",
                                    "status": "modified",
                                    "additions": 24,
                                    "deletions": 2,
                                    "changes": 26,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/src%2Fblack%2Flinegen.py",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/src%2Fblack%2Flinegen.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2Flinegen.py?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -305,9 +305,9 @@ def visit_STRING(self, leaf: Leaf) -> Iterator[Line]:\n             quote_len = 1 if docstring[1] != quote_char else 3\n             docstring = docstring[quote_len:-quote_len]\n             docstring_started_empty = not docstring\n+            indent = \" \" * 4 * self.current_line.depth\n \n             if is_multiline_string(leaf):\n-                indent = \" \" * 4 * self.current_line.depth\n                 docstring = fix_docstring(docstring, indent)\n             else:\n                 docstring = docstring.strip()\n@@ -329,7 +329,29 @@ def visit_STRING(self, leaf: Leaf) -> Iterator[Line]:\n \n             # We could enforce triple quotes at this point.\n             quote = quote_char * quote_len\n-            leaf.value = prefix + quote + docstring + quote\n+\n+            if Preview.long_docstring_quotes_on_newline in self.mode:\n+                # We need to find the length of the last line of the docstring\n+                # to find if we can add the closing quotes to the line without\n+                # exceeding the maximum line length.\n+                # If docstring is one line, then we need to add the length\n+                # of the indent, prefix, and starting quotes. Ending quote are\n+                # handled later\n+                lines = docstring.splitlines()\n+                last_line_length = len(lines[-1]) if docstring else 0\n+\n+                if len(lines) == 1:\n+                    last_line_length += len(indent) + len(prefix) + quote_len\n+\n+                # If adding closing quotes would cause the last line to exceed\n+                # the maximum line length then put a line break before the\n+                # closing quotes\n+                if last_line_length + quote_len > self.mode.line_length:\n+                    leaf.value = prefix + quote + docstring + \"\\n\" + indent + quote\n+                else:\n+                    leaf.value = prefix + quote + docstring + quote\n+            else:\n+                leaf.value = prefix + quote + docstring + quote\n \n         yield from self.visit_default(leaf)\n "
                                },
                                {
                                    "sha": "a418e0eb66510ef6ea451cbc51f8b4be035816d4",
                                    "filename": "src/black/mode.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/src%2Fblack%2Fmode.py",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/src%2Fblack%2Fmode.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2Fmode.py?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -147,6 +147,7 @@ class Preview(Enum):\n     remove_redundant_parens = auto()\n     one_element_subscript = auto()\n     annotation_parens = auto()\n+    long_docstring_quotes_on_newline = auto()\n \n \n class Deprecated(UserWarning):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7153be468c112963806ea2a7623717b1c3a88b7b",
                                    "filename": "tests/data/docstring.py",
                                    "status": "modified",
                                    "additions": 42,
                                    "deletions": 0,
                                    "changes": 42,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Fdata%2Fdocstring.py",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Fdata%2Fdocstring.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fdocstring.py?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -188,6 +188,27 @@ def my_god_its_full_of_stars_2():\n     \"I'm sorry Dave\u2001\"\n \n \n+def docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\"\"\"\n+\n+\n+def docstring_almost_at_line_limit2():\n+    \"\"\"long docstring.................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def docstring_at_line_limit():\n+    \"\"\"long docstring................................................................\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit():\n+    \"\"\"first line-----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\"\n+\n+\n # output\n \n class MyClass:\n@@ -375,3 +396,24 @@ def my_god_its_full_of_stars_1():\n # the space below is actually a \\u2001, removed in output\n def my_god_its_full_of_stars_2():\n     \"I'm sorry Dave\"\n+\n+\n+def docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\"\"\"\n+\n+\n+def docstring_almost_at_line_limit2():\n+    \"\"\"long docstring.................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def docstring_at_line_limit():\n+    \"\"\"long docstring................................................................\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit():\n+    \"\"\"first line-----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\""
                                },
                                {
                                    "sha": "2da4cd1acdb305215335ae6d3be1cf1f7aa7c19d",
                                    "filename": "tests/data/docstring_preview.py",
                                    "status": "added",
                                    "additions": 89,
                                    "deletions": 0,
                                    "changes": 89,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Fdata%2Fdocstring_preview.py",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Fdata%2Fdocstring_preview.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fdocstring_preview.py?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -0,0 +1,89 @@\n+def docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\n+    \"\"\"\n+\n+\n+def docstring_almost_at_line_limit_with_prefix():\n+    f\"\"\"long docstring................................................................\n+    \"\"\"\n+\n+\n+def mulitline_docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def mulitline_docstring_almost_at_line_limit_with_prefix():\n+    f\"\"\"long docstring................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def docstring_at_line_limit():\n+    \"\"\"long docstring................................................................\"\"\"\n+\n+\n+def docstring_at_line_limit_with_prefix():\n+    f\"\"\"long docstring...............................................................\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit():\n+    \"\"\"first line-----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit_with_prefix():\n+    f\"\"\"first line----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\"\n+\n+\n+# output\n+\n+\n+def docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\n+    \"\"\"\n+\n+\n+def docstring_almost_at_line_limit_with_prefix():\n+    f\"\"\"long docstring................................................................\n+    \"\"\"\n+\n+\n+def mulitline_docstring_almost_at_line_limit():\n+    \"\"\"long docstring.................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def mulitline_docstring_almost_at_line_limit_with_prefix():\n+    f\"\"\"long docstring................................................................\n+\n+    ..................................................................................\n+    \"\"\"\n+\n+\n+def docstring_at_line_limit():\n+    \"\"\"long docstring................................................................\"\"\"\n+\n+\n+def docstring_at_line_limit_with_prefix():\n+    f\"\"\"long docstring...............................................................\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit():\n+    \"\"\"first line-----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\"\n+\n+\n+def multiline_docstring_at_line_limit_with_prefix():\n+    f\"\"\"first line----------------------------------------------------------------------\n+\n+    second line----------------------------------------------------------------------\"\"\""
                                },
                                {
                                    "sha": "2f08d1f273d4040c4f62b09cba3bb0fea0b80a71",
                                    "filename": "tests/test_format.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Ftest_format.py",
                                    "raw_url": "https://github.com/psf/black/raw/20d8ccb54253f8a66321f6708d53e2a05a54079b/tests%2Ftest_format.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_format.py?ref=20d8ccb54253f8a66321f6708d53e2a05a54079b",
                                    "patch": "@@ -91,6 +91,7 @@\n     \"one_element_subscript\",\n     \"remove_await_parens\",\n     \"return_annotation_brackets\",\n+    \"docstring_preview\",\n ]\n \n SOURCES: List[str] = ["
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_format.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 1596,
                "created_at": "2020-08-13T03:20:45Z",
                "closed_at": "2020-09-06T15:03:01Z",
                "title": "Crash on concatenated string + comment",
                "labels": "T: bug, C: crash",
                "commits": [
                    {
                        "hash": "7bca930ca3d84bbd01e98937b6b8a493d0254c7c",
                        "commit_date": "2020-09-06T15:02:57Z",
                        "parents": "e3ccabb23c5dc5495bd8f96b5c90c1db6a350d6d",
                        "stat": {
                            "total": 5,
                            "additions": 55,
                            "deletions": 50,
                            "files": [
                                {
                                    "sha": "ed5256eefe16db123ec863e5ef9b68a6d7b94b87",
                                    "filename": "src/black/__init__.py",
                                    "status": "modified",
                                    "additions": 23,
                                    "deletions": 5,
                                    "changes": 28,
                                    "blob_url": "https://github.com/psf/black/blob/7bca930ca3d84bbd01e98937b6b8a493d0254c7c/src%2Fblack%2F__init__.py",
                                    "raw_url": "https://github.com/psf/black/raw/7bca930ca3d84bbd01e98937b6b8a493d0254c7c/src%2Fblack%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2F__init__.py?ref=7bca930ca3d84bbd01e98937b6b8a493d0254c7c",
                                    "patch": "@@ -2889,11 +2889,8 @@ class StringMerger(CustomSplitMapMixin, StringTransformer):\n     \"\"\"StringTransformer that merges strings together.\n \n     Requirements:\n-        (A) The line contains adjacent strings such that at most one substring\n-        has inline comments AND none of those inline comments are pragmas AND\n-        the set of all substring prefixes is either of length 1 or equal to\n-        {\"\", \"f\"} AND none of the substrings are raw strings (i.e. are prefixed\n-        with 'r').\n+        (A) The line contains adjacent strings such that ALL of the validation checks\n+        listed in StringMerger.__validate_msg(...)'s docstring pass.\n             OR\n         (B) The line contains a string which uses line continuation backslashes.\n \n@@ -3142,6 +3139,7 @@ def __validate_msg(line: Line, string_idx: int) -> TResult[None]:\n             * Ok(None), if ALL validation checks (listed below) pass.\n                 OR\n             * Err(CannotTransform), if any of the following are true:\n+                - The target string group does not contain ANY stand-alone comments.\n                 - The target string is not in a string group (i.e. it has no\n                   adjacent strings).\n                 - The string group has more than one inline comment.\n@@ -3150,6 +3148,26 @@ def __validate_msg(line: Line, string_idx: int) -> TResult[None]:\n                   length greater than one and is not equal to {\"\", \"f\"}.\n                 - The string group consists of raw strings.\n         \"\"\"\n+        # We first check for \"inner\" stand-alone comments (i.e. stand-alone\n+        # comments that have a string leaf before them AND after them).\n+        for inc in [1, -1]:\n+            i = string_idx\n+            found_sa_comment = False\n+            is_valid_index = is_valid_index_factory(line.leaves)\n+            while is_valid_index(i) and line.leaves[i].type in [\n+                token.STRING,\n+                STANDALONE_COMMENT,\n+            ]:\n+                if line.leaves[i].type == STANDALONE_COMMENT:\n+                    found_sa_comment = True\n+                elif found_sa_comment:\n+                    return TErr(\n+                        \"StringMerger does NOT merge string groups which contain \"\n+                        \"stand-alone comments.\"\n+                    )\n+\n+                i += inc\n+\n         num_of_inline_string_comments = 0\n         set_of_prefixes = set()\n         num_of_strings = 0"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "745a636cdcfcb5cd65d3e9e428dc0b30d9ccbb92",
                                    "filename": "tests/data/long_strings__regression.py",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 0,
                                    "changes": 27,
                                    "blob_url": "https://github.com/psf/black/blob/7bca930ca3d84bbd01e98937b6b8a493d0254c7c/tests%2Fdata%2Flong_strings__regression.py",
                                    "raw_url": "https://github.com/psf/black/raw/7bca930ca3d84bbd01e98937b6b8a493d0254c7c/tests%2Fdata%2Flong_strings__regression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Flong_strings__regression.py?ref=7bca930ca3d84bbd01e98937b6b8a493d0254c7c",
                                    "patch": "@@ -310,6 +310,19 @@ def who(self):\n             passenger_association=passenger_association,\n         )\n \n+xxxxxxx_xxxxxx_xxxxxxx = xxx(\n+    [\n+        xxxxxxxxxxxx(\n+            xxxxxx_xxxxxxx=(\n+                '((x.aaaaaaaaa = \"xxxxxx.xxxxxxxxxxxxxxxxxxxxx\") || (x.xxxxxxxxx = \"xxxxxxxxxxxx\")) && '\n+                # xxxxx xxxxxxxxxxxx xxxx xxx (xxxxxxxxxxxxxxxx) xx x xxxxxxxxx xx xxxxxx.\n+                \"(x.bbbbbbbbbbbb.xxx != \"\n+                '\"xxx:xxx:xxx::cccccccccccc:xxxxxxx-xxxx/xxxxxxxxxxx/xxxxxxxxxxxxxxxxx\") && '\n+            )\n+        )\n+    ]\n+)\n+\n if __name__ == \"__main__\":\n     for i in range(4, 8):\n         cmd = (\n@@ -709,6 +722,20 @@ def who(self):\n         )\n \n \n+xxxxxxx_xxxxxx_xxxxxxx = xxx(\n+    [\n+        xxxxxxxxxxxx(\n+            xxxxxx_xxxxxxx=(\n+                '((x.aaaaaaaaa = \"xxxxxx.xxxxxxxxxxxxxxxxxxxxx\") || (x.xxxxxxxxx ='\n+                ' \"xxxxxxxxxxxx\")) && '\n+                # xxxxx xxxxxxxxxxxx xxxx xxx (xxxxxxxxxxxxxxxx) xx x xxxxxxxxx xx xxxxxx.\n+                \"(x.bbbbbbbbbbbb.xxx != \"\n+                '\"xxx:xxx:xxx::cccccccccccc:xxxxxxx-xxxx/xxxxxxxxxxx/xxxxxxxxxxxxxxxxx\") && '\n+            )\n+        )\n+    ]\n+)\n+\n if __name__ == \"__main__\":\n     for i in range(4, 8):\n         cmd = ("
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_black.py",
                "testStepsFull": "pipenv run pytest",
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5"
            },
            {
                "id": 1493,
                "created_at": "2020-06-12T16:15:02Z",
                "closed_at": "2020-06-24T09:09:10Z",
                "title": "Black finds root incorrectly",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "2471b9256d9d9dfea1124d20072201693b9b0865",
                        "commit_date": "2020-06-24T09:09:07Z",
                        "parents": "f90f50a7436ca13517933c290ef007e7cb2e7258",
                        "stat": {
                            "total": 7,
                            "additions": 45,
                            "deletions": 38,
                            "files": [
                                {
                                    "sha": "d4c6e62bdbfb078f26199f78fa72dd7556cd35f1",
                                    "filename": "src/black/__init__.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 7,
                                    "changes": 23,
                                    "blob_url": "https://github.com/psf/black/blob/2471b9256d9d9dfea1124d20072201693b9b0865/src%2Fblack%2F__init__.py",
                                    "raw_url": "https://github.com/psf/black/raw/2471b9256d9d9dfea1124d20072201693b9b0865/src%2Fblack%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2F__init__.py?ref=2471b9256d9d9dfea1124d20072201693b9b0865",
                                    "patch": "@@ -5825,20 +5825,29 @@ def gen_python_files(\n def find_project_root(srcs: Iterable[str]) -> Path:\n     \"\"\"Return a directory containing .git, .hg, or pyproject.toml.\n \n-    That directory can be one of the directories passed in `srcs` or their\n-    common parent.\n+    That directory will be a common parent of all files and directories\n+    passed in `srcs`.\n \n     If no directory in the tree contains a marker that would specify it's the\n     project root, the root of the file system is returned.\n     \"\"\"\n     if not srcs:\n         return Path(\"/\").resolve()\n \n-    common_base = min(Path(src).resolve() for src in srcs)\n-    if common_base.is_dir():\n-        # Append a fake file so `parents` below returns `common_base_dir`, too.\n-        common_base /= \"fake-file\"\n-    for directory in common_base.parents:\n+    path_srcs = [Path(src).resolve() for src in srcs]\n+\n+    # A list of lists of parents for each 'src'. 'src' is included as a\n+    # \"parent\" of itself if it is a directory\n+    src_parents = [\n+        list(path.parents) + ([path] if path.is_dir() else []) for path in path_srcs\n+    ]\n+\n+    common_base = max(\n+        set.intersection(*(set(parents) for parents in src_parents)),\n+        key=lambda path: path.parts,\n+    )\n+\n+    for directory in (common_base, *common_base.parents):\n         if (directory / \".git\").exists():\n             return directory\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "3ed5daa4b494afb5b8a0918d8f60e7426f623201",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 22,
                                    "deletions": 0,
                                    "changes": 22,
                                    "blob_url": "https://github.com/psf/black/blob/2471b9256d9d9dfea1124d20072201693b9b0865/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/2471b9256d9d9dfea1124d20072201693b9b0865/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=2471b9256d9d9dfea1124d20072201693b9b0865",
                                    "patch": "@@ -1801,6 +1801,28 @@ def __init__(self) -> None:\n         self.assertEqual(config[\"exclude\"], r\"\\.pyi?$\")\n         self.assertEqual(config[\"include\"], r\"\\.py?$\")\n \n+    def test_find_project_root(self) -> None:\n+        with TemporaryDirectory() as workspace:\n+            root = Path(workspace)\n+            test_dir = root / \"test\"\n+            test_dir.mkdir()\n+\n+            src_dir = root / \"src\"\n+            src_dir.mkdir()\n+\n+            root_pyproject = root / \"pyproject.toml\"\n+            root_pyproject.touch()\n+            src_pyproject = src_dir / \"pyproject.toml\"\n+            src_pyproject.touch()\n+            src_python = src_dir / \"foo.py\"\n+            src_python.touch()\n+\n+            self.assertEqual(\n+                black.find_project_root((src_dir, test_dir)), root.resolve()\n+            )\n+            self.assertEqual(black.find_project_root((src_dir,)), src_dir.resolve())\n+            self.assertEqual(black.find_project_root((src_python,)), src_dir.resolve())\n+\n \n class BlackDTestCase(AioHTTPTestCase):\n     async def get_application(self) -> web.Application:"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_black.py",
                "testStepsFull": "pipenv run pytest",
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5"
            },
            {
                "id": 593,
                "created_at": "2018-11-05T12:10:17Z",
                "closed_at": "2019-05-09T16:01:17Z",
                "title": "Black can't parse new Python 3.7 async generator syntax",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                        "commit_date": "2019-05-09T15:59:29Z",
                        "parents": "8c8adedc2a74a494c24f93e405b6418ac32f54cd",
                        "stat": {
                            "total": 18,
                            "additions": 208,
                            "deletions": 190,
                            "files": [
                                {
                                    "sha": "c8aa30b812b49795c797e0e262159ebda024f67a",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 70,
                                    "deletions": 12,
                                    "changes": 82,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -48,6 +48,7 @@\n from blib2to3.pgen2 import driver, token\n from blib2to3.pgen2.grammar import Grammar\n from blib2to3.pgen2.parse import ParseError\n+from blib2to3.pgen2.tokenize import TokenizerConfig\n \n \n __version__ = \"19.3b0\"\n@@ -136,33 +137,44 @@ class Feature(Enum):\n     NUMERIC_UNDERSCORES = 3\n     TRAILING_COMMA_IN_CALL = 4\n     TRAILING_COMMA_IN_DEF = 5\n+    # The following two feature-flags are mutually exclusive, and exactly one should be\n+    # set for every version of python.\n+    ASYNC_IS_VALID_IDENTIFIER = 6\n+    ASYNC_IS_RESERVED_KEYWORD = 7\n \n \n VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {\n-    TargetVersion.PY27: set(),\n-    TargetVersion.PY33: {Feature.UNICODE_LITERALS},\n-    TargetVersion.PY34: {Feature.UNICODE_LITERALS},\n-    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},\n+    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},\n+    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n+    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},\n+    TargetVersion.PY35: {\n+        Feature.UNICODE_LITERALS,\n+        Feature.TRAILING_COMMA_IN_CALL,\n+        Feature.ASYNC_IS_VALID_IDENTIFIER,\n+    },\n     TargetVersion.PY36: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n+        Feature.ASYNC_IS_VALID_IDENTIFIER,\n     },\n     TargetVersion.PY37: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n+        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n     TargetVersion.PY38: {\n         Feature.UNICODE_LITERALS,\n         Feature.F_STRINGS,\n         Feature.NUMERIC_UNDERSCORES,\n         Feature.TRAILING_COMMA_IN_CALL,\n         Feature.TRAILING_COMMA_IN_DEF,\n+        Feature.ASYNC_IS_RESERVED_KEYWORD,\n     },\n }\n \n@@ -748,29 +760,75 @@ def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:\n         return tiow.read(), encoding, newline\n \n \n-def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:\n+@dataclass(frozen=True)\n+class ParserConfig:\n+    grammar: Grammar\n+    tokenizer_config: TokenizerConfig = TokenizerConfig()\n+\n+\n+def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:\n     if not target_versions:\n         # No target_version specified, so try all grammars.\n         return [\n-            pygram.python_grammar_no_print_statement_no_exec_statement,\n-            pygram.python_grammar_no_print_statement,\n-            pygram.python_grammar,\n+            # Python 3.7+\n+            ParserConfig(\n+                pygram.python_grammar_no_print_statement_no_exec_statement,\n+                TokenizerConfig(async_is_reserved_keyword=True),\n+            ),\n+            # Python 3.0-3.6\n+            ParserConfig(\n+                pygram.python_grammar_no_print_statement_no_exec_statement,\n+                TokenizerConfig(async_is_reserved_keyword=False),\n+            ),\n+            # Python 2.7 with future print_function import\n+            ParserConfig(pygram.python_grammar_no_print_statement),\n+            # Python 2.7\n+            ParserConfig(pygram.python_grammar),\n         ]\n     elif all(version.is_python2() for version in target_versions):\n         # Python 2-only code, so try Python 2 grammars.\n-        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]\n+        return [\n+            # Python 2.7 with future print_function import\n+            ParserConfig(pygram.python_grammar_no_print_statement),\n+            # Python 2.7\n+            ParserConfig(pygram.python_grammar),\n+        ]\n     else:\n         # Python 3-compatible code, so only try Python 3 grammar.\n-        return [pygram.python_grammar_no_print_statement_no_exec_statement]\n+        configs = []\n+        # If we have to parse both, try to parse async as a keyword first\n+        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):\n+            # Python 3.7+\n+            configs.append(\n+                ParserConfig(\n+                    pygram.python_grammar_no_print_statement_no_exec_statement,\n+                    TokenizerConfig(async_is_reserved_keyword=True),\n+                )\n+            )\n+        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):\n+            # Python 3.0-3.6\n+            configs.append(\n+                ParserConfig(\n+                    pygram.python_grammar_no_print_statement_no_exec_statement,\n+                    TokenizerConfig(async_is_reserved_keyword=False),\n+                )\n+            )\n+        # At least one of the above branches must have been taken, because every Python\n+        # version has exactly one of the two 'ASYNC_IS_*' flags\n+        return configs\n \n \n def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:\n     \"\"\"Given a string with source, return the lib2to3 Node.\"\"\"\n     if src_txt[-1:] != \"\\n\":\n         src_txt += \"\\n\"\n \n-    for grammar in get_grammars(set(target_versions)):\n-        drv = driver.Driver(grammar, pytree.convert)\n+    for parser_config in get_parser_configs(set(target_versions)):\n+        drv = driver.Driver(\n+            parser_config.grammar,\n+            pytree.convert,\n+            tokenizer_config=parser_config.tokenizer_config,\n+        )\n         try:\n             result = drv.parse_string(src_txt, True)\n             break"
                                },
                                {
                                    "sha": "e681b526a2219ae46dff248bb991173622c7698e",
                                    "filename": "blib2to3/pgen2/driver.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 3,
                                    "changes": 16,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Fdriver.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Fdriver.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Fdriver.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -29,12 +29,19 @@\n \n class Driver(object):\n \n-    def __init__(self, grammar, convert=None, logger=None):\n+    def __init__(\n+        self,\n+        grammar,\n+        convert=None,\n+        logger=None,\n+        tokenizer_config=tokenize.TokenizerConfig(),\n+    ):\n         self.grammar = grammar\n         if logger is None:\n             logger = logging.getLogger(__name__)\n         self.logger = logger\n         self.convert = convert\n+        self.tokenizer_config = tokenizer_config\n \n     def parse_tokens(self, tokens, debug=False):\n         \"\"\"Parse a series of tokens and return the syntax tree.\"\"\"\n@@ -97,7 +104,7 @@ def parse_tokens(self, tokens, debug=False):\n \n     def parse_stream_raw(self, stream, debug=False):\n         \"\"\"Parse a stream and return the syntax tree.\"\"\"\n-        tokens = tokenize.generate_tokens(stream.readline)\n+        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)\n         return self.parse_tokens(tokens, debug)\n \n     def parse_stream(self, stream, debug=False):\n@@ -111,7 +118,10 @@ def parse_file(self, filename, encoding=None, debug=False):\n \n     def parse_string(self, text, debug=False):\n         \"\"\"Parse a string and return the syntax tree.\"\"\"\n-        tokens = tokenize.generate_tokens(io.StringIO(text).readline)\n+        tokens = tokenize.generate_tokens(\n+            io.StringIO(text).readline,\n+            config=self.tokenizer_config,\n+        )\n         return self.parse_tokens(tokens, debug)\n \n     def _partially_consume_prefix(self, prefix, column):"
                                },
                                {
                                    "sha": "a4a354634385d62e1d71b3517574caf3028b4155",
                                    "filename": "blib2to3/pgen2/driver.pyi",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 1,
                                    "changes": 9,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Fdriver.pyi",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Fdriver.pyi",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Fdriver.pyi?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -8,13 +8,20 @@ from logging import Logger\n from blib2to3.pytree import _Convert, _NL\n from blib2to3.pgen2 import _Path\n from blib2to3.pgen2.grammar import Grammar\n+from blib2to3.pgen2.tokenize import TokenizerConfig\n \n \n class Driver:\n     grammar: Grammar\n     logger: Logger\n     convert: _Convert\n-    def __init__(self, grammar: Grammar, convert: Optional[_Convert] = ..., logger: Optional[Logger] = ...) -> None: ...\n+    def __init__(\n+        self,\n+        grammar: Grammar,\n+        convert: Optional[_Convert] = ...,\n+        logger: Optional[Logger] = ...,\n+        tokenizer_config: TokenizerConfig = ...\n+    ) -> None: ...\n     def parse_tokens(self, tokens: Iterable[Any], debug: bool = ...) -> _NL: ...\n     def parse_stream_raw(self, stream: IO[Text], debug: bool = ...) -> _NL: ...\n     def parse_stream(self, stream: IO[Text], debug: bool = ...) -> _NL: ..."
                                },
                                {
                                    "sha": "43e1d597bc9b64792dd19229830f7578032fbe41",
                                    "filename": "blib2to3/pgen2/tokenize.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 2,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Ftokenize.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -31,6 +31,7 @@\n \n import re\n from codecs import BOM_UTF8, lookup\n+from attr import dataclass\n from blib2to3.pgen2.token import *\n \n from . import token\n@@ -137,6 +138,10 @@ def _combinations(*l):\n \n tabsize = 8\n \n+@dataclass(frozen=True)\n+class TokenizerConfig:\n+    async_is_reserved_keyword: bool = False\n+\n class TokenError(Exception): pass\n \n class StopTokenizing(Exception): pass\n@@ -334,7 +339,7 @@ def untokenize(iterable):\n     ut = Untokenizer()\n     return ut.untokenize(iterable)\n \n-def generate_tokens(readline):\n+def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):\n     \"\"\"\n     The generate_tokens() generator requires one argument, readline, which\n     must be a callable object which provides the same interface as the\n@@ -356,6 +361,9 @@ def generate_tokens(readline):\n     contline = None\n     indents = [0]\n \n+    # If we know we're parsing 3.7+, we can unconditionally parse `async` and\n+    # `await` as keywords.\n+    async_is_reserved_keyword = config.async_is_reserved_keyword\n     # 'stashed' and 'async_*' are used for async/await parsing\n     stashed = None\n     async_def = False\n@@ -506,7 +514,7 @@ def generate_tokens(readline):\n                         yield (STRING, token, spos, epos, line)\n                 elif initial.isidentifier():               # ordinary name\n                     if token in ('async', 'await'):\n-                        if async_def:\n+                        if async_is_reserved_keyword or async_def:\n                             yield (ASYNC if token == 'async' else AWAIT,\n                                    token, spos, epos, line)\n                             continue"
                                },
                                {
                                    "sha": "ac0f0f1bf6f1f254016e9b006c498e87787f93f6",
                                    "filename": "blib2to3/pgen2/tokenize.pyi",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Ftokenize.pyi",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/blib2to3%2Fpgen2%2Ftokenize.pyi",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Ftokenize.pyi?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -2,13 +2,17 @@\n # NOTE: Only elements from __all__ are present.\n \n from typing import Callable, Iterable, Iterator, List, Text, Tuple\n+from attr import dataclass\n from blib2to3.pgen2.token import *  # noqa\n \n \n _Coord = Tuple[int, int]\n _TokenEater = Callable[[int, Text, _Coord, _Coord, Text], None]\n _TokenInfo = Tuple[int, Text, _Coord, _Coord, Text]\n \n+@dataclass(frozen=True)\n+class TokenizerConfig:\n+    async_is_reserved_keyword: bool = False\n \n class TokenError(Exception): ...\n class StopTokenizing(Exception): ..."
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c36a5e54b0e33282591537b0f9dd8dec63c3fa08",
                                    "filename": "tests/data/async_as_identifier.py",
                                    "status": "added",
                                    "additions": 49,
                                    "deletions": 0,
                                    "changes": 49,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Fdata%2Fasync_as_identifier.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Fdata%2Fasync_as_identifier.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fasync_as_identifier.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -0,0 +1,49 @@\n+def async():\n+    pass\n+\n+\n+def await():\n+    pass\n+\n+\n+await = lambda: None\n+async = lambda: None\n+async()\n+await()\n+\n+\n+def sync_fn():\n+    await = lambda: None\n+    async = lambda: None\n+    async()\n+    await()\n+\n+\n+async def async_fn():\n+    await async_fn()\n+\n+\n+# output\n+def async():\n+    pass\n+\n+\n+def await():\n+    pass\n+\n+\n+await = lambda: None\n+async = lambda: None\n+async()\n+await()\n+\n+\n+def sync_fn():\n+    await = lambda: None\n+    async = lambda: None\n+    async()\n+    await()\n+\n+\n+async def async_fn():\n+    await async_fn()"
                                },
                                {
                                    "sha": "4401b7b0e72002dc58ccc63d72ee34c82e782195",
                                    "filename": "tests/data/python37.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 0,
                                    "changes": 16,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Fdata%2Fpython37.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Fdata%2Fpython37.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fpython37.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -14,6 +14,14 @@ async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\n+\n+def awaited_generator_value(n):\n+    return (await awaitable for awaitable in awaitable_list)\n+\n+def make_arange(n):\n+    return (i * 2 for i in range(n) if await wrap(i))\n+\n+\n # output\n \n \n@@ -39,3 +47,11 @@ async def func():\n                 self.async_inc, arange(8), batch_size=3\n             )\n         ]\n+\n+\n+def awaited_generator_value(n):\n+    return (await awaitable for awaitable in awaitable_list)\n+\n+\n+def make_arange(n):\n+    return (i * 2 for i in range(n) if await wrap(i))"
                                },
                                {
                                    "sha": "0ea4ac55078393c93e74458d3c60ca98102262bb",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/psf/black/blob/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/f8617f975d56e81cfb4070ce65584f7b29a77e7a/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=f8617f975d56e81cfb4070ce65584f7b29a77e7a",
                                    "patch": "@@ -502,15 +502,35 @@ def test_stub(self) -> None:\n         self.assertFormatEqual(expected, actual)\n         black.assert_stable(source, actual, mode)\n \n+    @patch(\"black.dump_to_file\", dump_to_stderr)\n+    def test_async_as_identifier(self) -> None:\n+        source_path = (THIS_DIR / \"data\" / \"async_as_identifier.py\").resolve()\n+        source, expected = read_data(\"async_as_identifier\")\n+        actual = fs(source)\n+        self.assertFormatEqual(expected, actual)\n+        major, minor = sys.version_info[:2]\n+        if major < 3 or (major <= 3 and minor < 7):\n+            black.assert_equivalent(source, actual)\n+        black.assert_stable(source, actual, black.FileMode())\n+        # ensure black can parse this when the target is 3.6\n+        self.invokeBlack([str(source_path), \"--target-version\", \"py36\"])\n+        # but not on 3.7, because async/await is no longer an identifier\n+        self.invokeBlack([str(source_path), \"--target-version\", \"py37\"], exit_code=123)\n+\n     @patch(\"black.dump_to_file\", dump_to_stderr)\n     def test_python37(self) -> None:\n+        source_path = (THIS_DIR / \"data\" / \"python37.py\").resolve()\n         source, expected = read_data(\"python37\")\n         actual = fs(source)\n         self.assertFormatEqual(expected, actual)\n         major, minor = sys.version_info[:2]\n         if major > 3 or (major == 3 and minor >= 7):\n             black.assert_equivalent(source, actual)\n         black.assert_stable(source, actual, black.FileMode())\n+        # ensure black can parse this when the target is 3.7\n+        self.invokeBlack([str(source_path), \"--target-version\", \"py37\"])\n+        # but not on 3.6, because we use async as a reserved keyword\n+        self.invokeBlack([str(source_path), \"--target-version\", \"py36\"], exit_code=123)\n \n     @patch(\"black.dump_to_file\", dump_to_stderr)\n     def test_fmtonoff(self) -> None:"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 569,
                "created_at": "2018-10-17T10:42:53Z",
                "closed_at": "2022-09-24T03:37:26Z",
                "title": "Invalid code after indented `# fmt: off`",
                "labels": "T: bug, C: invalid code, F: fmtoff",
                "commits": [
                    {
                        "hash": "55db05519ebfc502680aa55d289b7e47f6b2c6af",
                        "commit_date": "2022-09-24T03:37:22Z",
                        "parents": "bfc013ab93d0993a6e24235291dddd4c4ecd64ee",
                        "stat": {
                            "total": 26,
                            "additions": 177,
                            "deletions": 151,
                            "files": [
                                {
                                    "sha": "48f5035c133592db3b6b8e1f80efcadcc131b4f4",
                                    "filename": "CHANGES.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/55db05519ebfc502680aa55d289b7e47f6b2c6af/CHANGES.md",
                                    "raw_url": "https://github.com/psf/black/raw/55db05519ebfc502680aa55d289b7e47f6b2c6af/CHANGES.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/CHANGES.md?ref=55db05519ebfc502680aa55d289b7e47f6b2c6af",
                                    "patch": "@@ -10,6 +10,9 @@\n \n <!-- Changes that affect Black's stable style -->\n \n+- Fix a crash when `# fmt: on` is used on a different block level than `# fmt: off`\n+  (#3281)\n+\n ### Preview style\n \n <!-- Changes that affect Black's preview style -->"
                                },
                                {
                                    "sha": "3bda5de1774ce35a0091619e5fd3a5a95c5cef33",
                                    "filename": "docs/contributing/reference/reference_functions.rst",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/55db05519ebfc502680aa55d289b7e47f6b2c6af/docs%2Fcontributing%2Freference%2Freference_functions.rst",
                                    "raw_url": "https://github.com/psf/black/raw/55db05519ebfc502680aa55d289b7e47f6b2c6af/docs%2Fcontributing%2Freference%2Freference_functions.rst",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/docs%2Fcontributing%2Freference%2Freference_functions.rst?ref=55db05519ebfc502680aa55d289b7e47f6b2c6af",
                                    "patch": "@@ -137,9 +137,9 @@ Utilities\n \n .. autofunction:: black.comments.is_fmt_on\n \n-.. autofunction:: black.comments.contains_fmt_on_at_column\n+.. autofunction:: black.comments.children_contains_fmt_on\n \n-.. autofunction:: black.nodes.first_leaf_column\n+.. autofunction:: black.nodes.first_leaf_of\n \n .. autofunction:: black.linegen.generate_trailers_to_omit\n "
                                },
                                {
                                    "sha": "2a4c254ecd9119b2796c7116d92890d94c024981",
                                    "filename": "src/black/comments.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 18,
                                    "changes": 34,
                                    "blob_url": "https://github.com/psf/black/blob/55db05519ebfc502680aa55d289b7e47f6b2c6af/src%2Fblack%2Fcomments.py",
                                    "raw_url": "https://github.com/psf/black/raw/55db05519ebfc502680aa55d289b7e47f6b2c6af/src%2Fblack%2Fcomments.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2Fcomments.py?ref=55db05519ebfc502680aa55d289b7e47f6b2c6af",
                                    "patch": "@@ -14,11 +14,12 @@\n     STANDALONE_COMMENT,\n     WHITESPACE,\n     container_of,\n-    first_leaf_column,\n+    first_leaf_of,\n     preceding_leaf,\n+    syms,\n )\n from blib2to3.pgen2 import token\n-from blib2to3.pytree import Leaf, Node, type_repr\n+from blib2to3.pytree import Leaf, Node\n \n # types\n LN = Union[Leaf, Node]\n@@ -230,7 +231,7 @@ def generate_ignored_nodes(\n             return\n \n         # fix for fmt: on in children\n-        if contains_fmt_on_at_column(container, leaf.column, preview=preview):\n+        if children_contains_fmt_on(container, preview=preview):\n             for child in container.children:\n                 if isinstance(child, Leaf) and is_fmt_on(child, preview=preview):\n                     if child.type in CLOSING_BRACKETS:\n@@ -240,10 +241,14 @@ def generate_ignored_nodes(\n                         # The alternative is to fail the formatting.\n                         yield child\n                     return\n-                if contains_fmt_on_at_column(child, leaf.column, preview=preview):\n+                if children_contains_fmt_on(child, preview=preview):\n                     return\n                 yield child\n         else:\n+            if container.type == token.DEDENT and container.next_sibling is None:\n+                # This can happen when there is no matching `# fmt: on` comment at the\n+                # same level as `# fmt: on`. We need to keep this DEDENT.\n+                return\n             yield container\n             container = container.next_sibling\n \n@@ -268,17 +273,15 @@ def _generate_ignored_nodes_from_fmt_skip(\n         for sibling in siblings:\n             yield sibling\n     elif (\n-        parent is not None\n-        and type_repr(parent.type) == \"suite\"\n-        and leaf.type == token.NEWLINE\n+        parent is not None and parent.type == syms.suite and leaf.type == token.NEWLINE\n     ):\n         # The `# fmt: skip` is on the colon line of the if/while/def/class/...\n         # statements. The ignored nodes should be previous siblings of the\n         # parent suite node.\n         leaf.prefix = \"\"\n         ignored_nodes: List[LN] = []\n         parent_sibling = parent.prev_sibling\n-        while parent_sibling is not None and type_repr(parent_sibling.type) != \"suite\":\n+        while parent_sibling is not None and parent_sibling.type != syms.suite:\n             ignored_nodes.insert(0, parent_sibling)\n             parent_sibling = parent_sibling.prev_sibling\n         # Special case for `async_stmt` where the ASYNC token is on the\n@@ -306,17 +309,12 @@ def is_fmt_on(container: LN, preview: bool) -> bool:\n     return fmt_on\n \n \n-def contains_fmt_on_at_column(container: LN, column: int, *, preview: bool) -> bool:\n-    \"\"\"Determine if children at a given column have formatting switched on.\"\"\"\n+def children_contains_fmt_on(container: LN, *, preview: bool) -> bool:\n+    \"\"\"Determine if children have formatting switched on.\"\"\"\n     for child in container.children:\n-        if (\n-            isinstance(child, Node)\n-            and first_leaf_column(child) == column\n-            or isinstance(child, Leaf)\n-            and child.column == column\n-        ):\n-            if is_fmt_on(child, preview=preview):\n-                return True\n+        leaf = first_leaf_of(child)\n+        if leaf is not None and is_fmt_on(leaf, preview=preview):\n+            return True\n \n     return False\n "
                                },
                                {
                                    "sha": "aeb2be389c8cccb4f1321b0b04ad845571e4ba10",
                                    "filename": "src/black/nodes.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 6,
                                    "changes": 14,
                                    "blob_url": "https://github.com/psf/black/blob/55db05519ebfc502680aa55d289b7e47f6b2c6af/src%2Fblack%2Fnodes.py",
                                    "raw_url": "https://github.com/psf/black/raw/55db05519ebfc502680aa55d289b7e47f6b2c6af/src%2Fblack%2Fnodes.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/src%2Fblack%2Fnodes.py?ref=55db05519ebfc502680aa55d289b7e47f6b2c6af",
                                    "patch": "@@ -502,12 +502,14 @@ def container_of(leaf: Leaf) -> LN:\n     return container\n \n \n-def first_leaf_column(node: Node) -> Optional[int]:\n-    \"\"\"Returns the column of the first leaf child of a node.\"\"\"\n-    for child in node.children:\n-        if isinstance(child, Leaf):\n-            return child.column\n-    return None\n+def first_leaf_of(node: LN) -> Optional[Leaf]:\n+    \"\"\"Returns the first leaf of the node tree.\"\"\"\n+    if isinstance(node, Leaf):\n+        return node\n+    if node.children:\n+        return first_leaf_of(node.children[0])\n+    else:\n+        return None\n \n \n def is_arith_like(node: LN) -> bool:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "71b1381ed0dc6d99ce6a4067b44fc88f78e50a73",
                                    "filename": "tests/data/simple_cases/fmtonoff5.py",
                                    "status": "modified",
                                    "additions": 122,
                                    "deletions": 0,
                                    "changes": 122,
                                    "blob_url": "https://github.com/psf/black/blob/55db05519ebfc502680aa55d289b7e47f6b2c6af/tests%2Fdata%2Fsimple_cases%2Ffmtonoff5.py",
                                    "raw_url": "https://github.com/psf/black/raw/55db05519ebfc502680aa55d289b7e47f6b2c6af/tests%2Fdata%2Fsimple_cases%2Ffmtonoff5.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fsimple_cases%2Ffmtonoff5.py?ref=55db05519ebfc502680aa55d289b7e47f6b2c6af",
                                    "patch": "@@ -34,3 +34,125 @@ def test_func():\n         return True\n \n     return False\n+\n+\n+# Regression test for https://github.com/psf/black/issues/2567.\n+if True:\n+    # fmt: off\n+    for _ in range( 1 ):\n+    # fmt: on\n+        print ( \"This won't be formatted\" )\n+    print ( \"This won't be formatted either\" )\n+else:\n+    print ( \"This will be formatted\" )\n+\n+\n+# Regression test for https://github.com/psf/black/issues/3184.\n+class A:\n+    async def call(param):\n+        if param:\n+            # fmt: off\n+            if param[0:4] in (\n+                \"ABCD\", \"EFGH\"\n+            )  :\n+                # fmt: on\n+                print ( \"This won't be formatted\" )\n+\n+            elif param[0:4] in (\"ZZZZ\",):\n+                print ( \"This won't be formatted either\" )\n+\n+        print ( \"This will be formatted\" )\n+\n+\n+# Regression test for https://github.com/psf/black/issues/2985\n+class Named(t.Protocol):\n+    # fmt: off\n+    @property\n+    def  this_wont_be_formatted ( self ) -> str: ...\n+\n+class Factory(t.Protocol):\n+    def  this_will_be_formatted ( self, **kwargs ) -> Named: ...\n+    # fmt: on\n+\n+\n+# output\n+\n+\n+# Regression test for https://github.com/psf/black/issues/3129.\n+setup(\n+    entry_points={\n+        # fmt: off\n+        \"console_scripts\": [\n+            \"foo-bar\"\n+            \"=foo.bar.:main\",\n+        # fmt: on\n+            ]  # Includes an formatted indentation.\n+    },\n+)\n+\n+\n+# Regression test for https://github.com/psf/black/issues/2015.\n+run(\n+    # fmt: off\n+    [\n+        \"ls\",\n+        \"-la\",\n+    ]\n+    # fmt: on\n+    + path,\n+    check=True,\n+)\n+\n+\n+# Regression test for https://github.com/psf/black/issues/3026.\n+def test_func():\n+    # yapf: disable\n+    if  unformatted(  args  ):\n+        return True\n+    # yapf: enable\n+    elif b:\n+        return True\n+\n+    return False\n+\n+\n+# Regression test for https://github.com/psf/black/issues/2567.\n+if True:\n+    # fmt: off\n+    for _ in range( 1 ):\n+    # fmt: on\n+        print ( \"This won't be formatted\" )\n+    print ( \"This won't be formatted either\" )\n+else:\n+    print(\"This will be formatted\")\n+\n+\n+# Regression test for https://github.com/psf/black/issues/3184.\n+class A:\n+    async def call(param):\n+        if param:\n+            # fmt: off\n+            if param[0:4] in (\n+                \"ABCD\", \"EFGH\"\n+            )  :\n+                # fmt: on\n+                print ( \"This won't be formatted\" )\n+\n+            elif param[0:4] in (\"ZZZZ\",):\n+                print ( \"This won't be formatted either\" )\n+\n+        print(\"This will be formatted\")\n+\n+\n+# Regression test for https://github.com/psf/black/issues/2985\n+class Named(t.Protocol):\n+    # fmt: off\n+    @property\n+    def  this_wont_be_formatted ( self ) -> str: ...\n+\n+\n+class Factory(t.Protocol):\n+    def this_will_be_formatted(self, **kwargs) -> Named:\n+        ...\n+\n+    # fmt: on"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_format.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 389,
                "created_at": "2018-07-01T10:18:31Z",
                "closed_at": "2018-07-02T16:50:09Z",
                "title": "Fails when importing __future__ as renaming it",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a",
                        "commit_date": "2018-07-02T16:49:47Z",
                        "parents": "3bdd42389128bbbe8b64a8e050563f09bff99979",
                        "stat": {
                            "total": 12,
                            "additions": 45,
                            "deletions": 33,
                            "files": [
                                {
                                    "sha": "36a180da702a3a003276dc3a8e1f25b96b4abd01",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 10,
                                    "changes": 29,
                                    "blob_url": "https://github.com/psf/black/blob/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a",
                                    "patch": "@@ -20,6 +20,7 @@\n     Callable,\n     Collection,\n     Dict,\n+    Generator,\n     Generic,\n     Iterable,\n     Iterator,\n@@ -2910,7 +2911,23 @@ def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf\n \n def get_future_imports(node: Node) -> Set[str]:\n     \"\"\"Return a set of __future__ imports in the file.\"\"\"\n-    imports = set()\n+    imports: Set[str] = set()\n+\n+    def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:\n+        for child in children:\n+            if isinstance(child, Leaf):\n+                if child.type == token.NAME:\n+                    yield child.value\n+            elif child.type == syms.import_as_name:\n+                orig_name = child.children[0]\n+                assert isinstance(orig_name, Leaf), \"Invalid syntax parsing imports\"\n+                assert orig_name.type == token.NAME, \"Invalid syntax parsing imports\"\n+                yield orig_name.value\n+            elif child.type == syms.import_as_names:\n+                yield from get_imports_from_children(child.children)\n+            else:\n+                assert False, \"Invalid syntax parsing imports\"\n+\n     for child in node.children:\n         if child.type != syms.simple_stmt:\n             break\n@@ -2929,15 +2946,7 @@ def get_future_imports(node: Node) -> Set[str]:\n             module_name = first_child.children[1]\n             if not isinstance(module_name, Leaf) or module_name.value != \"__future__\":\n                 break\n-            for import_from_child in first_child.children[3:]:\n-                if isinstance(import_from_child, Leaf):\n-                    if import_from_child.type == token.NAME:\n-                        imports.add(import_from_child.value)\n-                else:\n-                    assert import_from_child.type == syms.import_as_names\n-                    for leaf in import_from_child.children:\n-                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:\n-                            imports.add(leaf.value)\n+            imports |= set(get_imports_from_children(first_child.children[3:]))\n         else:\n             break\n     return imports"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "2fe70392af6f6174828efb6acc1f94779b313270",
                                    "filename": "tests/data/python2_unicode_literals.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 2,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/tests%2Fdata%2Fpython2_unicode_literals.py",
                                    "raw_url": "https://github.com/psf/black/raw/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/tests%2Fdata%2Fpython2_unicode_literals.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fpython2_unicode_literals.py?ref=dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a",
                                    "patch": "@@ -1,5 +1,7 @@\n #!/usr/bin/env python2\n-from __future__ import unicode_literals\n+from __future__ import unicode_literals as _unicode_literals\n+from __future__ import absolute_import\n+from __future__ import print_function as lol, with_function\n \n u'hello'\n U\"hello\"\n@@ -9,7 +11,9 @@\n \n \n #!/usr/bin/env python2\n-from __future__ import unicode_literals\n+from __future__ import unicode_literals as _unicode_literals\n+from __future__ import absolute_import\n+from __future__ import print_function as lol, with_function\n \n \"hello\"\n \"hello\""
                                },
                                {
                                    "sha": "cc53aa61221e346fc144b2106a2a3077aca9f193",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a",
                                    "patch": "@@ -735,6 +735,14 @@ def test_get_future_imports(self) -> None:\n         self.assertEqual(set(), black.get_future_imports(node))\n         node = black.lib2to3_parse(\"from some.module import black\\n\")\n         self.assertEqual(set(), black.get_future_imports(node))\n+        node = black.lib2to3_parse(\n+            \"from __future__ import unicode_literals as _unicode_literals\"\n+        )\n+        self.assertEqual({\"unicode_literals\"}, black.get_future_imports(node))\n+        node = black.lib2to3_parse(\n+            \"from __future__ import unicode_literals as _lol, print\"\n+        )\n+        self.assertEqual({\"unicode_literals\", \"print\"}, black.get_future_imports(node))\n \n     def test_debug_visitor(self) -> None:\n         source, _ = read_data(\"debug_visitor.py\")"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 385,
                "created_at": "2018-06-29T14:37:50Z",
                "closed_at": "2018-07-22T14:30:18Z",
                "title": "`# fmt: off` directly before `yield` only affects the `yield`",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "e94a41f92a568706700522aaad48ebd137fe1d8b",
                        "commit_date": "2018-07-22T14:30:02Z",
                        "parents": "df965b055808bbfbe32f4f20ac949b364dafc900",
                        "stat": {
                            "total": 13,
                            "additions": 87,
                            "deletions": 74,
                            "files": [
                                {
                                    "sha": "61cb88ae6236c063ff7791c866e299900dfd8dc9",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/e94a41f92a568706700522aaad48ebd137fe1d8b/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/e94a41f92a568706700522aaad48ebd137fe1d8b/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=e94a41f92a568706700522aaad48ebd137fe1d8b",
                                    "patch": "@@ -823,6 +823,7 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n ### 18.8b0\n \n * fix parsing of `__future__` imports with renames (#389)\n+* fix scope of `# fmt: off` when directly preceding `yield` and other nodes (#385)\n \n ### 18.6b4\n "
                                },
                                {
                                    "sha": "c9b8be975d239b30cc2ecf4bc7723c60a34ce13e",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 13,
                                    "changes": 14,
                                    "blob_url": "https://github.com/psf/black/blob/e94a41f92a568706700522aaad48ebd137fe1d8b/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/e94a41f92a568706700522aaad48ebd137fe1d8b/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=e94a41f92a568706700522aaad48ebd137fe1d8b",
                                    "patch": "@@ -797,18 +797,6 @@ def show(cls, code: Union[str, Leaf, Node]) -> None:\n     syms.testlist_gexp,\n     syms.testlist_star_expr,\n }\n-SURROUNDED_BY_BRACKETS = {\n-    syms.typedargslist,\n-    syms.arglist,\n-    syms.subscriptlist,\n-    syms.vfplist,\n-    syms.import_as_names,\n-    syms.yield_expr,\n-    syms.testlist_gexp,\n-    syms.testlist_star_expr,\n-    syms.listmaker,\n-    syms.dictsetmaker,\n-}\n TEST_DESCENDANTS = {\n     syms.test,\n     syms.lambdef,\n@@ -1853,7 +1841,7 @@ def container_of(leaf: Leaf) -> LN:\n         if parent.type == syms.file_input:\n             break\n \n-        if parent.type in SURROUNDED_BY_BRACKETS:\n+        if parent.prev_sibling is not None and parent.prev_sibling.type in BRACKETS:\n             break\n \n         container = parent"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "35f2889c4a2fdf3842afe1f8d57cd87ba1bae203",
                                    "filename": "tests/data/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 72,
                                    "deletions": 0,
                                    "changes": 72,
                                    "blob_url": "https://github.com/psf/black/blob/e94a41f92a568706700522aaad48ebd137fe1d8b/tests%2Fdata%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/e94a41f92a568706700522aaad48ebd137fe1d8b/tests%2Fdata%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtonoff.py?ref=e94a41f92a568706700522aaad48ebd137fe1d8b",
                                    "patch": "@@ -48,6 +48,39 @@ def spaces2(result= _core.Value(None)):\n     # fmt: off\n     key: 'value',\n }\n+\n+def subscriptlist():\n+    atom[\n+        # fmt: off\n+        'some big and',\n+        'complex subscript',\n+        # fmt: on\n+        goes + here, andhere,\n+       ]\n+\n+def import_as_names():\n+    # fmt: off\n+    from hello import a,        b\n+    'unformatted'\n+    # fmt: on\n+\n+def testlist_star_expr():\n+    # fmt: off\n+    a , b = *hello\n+    'unformatted'\n+    # fmt: on\n+\n+def yield_expr():\n+    # fmt: off\n+    yield hello\n+    'unformatted'\n+    # fmt: on\n+    'formatted'\n+    # fmt: off\n+    ( yield hello )\n+    'unformatted'\n+    # fmt: on\n+\n def example(session):\n     # fmt: off\n     result = session\\\n@@ -142,6 +175,7 @@ def single_literal_yapf_disable():\n     xxxxxxxxxx_xxxxxxxxxxx_xxxxxxx_xxxxxxxxx=5\n )\n # fmt: off\n+yield  'hello'\n # No formatting to the end of the file\n l=[1,2,3]\n d={'a':1,\n@@ -219,6 +253,43 @@ def spaces2(result=_core.Value(None)):\n }\n \n \n+def subscriptlist():\n+    atom[\n+        # fmt: off\n+        'some big and',\n+        'complex subscript',\n+        # fmt: on\n+        goes + here,\n+        andhere,\n+    ]\n+\n+\n+def import_as_names():\n+    # fmt: off\n+    from hello import a,        b\n+    'unformatted'\n+    # fmt: on\n+\n+\n+def testlist_star_expr():\n+    # fmt: off\n+    a , b = *hello\n+    'unformatted'\n+    # fmt: on\n+\n+\n+def yield_expr():\n+    # fmt: off\n+    yield hello\n+    'unformatted'\n+    # fmt: on\n+    \"formatted\"\n+    # fmt: off\n+    ( yield hello )\n+    'unformatted'\n+    # fmt: on\n+\n+\n def example(session):\n     # fmt: off\n     result = session\\\n@@ -327,6 +398,7 @@ def single_literal_yapf_disable():\n     xxxxxxxxxx_xxxxxxxxxxx_xxxxxxx_xxxxxxxxx=5,\n )\n # fmt: off\n+yield  'hello'\n # No formatting to the end of the file\n l=[1,2,3]\n d={'a':1,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 371,
                "created_at": "2018-06-21T20:28:01Z",
                "closed_at": "2018-06-21T21:03:19Z",
                "title": "Freeze on multiple comments directly preceding `# fmt: off`",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "d93e72680625a100c7c5701280f1bcf83124ae40",
                        "commit_date": "2018-06-21T21:03:01Z",
                        "parents": "98b6c887da891eb1a4d9401a13ec12f9c5e7f7eb",
                        "stat": {
                            "total": 1,
                            "additions": 15,
                            "deletions": 14,
                            "files": [
                                {
                                    "sha": "96d169a2118d3ad237e60b6fe51713331dafbfb7",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/psf/black/blob/d93e72680625a100c7c5701280f1bcf83124ae40/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/d93e72680625a100c7c5701280f1bcf83124ae40/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=d93e72680625a100c7c5701280f1bcf83124ae40",
                                    "patch": "@@ -820,6 +820,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ## Change Log\n \n+### 18.6b4\n+\n+* hotfix: don't freeze when multiple comments directly precede `# fmt: off` (#371)\n+\n+\n ### 18.6b3\n \n * typing stub files (`.pyi`) now have blank lines added after constants (#340)"
                                },
                                {
                                    "sha": "2d23298e9c9ec65781f5c02f96794181bd9972d7",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/d93e72680625a100c7c5701280f1bcf83124ae40/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/d93e72680625a100c7c5701280f1bcf83124ae40/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=d93e72680625a100c7c5701280f1bcf83124ae40",
                                    "patch": "@@ -2608,7 +2608,7 @@ def convert_one_fmt_off_pair(node: Node) -> bool:\n                 )\n                 return True\n \n-            previous_consumed += comment.consumed\n+            previous_consumed = comment.consumed\n \n     return False\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b2e9fc0ad97b755d366faad076a3e67721f02cb8",
                                    "filename": "tests/data/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/d93e72680625a100c7c5701280f1bcf83124ae40/tests%2Fdata%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/d93e72680625a100c7c5701280f1bcf83124ae40/tests%2Fdata%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtonoff.py?ref=d93e72680625a100c7c5701280f1bcf83124ae40",
                                    "patch": "@@ -11,6 +11,10 @@\n                          Y, Z)\n # fmt: on\n f'trigger 3.6 mode'\n+# Comment 1\n+\n+# Comment 2\n+\n # fmt: off\n def func_no_args():\n   a; b; c\n@@ -159,6 +163,10 @@ def single_literal_yapf_disable():\n                          Y, Z)\n # fmt: on\n f\"trigger 3.6 mode\"\n+# Comment 1\n+\n+# Comment 2\n+\n # fmt: off\n def func_no_args():\n   a; b; c"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run timeout 30s python setup.py test",
                "testStepsFull": "pipenv runtimeout 30s python setup.py test"
            },
            {
                "id": 335,
                "created_at": "2018-06-10T06:12:49Z",
                "closed_at": "2018-06-20T00:37:04Z",
                "title": "fmt: off is ignored after first function",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                        "commit_date": "2018-06-20T00:32:41Z",
                        "parents": "8a8c58252cc023ae250d6febd24f50a8166450d4",
                        "stat": {
                            "total": 179,
                            "additions": 283,
                            "deletions": 104,
                            "files": [
                                {
                                    "sha": "716a14edfb07921586998d711c9d35b5fc8cb104",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/df2ae3bbe6c45298aabb6c04e85cb353205626f1/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/df2ae3bbe6c45298aabb6c04e85cb353205626f1/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                                    "patch": "@@ -824,7 +824,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * typing stub files (`.pyi`) now have blank lines added after constants (#340)\n \n-* `# fmt: off` and `# fmt: on` now work also within bracket pairs (#329)\n+* `# fmt: off` and `# fmt: on` are now much more dependable:\n+\n+  * they now work also within bracket pairs (#329)\n+\n+  * they now correctly work across function/class boundaries (#335)\n \n * fixed improper formatting of f-strings with quotes inside interpolated\n   expressions (#322)"
                                },
                                {
                                    "sha": "7e39c92009d2aefcf3b2910f9dce1fcca8b2cbe2",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 60,
                                    "deletions": 178,
                                    "changes": 238,
                                    "blob_url": "https://github.com/psf/black/blob/df2ae3bbe6c45298aabb6c04e85cb353205626f1/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/df2ae3bbe6c45298aabb6c04e85cb353205626f1/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                                    "patch": "@@ -29,7 +29,6 @@\n     Sequence,\n     Set,\n     Tuple,\n-    Type,\n     TypeVar,\n     Union,\n     cast,\n@@ -90,34 +89,6 @@ class CannotSplit(Exception):\n     \"\"\"\n \n \n-class FormatError(Exception):\n-    \"\"\"Base exception for `# fmt: on` and `# fmt: off` handling.\n-\n-    It holds the number of bytes of the prefix consumed before the format\n-    control comment appeared.\n-    \"\"\"\n-\n-    def __init__(self, consumed: int) -> None:\n-        super().__init__(consumed)\n-        self.consumed = consumed\n-\n-    def trim_prefix(self, leaf: Leaf) -> None:\n-        leaf.prefix = leaf.prefix[self.consumed :]\n-\n-    def leaf_from_consumed(self, leaf: Leaf) -> Leaf:\n-        \"\"\"Returns a new Leaf from the consumed part of the prefix.\"\"\"\n-        unformatted_prefix = leaf.prefix[: self.consumed]\n-        return Leaf(token.NEWLINE, unformatted_prefix)\n-\n-\n-class FormatOn(FormatError):\n-    \"\"\"Found a comment like `# fmt: on` in the file.\"\"\"\n-\n-\n-class FormatOff(FormatError):\n-    \"\"\"Found a comment like `# fmt: off` in the file.\"\"\"\n-\n-\n class WriteBack(Enum):\n     NO = 0\n     YES = 1\n@@ -759,13 +730,15 @@ def visit_default(self, node: LN) -> Iterator[T]:\n             out(f\" {node.value!r}\", fg=\"blue\", bold=False)\n \n     @classmethod\n-    def show(cls, code: str) -> None:\n+    def show(cls, code: Union[str, Leaf, Node]) -> None:\n         \"\"\"Pretty-print the lib2to3 AST of a given string of `code`.\n \n         Convenience method for debugging.\n         \"\"\"\n         v: DebugVisitor[None] = DebugVisitor()\n-        list(v.visit(lib2to3_parse(code)))\n+        if isinstance(code, str):\n+            code = lib2to3_parse(code)\n+        list(v.visit(code))\n \n \n KEYWORDS = set(keyword.kwlist)\n@@ -1306,55 +1279,6 @@ def __bool__(self) -> bool:\n         return bool(self.leaves or self.comments)\n \n \n-class UnformattedLines(Line):\n-    \"\"\"Just like :class:`Line` but stores lines which aren't reformatted.\"\"\"\n-\n-    def append(self, leaf: Leaf, preformatted: bool = True) -> None:\n-        \"\"\"Just add a new `leaf` to the end of the lines.\n-\n-        The `preformatted` argument is ignored.\n-\n-        Keeps track of indentation `depth`, which is useful when the user\n-        says `# fmt: on`. Otherwise, doesn't do anything with the `leaf`.\n-        \"\"\"\n-        try:\n-            list(generate_comments(leaf))\n-        except FormatOn as f_on:\n-            self.leaves.append(f_on.leaf_from_consumed(leaf))\n-            raise\n-\n-        self.leaves.append(leaf)\n-        if leaf.type == token.INDENT:\n-            self.depth += 1\n-        elif leaf.type == token.DEDENT:\n-            self.depth -= 1\n-\n-    def __str__(self) -> str:\n-        \"\"\"Render unformatted lines from leaves which were added with `append()`.\n-\n-        `depth` is not used for indentation in this case.\n-        \"\"\"\n-        if not self:\n-            return \"\\n\"\n-\n-        res = \"\"\n-        for leaf in self.leaves:\n-            res += str(leaf)\n-        return res\n-\n-    def append_comment(self, comment: Leaf) -> bool:\n-        \"\"\"Not implemented in this class. Raises `NotImplementedError`.\"\"\"\n-        raise NotImplementedError(\"Unformatted lines don't store comments separately.\")\n-\n-    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n-        \"\"\"Does nothing and returns False.\"\"\"\n-        return False\n-\n-    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n-        \"\"\"Does nothing and returns False.\"\"\"\n-        return False\n-\n-\n @dataclass\n class EmptyLineTracker:\n     \"\"\"Provides a stateful method that returns the number of potential extra\n@@ -1376,9 +1300,6 @@ def maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\n         This is for separating `def`, `async def` and `class` with extra empty\n         lines (two on module-level).\n         \"\"\"\n-        if isinstance(current_line, UnformattedLines):\n-            return 0, 0\n-\n         before, after = self._maybe_empty_lines(current_line)\n         before -= self.previous_after\n         self.previous_after = after\n@@ -1482,7 +1403,7 @@ class LineGenerator(Visitor[Line]):\n     current_line: Line = Factory(Line)\n     remove_u_prefix: bool = False\n \n-    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n+    def line(self, indent: int = 0) -> Iterator[Line]:\n         \"\"\"Generate a line.\n \n         If the line is empty, only emit if it makes sense.\n@@ -1491,67 +1412,39 @@ def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:\n         If any lines were generated, set up a new current_line.\n         \"\"\"\n         if not self.current_line:\n-            if self.current_line.__class__ == type:\n-                self.current_line.depth += indent\n-            else:\n-                self.current_line = type(depth=self.current_line.depth + indent)\n+            self.current_line.depth += indent\n             return  # Line is empty, don't emit. Creating a new one unnecessary.\n \n         complete_line = self.current_line\n-        self.current_line = type(depth=complete_line.depth + indent)\n+        self.current_line = Line(depth=complete_line.depth + indent)\n         yield complete_line\n \n-    def visit(self, node: LN) -> Iterator[Line]:\n-        \"\"\"Main method to visit `node` and its children.\n-\n-        Yields :class:`Line` objects.\n-        \"\"\"\n-        if isinstance(self.current_line, UnformattedLines):\n-            # File contained `# fmt: off`\n-            yield from self.visit_unformatted(node)\n-\n-        else:\n-            yield from super().visit(node)\n-\n     def visit_default(self, node: LN) -> Iterator[Line]:\n         \"\"\"Default `visit_*()` implementation. Recurses to children of `node`.\"\"\"\n         if isinstance(node, Leaf):\n             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()\n-            try:\n-                for comment in generate_comments(node):\n-                    if any_open_brackets:\n-                        # any comment within brackets is subject to splitting\n-                        self.current_line.append(comment)\n-                    elif comment.type == token.COMMENT:\n-                        # regular trailing comment\n-                        self.current_line.append(comment)\n-                        yield from self.line()\n-\n-                    else:\n-                        # regular standalone comment\n-                        yield from self.line()\n-\n-                        self.current_line.append(comment)\n-                        yield from self.line()\n-\n-            except FormatOff as f_off:\n-                f_off.trim_prefix(node)\n-                yield from self.line(type=UnformattedLines)\n-                yield from self.visit(node)\n-\n-            except FormatOn as f_on:\n-                # This only happens here if somebody says \"fmt: on\" multiple\n-                # times in a row.\n-                f_on.trim_prefix(node)\n-                yield from self.visit_default(node)\n+            for comment in generate_comments(node):\n+                if any_open_brackets:\n+                    # any comment within brackets is subject to splitting\n+                    self.current_line.append(comment)\n+                elif comment.type == token.COMMENT:\n+                    # regular trailing comment\n+                    self.current_line.append(comment)\n+                    yield from self.line()\n \n-            else:\n-                normalize_prefix(node, inside_brackets=any_open_brackets)\n-                if self.normalize_strings and node.type == token.STRING:\n-                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n-                    normalize_string_quotes(node)\n-                if node.type not in WHITESPACE:\n-                    self.current_line.append(node)\n+                else:\n+                    # regular standalone comment\n+                    yield from self.line()\n+\n+                    self.current_line.append(comment)\n+                    yield from self.line()\n+\n+            normalize_prefix(node, inside_brackets=any_open_brackets)\n+            if self.normalize_strings and node.type == token.STRING:\n+                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)\n+                normalize_string_quotes(node)\n+            if node.type not in WHITESPACE:\n+                self.current_line.append(node)\n         yield from super().visit_default(node)\n \n     def visit_INDENT(self, node: Node) -> Iterator[Line]:\n@@ -1648,23 +1541,10 @@ def visit_ENDMARKER(self, leaf: Leaf) -> Iterator[Line]:\n         yield from self.visit_default(leaf)\n         yield from self.line()\n \n-    def visit_unformatted(self, node: LN) -> Iterator[Line]:\n-        \"\"\"Used when file contained a `# fmt: off`.\"\"\"\n-        if isinstance(node, Node):\n-            for child in node.children:\n-                yield from self.visit(child)\n-\n-        else:\n-            try:\n-                self.current_line.append(node)\n-            except FormatOn as f_on:\n-                f_on.trim_prefix(node)\n-                yield from self.line()\n-                yield from self.visit(node)\n-\n-            if node.type == token.ENDMARKER:\n-                # somebody decided not to put a final `# fmt: on`\n-                yield from self.line()\n+    def visit_STANDALONE_COMMENT(self, leaf: Leaf) -> Iterator[Line]:\n+        if not self.current_line.bracket_tracker.any_open_brackets():\n+            yield from self.line()\n+        yield from self.visit_default(leaf)\n \n     def __attrs_post_init__(self) -> None:\n         \"\"\"You are in a twisty little maze of passages.\"\"\"\n@@ -1969,6 +1849,9 @@ def container_of(leaf: Leaf) -> LN:\n         if parent.children[0].prefix != same_prefix:\n             break\n \n+        if parent.type == syms.file_input:\n+            break\n+\n         if parent.type in SURROUNDED_BY_BRACKETS:\n             break\n \n@@ -2106,16 +1989,6 @@ def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     \"\"\"\n     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):\n         yield Leaf(pc.type, pc.value, prefix=\"\\n\" * pc.newlines)\n-        if pc.value in FMT_ON:\n-            raise FormatOn(pc.consumed)\n-\n-        if pc.value in FMT_OFF:\n-            if pc.type == STANDALONE_COMMENT:\n-                raise FormatOff(pc.consumed)\n-\n-            prev = preceding_leaf(leaf)\n-            if not prev or prev.type in WHITESPACE:  # standalone comment in disguise\n-                raise FormatOff(pc.consumed)\n \n \n @dataclass\n@@ -2188,7 +2061,7 @@ def split_line(\n     If `py36` is True, splitting may generate syntax that is only compatible\n     with Python 3.6 and later.\n     \"\"\"\n-    if isinstance(line, UnformattedLines) or line.is_comment:\n+    if line.is_comment:\n         yield line\n         return\n \n@@ -2680,28 +2553,29 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n \n \n def normalize_fmt_off(node: Node) -> None:\n-    \"\"\"Allow `# fmt: off`/`# fmt: on` within bracket pairs.\n-\n-    Ignores `# fmt: off` and `# fmt: on` outside of brackets.\n-\n-    Raises :exc:`SyntaxError` if no matching `# fmt: on` is found for a `# fmt: off`\n-    given inside brackets.\n-    \"\"\"\n+    \"\"\"Convert content between `# fmt: off`/`# fmt: on` into standalone comments.\"\"\"\n     try_again = True\n     while try_again:\n-        try_again = hide_fmt_off(node)\n+        try_again = convert_one_fmt_off_pair(node)\n \n \n-def hide_fmt_off(node: Node) -> bool:\n-    bt = BracketTracker()\n-    for leaf in node.leaves():\n-        bt.mark(leaf)\n-        if bt.depth == 0:\n-            continue\n+def convert_one_fmt_off_pair(node: Node) -> bool:\n+    \"\"\"Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.\n \n+    Returns True if a pair was converted.\n+    \"\"\"\n+    for leaf in node.leaves():\n         previous_consumed = 0\n         for comment in list_comments(leaf.prefix, is_endmarker=False):\n             if comment.value in FMT_OFF:\n+                # We only want standalone comments. If there's no previous leaf or\n+                # the previous leaf is indentation, it's a standalone comment in\n+                # disguise.\n+                if comment.type != STANDALONE_COMMENT:\n+                    prev = preceding_leaf(leaf)\n+                    if prev and prev.type not in WHITESPACE:\n+                        continue\n+\n                 ignored_nodes = list(generate_ignored_nodes(leaf))\n                 first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n                 parent = first.parent\n@@ -2710,6 +2584,10 @@ def hide_fmt_off(node: Node) -> bool:\n                 hidden_value = (\n                     comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n                 )\n+                if hidden_value.endswith(\"\\n\"):\n+                    # That happens when one of the `ignored_nodes` ended with a NEWLINE\n+                    # leaf (possibly followed by a DEDENT).\n+                    hidden_value = hidden_value[:-1]\n                 first_idx = None\n                 for ignored in ignored_nodes:\n                     index = ignored.remove()\n@@ -2733,8 +2611,12 @@ def hide_fmt_off(node: Node) -> bool:\n \n \n def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n+    \"\"\"Starting from the container of `leaf`, generate all leaves until `# fmt: on`.\n+\n+    Stops at the end of the block.\n+    \"\"\"\n     container: Optional[LN] = container_of(leaf)\n-    while container is not None:\n+    while container is not None and container.type != token.ENDMARKER:\n         for comment in list_comments(container.prefix, is_endmarker=False):\n             if comment.value in FMT_ON:\n                 return"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "43095060a2417a1fccce55942dd1a1221e4ac288",
                                    "filename": "tests/data/fmtonoff2.py",
                                    "status": "added",
                                    "additions": 31,
                                    "deletions": 0,
                                    "changes": 31,
                                    "blob_url": "https://github.com/psf/black/blob/df2ae3bbe6c45298aabb6c04e85cb353205626f1/tests%2Fdata%2Ffmtonoff2.py",
                                    "raw_url": "https://github.com/psf/black/raw/df2ae3bbe6c45298aabb6c04e85cb353205626f1/tests%2Fdata%2Ffmtonoff2.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtonoff2.py?ref=df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                                    "patch": "@@ -0,0 +1,31 @@\n+import pytest\n+\n+TmSt = 1\n+TmEx = 2\n+\n+# fmt: off\n+\n+# Test data:\n+#   Position, Volume, State, TmSt/TmEx/None, [call, [arg1...]]\n+\n+@pytest.mark.parametrize('test', [\n+\n+    # Test don't manage the volume\n+    [\n+        ('stuff', 'in')\n+    ],\n+])\n+def test_fader(test):\n+    pass\n+\n+def check_fader(test):\n+    pass\n+\n+def test_calculate_fades():\n+    calcs = [\n+        # one is zero/none\n+        (0, 4, 0, 0, 10,        0, 0, 6, 10),\n+        (None, 4, 0, 0, 10,     0, 0, 6, 10),\n+    ]\n+\n+# fmt: on"
                                },
                                {
                                    "sha": "6638dc4f119f83ab4c2cd3e2e14272362f803813",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/df2ae3bbe6c45298aabb6c04e85cb353205626f1/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/df2ae3bbe6c45298aabb6c04e85cb353205626f1/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                                    "patch": "@@ -400,6 +400,14 @@ def test_fmtonoff(self) -> None:\n         black.assert_equivalent(source, actual)\n         black.assert_stable(source, actual, line_length=ll)\n \n+    @patch(\"black.dump_to_file\", dump_to_stderr)\n+    def test_fmtonoff2(self) -> None:\n+        source, expected = read_data(\"fmtonoff2\")\n+        actual = fs(source)\n+        self.assertFormatEqual(expected, actual)\n+        black.assert_equivalent(source, actual)\n+        black.assert_stable(source, actual, line_length=ll)\n+\n     @patch(\"black.dump_to_file\", dump_to_stderr)\n     def test_remove_empty_parentheses_after_class(self) -> None:\n         source, expected = read_data(\"class_blank_parentheses\")"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 334,
                "created_at": "2018-06-10T05:40:18Z",
                "closed_at": "2018-06-20T03:46:26Z",
                "title": "Internal error + error 12/11 with multiple format on/off",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "e1ef57a29e03fb49688d27ed72c58ce80809f50f",
                        "commit_date": "2018-06-20T03:44:47Z",
                        "parents": "df2ae3bbe6c45298aabb6c04e85cb353205626f1",
                        "stat": {
                            "total": 19,
                            "additions": 51,
                            "deletions": 32,
                            "files": [
                                {
                                    "sha": "39510390acfcb07523ef3551c1c2152dbc071cbb",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/e1ef57a29e03fb49688d27ed72c58ce80809f50f/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/e1ef57a29e03fb49688d27ed72c58ce80809f50f/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=e1ef57a29e03fb49688d27ed72c58ce80809f50f",
                                    "patch": "@@ -830,6 +830,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n   * they now correctly work across function/class boundaries (#335)\n \n+  * they now work when an indentation block starts with empty lines or misaligned\n+    comments (#334)\n+\n * fixed improper formatting of f-strings with quotes inside interpolated\n   expressions (#322)\n "
                                },
                                {
                                    "sha": "a51ffc30b9fe9195c1ff936d403387e7fe8b4110",
                                    "filename": "blib2to3/pgen2/driver.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 11,
                                    "changes": 17,
                                    "blob_url": "https://github.com/psf/black/blob/e1ef57a29e03fb49688d27ed72c58ce80809f50f/blib2to3%2Fpgen2%2Fdriver.py",
                                    "raw_url": "https://github.com/psf/black/raw/e1ef57a29e03fb49688d27ed72c58ce80809f50f/blib2to3%2Fpgen2%2Fdriver.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Fdriver.py?ref=e1ef57a29e03fb49688d27ed72c58ce80809f50f",
                                    "patch": "@@ -70,24 +70,19 @@ def parse_tokens(self, tokens, debug=False):\n             if debug:\n                 self.logger.debug(\"%s %r (prefix=%r)\",\n                                   token.tok_name[type], value, prefix)\n-            if type in {token.INDENT, token.DEDENT}:\n-                _prefix = prefix\n+            if type == token.INDENT:\n+                indent_columns.append(len(value))\n+                _prefix = prefix + value\n                 prefix = \"\"\n-            if type == token.DEDENT:\n+                value = \"\"\n+            elif type == token.DEDENT:\n                 _indent_col = indent_columns.pop()\n-                prefix, _prefix = self._partially_consume_prefix(_prefix, _indent_col)\n+                prefix, _prefix = self._partially_consume_prefix(prefix, _indent_col)\n             if p.addtoken(type, value, (prefix, start)):\n                 if debug:\n                     self.logger.debug(\"Stop.\")\n                 break\n             prefix = \"\"\n-            if type == token.INDENT:\n-                indent_columns.append(len(value))\n-                if _prefix.startswith(value):\n-                    # Don't double-indent.  Since we're delaying the prefix that\n-                    # would normally belong to INDENT, we need to put the value\n-                    # at the end versus at the beginning.\n-                    _prefix = _prefix[len(value):] + value\n             if type in {token.INDENT, token.DEDENT}:\n                 prefix = _prefix\n             lineno, column = end"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "fa60010d421da59e26337be14285411fee73b8a7",
                                    "filename": "tests/data/debug_visitor.out",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 8,
                                    "changes": 22,
                                    "blob_url": "https://github.com/psf/black/blob/e1ef57a29e03fb49688d27ed72c58ce80809f50f/tests%2Fdata%2Fdebug_visitor.out",
                                    "raw_url": "https://github.com/psf/black/raw/e1ef57a29e03fb49688d27ed72c58ce80809f50f/tests%2Fdata%2Fdebug_visitor.out",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Fdebug_visitor.out?ref=e1ef57a29e03fb49688d27ed72c58ce80809f50f",
                                    "patch": "@@ -36,10 +36,11 @@ file_input\n         NEWLINE\n  '\\n'\n         INDENT\n- '    '\n+ ''\n         simple_stmt\n           expr_stmt\n             NAME\n+ '    '\n  'tree_depth'\n             annassign\n               COLON\n@@ -109,10 +110,11 @@ file_input\n             NEWLINE\n  '\\n'\n             INDENT\n- '        '\n+ ''\n             simple_stmt\n               expr_stmt\n                 NAME\n+ '        '\n  'indent'\n                 EQUAL\n  ' '\n@@ -184,10 +186,11 @@ file_input\n                 NEWLINE\n  '\\n'\n                 INDENT\n- '            '\n+ ''\n                 simple_stmt\n                   expr_stmt\n                     NAME\n+ '            '\n  '_type'\n                     EQUAL\n  ' '\n@@ -297,10 +300,11 @@ file_input\n                     NEWLINE\n  '\\n'\n                     INDENT\n- '                '\n+ ''\n                     simple_stmt\n                       yield_expr\n                         NAME\n+ '                '\n  'yield'\n                         yield_arg\n                           NAME\n@@ -410,10 +414,11 @@ file_input\n                 NEWLINE\n  '\\n'\n                 INDENT\n- '            '\n+ ''\n                 simple_stmt\n                   expr_stmt\n                     NAME\n+ '            '\n  '_type'\n                     EQUAL\n  ' '\n@@ -542,11 +547,11 @@ file_input\n                     NEWLINE\n  '\\n'\n                     INDENT\n- '                '\n+ ''\n                     simple_stmt\n                       power\n                         NAME\n- \"# We don't have to handle prefixes for `Node` objects since\\n                # that delegates to the first child anyway.\\n                \"\n+ \"                # We don't have to handle prefixes for `Node` objects since\\n                # that delegates to the first child anyway.\\n                \"\n  'out'\n                         trailer\n                           LPAR\n@@ -699,9 +704,10 @@ file_input\n               NEWLINE\n  '\\n'\n               INDENT\n- '        '\n+ ''\n               simple_stmt\n                 STRING\n+ '        '\n  '\"\"\"Pretty-prints a given string of `code`.\\n\\n        Convenience method for debugging.\\n        \"\"\"'\n                 NEWLINE\n  '\\n'"
                                },
                                {
                                    "sha": "e8657c749b3e69e909b51978d22f443e617e5122",
                                    "filename": "tests/data/fmtonoff2.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/psf/black/blob/e1ef57a29e03fb49688d27ed72c58ce80809f50f/tests%2Fdata%2Ffmtonoff2.py",
                                    "raw_url": "https://github.com/psf/black/raw/e1ef57a29e03fb49688d27ed72c58ce80809f50f/tests%2Fdata%2Ffmtonoff2.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtonoff2.py?ref=e1ef57a29e03fb49688d27ed72c58ce80809f50f",
                                    "patch": "@@ -19,8 +19,17 @@ def test_fader(test):\n     pass\n \n def check_fader(test):\n+\n+    pass\n+\n+def verify_fader(test):\n+  # misaligned comment\n     pass\n \n+def verify_fader(test):\n+    \"\"\"Hey, ho.\"\"\"\n+    assert test.passed()\n+\n def test_calculate_fades():\n     calcs = [\n         # one is zero/none"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 329,
                "created_at": "2018-06-09T17:49:06Z",
                "closed_at": "2018-06-19T05:38:14Z",
                "title": "fmt:on/off placed asymmetrically ends up with a cryptic exception",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "8a8c58252cc023ae250d6febd24f50a8166450d4",
                        "commit_date": "2018-06-19T05:37:46Z",
                        "parents": "013cb2b374cc6d649d25bc35ba592a8f88f484df",
                        "stat": {
                            "total": 7,
                            "additions": 178,
                            "deletions": 171,
                            "files": [
                                {
                                    "sha": "67ff18aff76d57c793b54614e9bfa4debb5f5d22",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/8a8c58252cc023ae250d6febd24f50a8166450d4/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/8a8c58252cc023ae250d6febd24f50a8166450d4/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=8a8c58252cc023ae250d6febd24f50a8166450d4",
                                    "patch": "@@ -824,6 +824,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * typing stub files (`.pyi`) now have blank lines added after constants (#340)\n \n+* `# fmt: off` and `# fmt: on` now work also within bracket pairs (#329)\n+\n * fixed improper formatting of f-strings with quotes inside interpolated\n   expressions (#322)\n "
                                },
                                {
                                    "sha": "7682f7c393776a1b75be292e36f6620fed8925a3",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 102,
                                    "deletions": 1,
                                    "changes": 103,
                                    "blob_url": "https://github.com/psf/black/blob/8a8c58252cc023ae250d6febd24f50a8166450d4/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/8a8c58252cc023ae250d6febd24f50a8166450d4/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=8a8c58252cc023ae250d6febd24f50a8166450d4",
                                    "patch": "@@ -628,6 +628,7 @@ def format_str(\n     is_pyi = bool(mode & FileMode.PYI)\n     py36 = bool(mode & FileMode.PYTHON36) or is_python36(src_node)\n     normalize_strings = not bool(mode & FileMode.NO_STRING_NORMALIZATION)\n+    normalize_fmt_off(src_node)\n     lines = LineGenerator(\n         remove_u_prefix=py36 or \"unicode_literals\" in future_imports,\n         is_pyi=is_pyi,\n@@ -781,6 +782,7 @@ def show(cls, code: str) -> None:\n     syms.classdef,\n }\n STANDALONE_COMMENT = 153\n+token.tok_name[STANDALONE_COMMENT] = \"STANDALONE_COMMENT\"\n LOGIC_OPERATORS = {\"and\", \"or\"}\n COMPARATORS = {\n     token.LESS,\n@@ -821,6 +823,18 @@ def show(cls, code: str) -> None:\n     syms.testlist_gexp,\n     syms.testlist_star_expr,\n }\n+SURROUNDED_BY_BRACKETS = {\n+    syms.typedargslist,\n+    syms.arglist,\n+    syms.subscriptlist,\n+    syms.vfplist,\n+    syms.import_as_names,\n+    syms.yield_expr,\n+    syms.testlist_gexp,\n+    syms.testlist_star_expr,\n+    syms.listmaker,\n+    syms.dictsetmaker,\n+}\n TEST_DESCENDANTS = {\n     syms.test,\n     syms.lambdef,\n@@ -1940,6 +1954,28 @@ def child_towards(ancestor: Node, descendant: LN) -> Optional[LN]:\n     return node\n \n \n+def container_of(leaf: Leaf) -> LN:\n+    \"\"\"Return `leaf` or one of its ancestors that is the topmost container of it.\n+\n+    By \"container\" we mean a node where `leaf` is the very first child.\n+    \"\"\"\n+    same_prefix = leaf.prefix\n+    container: LN = leaf\n+    while container:\n+        parent = container.parent\n+        if parent is None:\n+            break\n+\n+        if parent.children[0].prefix != same_prefix:\n+            break\n+\n+        if parent.type in SURROUNDED_BY_BRACKETS:\n+            break\n+\n+        container = parent\n+    return container\n+\n+\n def is_split_after_delimiter(leaf: Leaf, previous: Leaf = None) -> int:\n     \"\"\"Return the priority of the `leaf` delimiter, given a line break after it.\n \n@@ -2091,7 +2127,7 @@ class ProtoComment:\n \n \n @lru_cache(maxsize=4096)\n-def list_comments(prefix: str, is_endmarker: bool) -> List[ProtoComment]:\n+def list_comments(prefix: str, *, is_endmarker: bool) -> List[ProtoComment]:\n     result: List[ProtoComment] = []\n     if not prefix or \"#\" not in prefix:\n         return result\n@@ -2643,6 +2679,71 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after\n \n \n+def normalize_fmt_off(node: Node) -> None:\n+    \"\"\"Allow `# fmt: off`/`# fmt: on` within bracket pairs.\n+\n+    Ignores `# fmt: off` and `# fmt: on` outside of brackets.\n+\n+    Raises :exc:`SyntaxError` if no matching `# fmt: on` is found for a `# fmt: off`\n+    given inside brackets.\n+    \"\"\"\n+    try_again = True\n+    while try_again:\n+        try_again = hide_fmt_off(node)\n+\n+\n+def hide_fmt_off(node: Node) -> bool:\n+    bt = BracketTracker()\n+    for leaf in node.leaves():\n+        bt.mark(leaf)\n+        if bt.depth == 0:\n+            continue\n+\n+        previous_consumed = 0\n+        for comment in list_comments(leaf.prefix, is_endmarker=False):\n+            if comment.value in FMT_OFF:\n+                ignored_nodes = list(generate_ignored_nodes(leaf))\n+                first = ignored_nodes[0]  # Can be a container node with the `leaf`.\n+                parent = first.parent\n+                prefix = first.prefix\n+                first.prefix = prefix[comment.consumed :]\n+                hidden_value = (\n+                    comment.value + \"\\n\" + \"\".join(str(n) for n in ignored_nodes)\n+                )\n+                first_idx = None\n+                for ignored in ignored_nodes:\n+                    index = ignored.remove()\n+                    if first_idx is None:\n+                        first_idx = index\n+                assert parent is not None, \"INTERNAL ERROR: fmt: on/off handling (1)\"\n+                assert first_idx is not None, \"INTERNAL ERROR: fmt: on/off handling (2)\"\n+                parent.insert_child(\n+                    first_idx,\n+                    Leaf(\n+                        STANDALONE_COMMENT,\n+                        hidden_value,\n+                        prefix=prefix[:previous_consumed] + \"\\n\" * comment.newlines,\n+                    ),\n+                )\n+                return True\n+\n+            previous_consumed += comment.consumed\n+\n+    return False\n+\n+\n+def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:\n+    container: Optional[LN] = container_of(leaf)\n+    while container is not None:\n+        for comment in list_comments(container.prefix, is_endmarker=False):\n+            if comment.value in FMT_ON:\n+                return\n+\n+        yield container\n+\n+        container = container.next_sibling\n+\n+\n def maybe_make_parens_invisible_in_atom(node: LN) -> bool:\n     \"\"\"If it's safe, make the parens in the atom `node` invisible, recursively.\"\"\"\n     if ("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "373e5c7090396e994b19e2048271ded0aad5bc5a",
                                    "filename": "tests/data/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 67,
                                    "deletions": 6,
                                    "changes": 73,
                                    "blob_url": "https://github.com/psf/black/blob/8a8c58252cc023ae250d6febd24f50a8166450d4/tests%2Fdata%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/8a8c58252cc023ae250d6febd24f50a8166450d4/tests%2Fdata%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fdata%2Ffmtonoff.py?ref=8a8c58252cc023ae250d6febd24f50a8166450d4",
                                    "patch": "@@ -40,6 +40,10 @@ def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h=\"\", i=r''\n def spaces_types(a: int = 1, b: tuple = (), c: list = [], d: dict = {}, e: bool = True, f: int = -1, g: int = 1 if False else 2, h: str = \"\", i: str = r''): ...\n def spaces2(result= _core.Value(None)):\n  ...\n+something = {\n+    # fmt: off\n+    key: 'value',\n+}\n def example(session):\n     # fmt: off\n     result = session\\\n@@ -78,17 +82,40 @@ def long_lines():\n             \\n?\n         )\n         $\n-        \"\"\", # fmt: off\n-        re.MULTILINE | re.VERBOSE\n+        \"\"\",\n+        # fmt: off\n+        re.MULTILINE|re.VERBOSE\n+        # fmt: on\n     )\n-    # fmt: on\n def single_literal_yapf_disable():\n     \"\"\"Black does not support this.\"\"\"\n     BAZ = {\n         (1, 2, 3, 4),\n         (5, 6, 7, 8),\n         (9, 10, 11, 12),\n     }  # yapf: disable\n+cfg.rule(\n+    \"Default\", \"address\",\n+    xxxx_xxxx=[\"xxx-xxxxxx-xxxxxxxxxx\"],\n+    xxxxxx=\"xx_xxxxx\", xxxxxxx=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n+    xxxxxxxxx_xxxx=True, xxxxxxxx_xxxxxxxxxx=False,\n+    xxxxxx_xxxxxx=2, xxxxxx_xxxxx_xxxxxxxx=70, xxxxxx_xxxxxx_xxxxx=True,\n+    # fmt: off\n+    xxxxxxx_xxxxxxxxxxxx={\n+        \"xxxxxxxx\": {\n+            \"xxxxxx\": False,\n+            \"xxxxxxx\": False,\n+            \"xxxx_xxxxxx\": \"xxxxx\",\n+        },\n+        \"xxxxxxxx-xxxxx\": {\n+            \"xxxxxx\": False,\n+            \"xxxxxxx\": True,\n+            \"xxxx_xxxxxx\": \"xxxxxx\",\n+        },\n+    },\n+    # fmt: on\n+    xxxxxxxxxx_xxxxxxxxxxx_xxxxxxx_xxxxxxxxx=5\n+)\n # fmt: off\n # No formatting to the end of the file\n l=[1,2,3]\n@@ -157,6 +184,12 @@ def spaces2(result=_core.Value(None)):\n     ...\n \n \n+something = {\n+    # fmt: off\n+    key: 'value',\n+}\n+\n+\n def example(session):\n     # fmt: off\n     result = session\\\n@@ -202,17 +235,45 @@ def long_lines():\n             \\n?\n         )\n         $\n-        \"\"\",  # fmt: off\n-        re.MULTILINE | re.VERBOSE,\n+        \"\"\",\n+        # fmt: off\n+        re.MULTILINE|re.VERBOSE\n+        # fmt: on\n     )\n-    # fmt: on\n \n \n def single_literal_yapf_disable():\n     \"\"\"Black does not support this.\"\"\"\n     BAZ = {(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)}  # yapf: disable\n \n \n+cfg.rule(\n+    \"Default\",\n+    \"address\",\n+    xxxx_xxxx=[\"xxx-xxxxxx-xxxxxxxxxx\"],\n+    xxxxxx=\"xx_xxxxx\",\n+    xxxxxxx=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n+    xxxxxxxxx_xxxx=True,\n+    xxxxxxxx_xxxxxxxxxx=False,\n+    xxxxxx_xxxxxx=2,\n+    xxxxxx_xxxxx_xxxxxxxx=70,\n+    xxxxxx_xxxxxx_xxxxx=True,\n+    # fmt: off\n+    xxxxxxx_xxxxxxxxxxxx={\n+        \"xxxxxxxx\": {\n+            \"xxxxxx\": False,\n+            \"xxxxxxx\": False,\n+            \"xxxx_xxxxxx\": \"xxxxx\",\n+        },\n+        \"xxxxxxxx-xxxxx\": {\n+            \"xxxxxx\": False,\n+            \"xxxxxxx\": True,\n+            \"xxxx_xxxxxx\": \"xxxxxx\",\n+        },\n+    },\n+    # fmt: on\n+    xxxxxxxxxx_xxxxxxxxxxx_xxxxxxx_xxxxxxxxx=5,\n+)\n # fmt: off\n # No formatting to the end of the file\n l=[1,2,3]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "ppipenv run ython setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 297,
                "created_at": "2018-06-03T20:22:21Z",
                "closed_at": "2018-06-05T04:28:37Z",
                "title": "Additional space after star unpacking",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "bbc09a4f013f2a584f143f3f5e3f76f6082367d4",
                        "commit_date": "2018-06-05T01:42:16Z",
                        "parents": "728e5a2f1ed16e2cfe0ca5586edac6c10da436c5",
                        "stat": {
                            "total": 1,
                            "additions": 6,
                            "deletions": 5,
                            "files": [
                                {
                                    "sha": "46c0907894d8a9b713f1899eb4dbdb818439f13b",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=bbc09a4f013f2a584f143f3f5e3f76f6082367d4",
                                    "patch": "@@ -768,6 +768,7 @@ def show(cls, code: str) -> None:\n     syms.dictsetmaker,\n     syms.listmaker,\n     syms.testlist_gexp,\n+    syms.testlist_star_expr,\n }\n TEST_DESCENDANTS = {\n     syms.test,"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "0d2ff68e4aa36bd6ce6606eca9f6c12f533f542a",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=bbc09a4f013f2a584f143f3f5e3f76f6082367d4",
                                    "patch": "@@ -147,7 +147,7 @@\n  slice[0:1:2]\n  slice[:]\n  slice[:-1]\n-@@ -133,107 +156,159 @@\n+@@ -133,108 +156,160 @@\n  numpy[-(c + 1) :, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]\n@@ -183,6 +183,7 @@\n  c = 1\n  d = (1,) + a + (2,)\n  e = (1,).count(1)\n+ f = 1, *range(10)\n -what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(vars_to_remove)\n -what_is_up_with_those_new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n -result = session.query(models.Customer.id).filter(models.Customer.account_id == account_id, models.Customer.email == email_address).order_by(models.Customer.id.asc(),).all()"
                                },
                                {
                                    "sha": "ea927a59f402589949e3dfaa416e9b7db47fbf37",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/bbc09a4f013f2a584f143f3f5e3f76f6082367d4/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=bbc09a4f013f2a584f143f3f5e3f76f6082367d4",
                                    "patch": "@@ -152,6 +152,7 @@\n c = 1\n d = (1,) + a + (2,)\n e = (1,).count(1)\n+f = 1, *range(10)\n what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(vars_to_remove)\n what_is_up_with_those_new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n result = session.query(models.Customer.id).filter(models.Customer.account_id == account_id, models.Customer.email == email_address).order_by(models.Customer.id.asc(),).all()\n@@ -426,6 +427,7 @@ async def f():\n c = 1\n d = (1,) + a + (2,)\n e = (1,).count(1)\n+f = 1, *range(10)\n what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(\n     vars_to_remove\n )"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 273,
                "created_at": "2018-05-29T15:18:05Z",
                "closed_at": "2018-06-05T04:28:37Z",
                "title": "Only put optional parentheses when contents can be split",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "23a00f051576d2e7edd18b6af382902cc34ea4a2",
                        "commit_date": "2018-06-05T03:24:50Z",
                        "parents": "7fc6ce990669464f5172b63fafa3724f5f308be3",
                        "stat": {
                            "total": 4,
                            "additions": 21,
                            "deletions": 17,
                            "files": [
                                {
                                    "sha": "bfd77f94ab62f81a095807134919880b79cc0643",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/23a00f051576d2e7edd18b6af382902cc34ea4a2/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/23a00f051576d2e7edd18b6af382902cc34ea4a2/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=23a00f051576d2e7edd18b6af382902cc34ea4a2",
                                    "patch": "@@ -719,6 +719,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * the header output in `--diff` now actually conforms to the unified diff spec\n \n+* fixed long trivial assignments being wrapped in unnecessary parentheses (#273)\n+\n * fixed stdin handling not working correctly if an old version of Click was\n   used (#276)\n "
                                },
                                {
                                    "sha": "35af598e59032c62397bff766d06b006df41174c",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 4,
                                    "changes": 15,
                                    "blob_url": "https://github.com/psf/black/blob/23a00f051576d2e7edd18b6af382902cc34ea4a2/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/23a00f051576d2e7edd18b6af382902cc34ea4a2/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=23a00f051576d2e7edd18b6af382902cc34ea4a2",
                                    "patch": "@@ -2213,11 +2213,14 @@ def right_hand_split(\n             result.append(leaf, preformatted=True)\n             for comment_after in line.comments_after(leaf):\n                 result.append(comment_after, preformatted=True)\n-    bracket_split_succeeded_or_raise(head, body, tail)\n     assert opening_bracket and closing_bracket\n+    body.should_explode = should_explode(body, opening_bracket)\n+    bracket_split_succeeded_or_raise(head, body, tail)\n     if (\n+        # the body shouldn't be exploded\n+        not body.should_explode\n         # the opening bracket is an optional paren\n-        opening_bracket.type == token.LPAR\n+        and opening_bracket.type == token.LPAR\n         and not opening_bracket.value\n         # the closing bracket is an optional paren\n         and closing_bracket.type == token.RPAR\n@@ -2234,11 +2237,15 @@ def right_hand_split(\n                 yield from right_hand_split(line, line_length, py36=py36, omit=omit)\n                 return\n             except CannotSplit:\n-                pass\n+                if len(body.leaves) == 1 and not is_line_short_enough(\n+                    body, line_length=line_length\n+                ):\n+                    raise CannotSplit(\n+                        \"Splitting failed, body is still too long and can't be split.\"\n+                    )\n \n     ensure_visible(opening_bracket)\n     ensure_visible(closing_bracket)\n-    body.should_explode = should_explode(body, opening_bracket)\n     for result in (head, body, tail):\n         if result:\n             yield result"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "816cdef88819b9991668f6090aa89a71662d66a1",
                                    "filename": "tests/cantfit.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/23a00f051576d2e7edd18b6af382902cc34ea4a2/tests%2Fcantfit.py",
                                    "raw_url": "https://github.com/psf/black/raw/23a00f051576d2e7edd18b6af382902cc34ea4a2/tests%2Fcantfit.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcantfit.py?ref=23a00f051576d2e7edd18b6af382902cc34ea4a2",
                                    "patch": "@@ -25,6 +25,9 @@\n     \"eggs with spam and eggs and spam with eggs with spam and eggs and spam with eggs with spam and eggs and spam with eggs\",\n     this_is_a_ridiculously_long_name_and_nobody_in_their_right_mind_would_use_one_like_it=0,\n )\n+string_variable_name = (\n+    \"a string that is waaaaaaaayyyyyyyy too long, even in parens, there's nothing you can do\"  # noqa\n+)\n \n \n # output\n@@ -67,3 +70,4 @@\n     \"eggs with spam and eggs and spam with eggs with spam and eggs and spam with eggs with spam and eggs and spam with eggs\",\n     this_is_a_ridiculously_long_name_and_nobody_in_their_right_mind_would_use_one_like_it=0,\n )\n+string_variable_name = \"a string that is waaaaaaaayyyyyyyy too long, even in parens, there's nothing you can do\"  # noqa"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 238,
                "created_at": "2018-05-20T21:54:47Z",
                "closed_at": "2018-05-22T07:40:00Z",
                "title": "Omitting rightmost bracket splits + a moving inline comment causes unstable formatting",
                "labels": "T: bug, C: unstable formatting",
                "commits": [
                    {
                        "hash": "658eb7161d8d0c23bafe4881e70518c33a74a4c1",
                        "commit_date": "2018-05-22T07:38:31Z",
                        "parents": "9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                        "stat": {
                            "total": 7,
                            "additions": 54,
                            "deletions": 47,
                            "files": [
                                {
                                    "sha": "af1f8479f358e9b55bccc917c3ef0b540a6c02a4",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/658eb7161d8d0c23bafe4881e70518c33a74a4c1/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/658eb7161d8d0c23bafe4881e70518c33a74a4c1/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=658eb7161d8d0c23bafe4881e70518c33a74a4c1",
                                    "patch": "@@ -661,9 +661,13 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed optional parentheses being removed within `# fmt: off` sections (#224)\n \n-* fixed invalid code produced when stars in very long imports were incorrectly \n+* fixed invalid code produced when stars in very long imports were incorrectly\n   wrapped in optional parentheses (#234)\n \n+* fixed unstable formatting when inline comments were moved around in\n+  a trailer that was omitted from line splitting on a large expression\n+  (#238) \n+\n ### 18.5b0\n \n * call chains are now formatted according to the"
                                },
                                {
                                    "sha": "e1a71e844939d9b1ad671c76440421fb439d8f90",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 4,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/658eb7161d8d0c23bafe4881e70518c33a74a4c1/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/658eb7161d8d0c23bafe4881e70518c33a74a4c1/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=658eb7161d8d0c23bafe4881e70518c33a74a4c1",
                                    "patch": "@@ -2599,7 +2599,8 @@ def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf\n         if length > line_length:\n             break\n \n-        if leaf.type == STANDALONE_COMMENT:\n+        has_inline_comment = leaf_length > len(leaf.value) + len(leaf.prefix)\n+        if leaf.type == STANDALONE_COMMENT or has_inline_comment:\n             break\n \n         optional_brackets.discard(id(leaf))\n@@ -2940,9 +2941,6 @@ def enumerate_with_length(\n \n         comment: Optional[Leaf]\n         for comment in line.comments_after(leaf, index):\n-            if \"\\n\" in comment.prefix:\n-                return  # Oops, standalone comment!\n-\n             length += len(comment.value)\n \n         yield index, leaf, length"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "4a4bb140d9e7a3dea06c961b5866c5f785132450",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 2,
                                    "changes": 22,
                                    "blob_url": "https://github.com/psf/black/blob/658eb7161d8d0c23bafe4881e70518c33a74a4c1/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/658eb7161d8d0c23bafe4881e70518c33a74a4c1/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=658eb7161d8d0c23bafe4881e70518c33a74a4c1",
                                    "patch": "@@ -116,7 +116,7 @@\n  call(**self.screen_kwargs)\n  call(b, **self.screen_kwargs)\n  lukasz.langa.pl\n-@@ -93,11 +114,11 @@\n+@@ -93,23 +114,25 @@\n  1.0 .real\n  ....__class__\n  list[str]\n@@ -127,9 +127,27 @@\n  very_long_variable_name_filters: t.List[\n      t.Tuple[str, t.Union[str, t.List[t.Optional[str]]]],\n  ]\n+ xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(  # type: ignore\n+     sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+ )\n+-xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(  # type: ignore\n+-    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+-)\n+ xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[\n+     ..., List[SomeClass]\n+-] = classmethod(sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__))  # type: ignore\n++] = classmethod(  # type: ignore\n++    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n++)\n++xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(\n++    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n++)  # type: ignore\n  slice[0]\n  slice[0:1]\n-@@ -124,107 +145,159 @@\n+ slice[0:1:2]\n+ slice[:]\n+ slice[:-1]\n+@@ -133,107 +156,159 @@\n  numpy[-(c + 1) :, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]"
                                },
                                {
                                    "sha": "f35a6fd2da73155f310838b938086d832c01bdad",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/psf/black/blob/658eb7161d8d0c23bafe4881e70518c33a74a4c1/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/658eb7161d8d0c23bafe4881e70518c33a74a4c1/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=658eb7161d8d0c23bafe4881e70518c33a74a4c1",
                                    "patch": "@@ -99,6 +99,15 @@\n very_long_variable_name_filters: t.List[\n     t.Tuple[str, t.Union[str, t.List[t.Optional[str]]]],\n ]\n+xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(  # type: ignore\n+    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+)\n+xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(  # type: ignore\n+    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+)\n+xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[\n+    ..., List[SomeClass]\n+] = classmethod(sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__))  # type: ignore\n slice[0]\n slice[0:1]\n slice[0:1:2]\n@@ -354,6 +363,17 @@ async def f():\n very_long_variable_name_filters: t.List[\n     t.Tuple[str, t.Union[str, t.List[t.Optional[str]]]],\n ]\n+xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(  # type: ignore\n+    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+)\n+xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[\n+    ..., List[SomeClass]\n+] = classmethod(  # type: ignore\n+    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+)\n+xxxx_xxx_xxxx_xxxxx_xxxx_xxx: Callable[..., List[SomeClass]] = classmethod(\n+    sync(async_xxxx_xxx_xxxx_xxxxx_xxxx_xxx.__func__)\n+)  # type: ignore\n slice[0]\n slice[0:1]\n slice[0:1:2]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 237,
                "created_at": "2018-05-20T21:26:26Z",
                "closed_at": "2018-05-21T19:30:40Z",
                "title": "Omitting rightmost bracket splits with inline comments produces invalid code",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "91de9ea6e3a83fe29c165814271a9bffdd5e6097",
                        "commit_date": "2018-05-21T19:29:36Z",
                        "parents": "f1f12284e0e9424890b2616b198e69ed7e147116",
                        "stat": {
                            "total": 1,
                            "additions": 17,
                            "deletions": 16,
                            "files": [
                                {
                                    "sha": "7038d9656cbc64e8239c9c73e77f513c47b1a6e6",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/91de9ea6e3a83fe29c165814271a9bffdd5e6097/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/91de9ea6e3a83fe29c165814271a9bffdd5e6097/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=91de9ea6e3a83fe29c165814271a9bffdd5e6097",
                                    "patch": "@@ -654,7 +654,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n ### 18.5b1 (unreleased)\n \n * Python grammar pickle caches are stored with the formatting caches, making\n-  *Black* work in environments where site-packages is not user-writable (#192).\n+  *Black* work in environments where site-packages is not user-writable (#192)\n+\n+* fixed invalid code produced when standalone comments were present in a trailer\n+  that was omitted from line splitting on a large expression (#237)\n+\n \n ### 18.5b0\n "
                                },
                                {
                                    "sha": "8d551640f365e8dd0072adc08cbb0452cad1c715",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/91de9ea6e3a83fe29c165814271a9bffdd5e6097/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/91de9ea6e3a83fe29c165814271a9bffdd5e6097/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=91de9ea6e3a83fe29c165814271a9bffdd5e6097",
                                    "patch": "@@ -2606,6 +2606,9 @@ def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf\n         if length > line_length:\n             break\n \n+        if leaf.type == STANDALONE_COMMENT:\n+            break\n+\n         optional_brackets.discard(id(leaf))\n         if opening_bracket:\n             if leaf is opening_bracket:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f8cd5edb10677a8aa92b0c70ca3d4ff65a5ade8a",
                                    "filename": "tests/composition.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/91de9ea6e3a83fe29c165814271a9bffdd5e6097/tests%2Fcomposition.py",
                                    "raw_url": "https://github.com/psf/black/raw/91de9ea6e3a83fe29c165814271a9bffdd5e6097/tests%2Fcomposition.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomposition.py?ref=91de9ea6e3a83fe29c165814271a9bffdd5e6097",
                                    "patch": "@@ -32,6 +32,14 @@ def test(self) -> None:\n                         # Another\n                     ):\n                         print(i)\n+        xxxxxxxxxxxxxxxx = Yyyy2YyyyyYyyyyy(\n+            push_manager=context.request.resource_manager,\n+            max_items_to_push=num_items,\n+            batch_size=Yyyy2YyyyYyyyyYyyy.FULL_SIZE,\n+        ).push(\n+            # Only send the first n items.\n+            items=items[:num_items]\n+        )\n         return (\n             \"Utterly failed doctest test for %s\\n\"\n             '  File \"%s\", line %s, in %s\\n\\n%s'"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 234,
                "created_at": "2018-05-20T03:16:47Z",
                "closed_at": "2018-05-21T23:38:42Z",
                "title": "Star import after a long path produces invalid code",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                        "commit_date": "2018-05-21T23:37:29Z",
                        "parents": "808754af18f1cea2444c6facf63f3a86952c5d44",
                        "stat": {
                            "total": 28,
                            "additions": 56,
                            "deletions": 28,
                            "files": [
                                {
                                    "sha": "90731d231c2171ba130134e2a52e9f59726e211a",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                                    "patch": "@@ -661,6 +661,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed optional parentheses being removed within `# fmt: off` sections (#224)\n \n+* fixed invalid code produced when stars in very long imports were incorrectly \n+  wrapped in optional parentheses (#234)\n \n ### 18.5b0\n "
                                },
                                {
                                    "sha": "afc37d99fe3ac547383c1fa3c59cfba763aefc51",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 28,
                                    "changes": 44,
                                    "blob_url": "https://github.com/psf/black/blob/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                                    "patch": "@@ -1375,32 +1375,6 @@ def visit_decorators(self, node: Node) -> Iterator[Line]:\n             yield from self.line()\n             yield from self.visit(child)\n \n-    def visit_import_from(self, node: Node) -> Iterator[Line]:\n-        \"\"\"Visit import_from and maybe put invisible parentheses.\n-\n-        This is separate from `visit_stmt` because import statements don't\n-        support arbitrary atoms and thus handling of parentheses is custom.\n-        \"\"\"\n-        check_lpar = False\n-        for index, child in enumerate(node.children):\n-            if check_lpar:\n-                if child.type == token.LPAR:\n-                    # make parentheses invisible\n-                    child.value = \"\"  # type: ignore\n-                    node.children[-1].value = \"\"  # type: ignore\n-                else:\n-                    # insert invisible parentheses\n-                    node.insert_child(index, Leaf(token.LPAR, \"\"))\n-                    node.append_child(Leaf(token.RPAR, \"\"))\n-                break\n-\n-            check_lpar = (\n-                child.type == token.NAME and child.value == \"import\"  # type: ignore\n-            )\n-\n-        for child in node.children:\n-            yield from self.visit(child)\n-\n     def visit_SEMI(self, leaf: Leaf) -> Iterator[Line]:\n         \"\"\"Remove a semicolon and put the other statement on a separate line.\"\"\"\n         yield from self.line()\n@@ -1447,6 +1421,7 @@ def __attrs_post_init__(self) -> None:\n         self.visit_classdef = partial(v, keywords={\"class\"}, parens=\u00d8)\n         self.visit_expr_stmt = partial(v, keywords=\u00d8, parens=ASSIGNMENTS)\n         self.visit_return_stmt = partial(v, keywords={\"return\"}, parens={\"return\"})\n+        self.visit_import_from = partial(v, keywords=\u00d8, parens={\"import\"})\n         self.visit_async_funcdef = self.visit_async_stmt\n         self.visit_decorated = self.visit_decorators\n \n@@ -2343,16 +2318,29 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n         return  # This `node` has a prefix with `# fmt: off`, don't mess with parens.\n \n     check_lpar = False\n-    for child in list(node.children):\n+    for index, child in enumerate(list(node.children)):\n         if check_lpar:\n             if child.type == syms.atom:\n                 maybe_make_parens_invisible_in_atom(child)\n             elif is_one_tuple(child):\n                 # wrap child in visible parentheses\n                 lpar = Leaf(token.LPAR, \"(\")\n                 rpar = Leaf(token.RPAR, \")\")\n-                index = child.remove() or 0\n+                child.remove()\n                 node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n+            elif node.type == syms.import_from:\n+                # \"import from\" nodes store parentheses directly as part of\n+                # the statement\n+                if child.type == token.LPAR:\n+                    # make parentheses invisible\n+                    child.value = \"\"  # type: ignore\n+                    node.children[-1].value = \"\"  # type: ignore\n+                elif child.type != token.STAR:\n+                    # insert invisible parentheses\n+                    node.insert_child(index, Leaf(token.LPAR, \"\"))\n+                    node.append_child(Leaf(token.RPAR, \"\"))\n+                break\n+\n             elif not (isinstance(child, Leaf) and is_multiline_string(child)):\n                 # wrap child in invisible parentheses\n                 lpar = Leaf(token.LPAR, \"\")"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "5666dd88c47c73229f0215de6b5355eb161b76b0",
                                    "filename": "tests/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/tests%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/tests%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffmtonoff.py?ref=9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                                    "patch": "@@ -6,6 +6,10 @@\n \n from library import some_connection, \\\n                     some_decorator\n+# fmt: off\n+from third_party import (X,\n+                         Y, Z)\n+# fmt: on\n f'trigger 3.6 mode'\n # fmt: off\n def func_no_args():\n@@ -102,6 +106,10 @@ def single_literal_yapf_disable():\n \n from library import some_connection, some_decorator\n \n+# fmt: off\n+from third_party import (X,\n+                         Y, Z)\n+# fmt: on\n f\"trigger 3.6 mode\"\n # fmt: off\n def func_no_args():"
                                },
                                {
                                    "sha": "0c98650f8339c56728442cdca93345d268c045c7",
                                    "filename": "tests/import_spacing.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/tests%2Fimport_spacing.py",
                                    "raw_url": "https://github.com/psf/black/raw/9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9/tests%2Fimport_spacing.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fimport_spacing.py?ref=9a6c88c7f4b4db14631f8dc9e8d44b3aed9d57c9",
                                    "patch": "@@ -24,6 +24,7 @@\n     Just, Enough, Libraries, To, Fit, In, This, Nice, Split, Which, We, No, Longer, Use\n )\n from name_of_a_company.extremely_long_project_name.component.ttypes import CuteLittleServiceHandlerFactoryyy\n+from name_of_a_company.extremely_long_project_name.extremely_long_component_name.ttypes import *\n \n from .a.b.c.subprocess import *\n from . import (tasks)\n@@ -87,6 +88,7 @@\n from name_of_a_company.extremely_long_project_name.component.ttypes import (\n     CuteLittleServiceHandlerFactoryyy\n )\n+from name_of_a_company.extremely_long_project_name.extremely_long_component_name.ttypes import *\n \n from .a.b.c.subprocess import *\n from . import tasks"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 232,
                "created_at": "2018-05-19T17:20:00Z",
                "closed_at": "2018-06-05T04:28:37Z",
                "title": "Multiline strings cause unnecessary optional parentheses",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "d638d56e0e383caeeff048e76b02b50b29367c3d",
                        "commit_date": "2018-06-05T04:27:51Z",
                        "parents": "23a00f051576d2e7edd18b6af382902cc34ea4a2",
                        "stat": {
                            "total": 15,
                            "additions": 100,
                            "deletions": 85,
                            "files": [
                                {
                                    "sha": "ac25db156e5e2b307246c38fc7a2f784e6d8e31d",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/d638d56e0e383caeeff048e76b02b50b29367c3d/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/d638d56e0e383caeeff048e76b02b50b29367c3d/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=d638d56e0e383caeeff048e76b02b50b29367c3d",
                                    "patch": "@@ -721,6 +721,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed long trivial assignments being wrapped in unnecessary parentheses (#273)\n \n+* fixed unnecessary parentheses when a line contained multiline strings (#232)\n+\n * fixed stdin handling not working correctly if an old version of Click was\n   used (#276)\n "
                                },
                                {
                                    "sha": "551d3c1dbdaf9c82f38a48729b78aed92c6c8079",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 69,
                                    "deletions": 15,
                                    "changes": 84,
                                    "blob_url": "https://github.com/psf/black/blob/d638d56e0e383caeeff048e76b02b50b29367c3d/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/d638d56e0e383caeeff048e76b02b50b29367c3d/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=d638d56e0e383caeeff048e76b02b50b29367c3d",
                                    "patch": "@@ -1094,6 +1094,13 @@ def contains_standalone_comments(self, depth_limit: int = sys.maxsize) -> bool:\n \n         return False\n \n+    def contains_multiline_strings(self) -> bool:\n+        for leaf in self.leaves:\n+            if is_multiline_string(leaf):\n+                return True\n+\n+        return False\n+\n     def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n         \"\"\"Remove trailing comma if there is one and it's safe.\"\"\"\n         if not (\n@@ -2225,24 +2232,35 @@ def right_hand_split(\n         # the closing bracket is an optional paren\n         and closing_bracket.type == token.RPAR\n         and not closing_bracket.value\n-        # there are no standalone comments in the body\n-        and not line.contains_standalone_comments(0)\n-        # and it's not an import (optional parens are the only thing we can split\n-        # on in this case; attempting a split without them is a waste of time)\n+        # it's not an import (optional parens are the only thing we can split on\n+        # in this case; attempting a split without them is a waste of time)\n         and not line.is_import\n+        # there are no standalone comments in the body\n+        and not body.contains_standalone_comments(0)\n+        # and we can actually remove the parens\n+        and can_omit_invisible_parens(body, line_length)\n     ):\n         omit = {id(closing_bracket), *omit}\n-        if can_omit_invisible_parens(body, line_length):\n-            try:\n-                yield from right_hand_split(line, line_length, py36=py36, omit=omit)\n-                return\n-            except CannotSplit:\n-                if len(body.leaves) == 1 and not is_line_short_enough(\n-                    body, line_length=line_length\n-                ):\n-                    raise CannotSplit(\n-                        \"Splitting failed, body is still too long and can't be split.\"\n-                    )\n+        try:\n+            yield from right_hand_split(line, line_length, py36=py36, omit=omit)\n+            return\n+\n+        except CannotSplit:\n+            if not (\n+                can_be_split(body)\n+                or is_line_short_enough(body, line_length=line_length)\n+            ):\n+                raise CannotSplit(\n+                    \"Splitting failed, body is still too long and can't be split.\"\n+                )\n+\n+            elif head.contains_multiline_strings() or tail.contains_multiline_strings():\n+                raise CannotSplit(\n+                    \"The current optional pair of parentheses is bound to fail to \"\n+                    \"satisfy the splitting algorithm becase the head or the tail \"\n+                    \"contains multiline strings which by definition never fit one \"\n+                    \"line.\"\n+                )\n \n     ensure_visible(opening_bracket)\n     ensure_visible(closing_bracket)\n@@ -3190,6 +3208,42 @@ def is_line_short_enough(line: Line, *, line_length: int, line_str: str = \"\") ->\n     )\n \n \n+def can_be_split(line: Line) -> bool:\n+    \"\"\"Return False if the line cannot be split *for sure*.\n+\n+    This is not an exhaustive search but a cheap heuristic that we can use to\n+    avoid some unfortunate formattings (mostly around wrapping unsplittable code\n+    in unnecessary parentheses).\n+    \"\"\"\n+    leaves = line.leaves\n+    if len(leaves) < 2:\n+        return False\n+\n+    if leaves[0].type == token.STRING and leaves[1].type == token.DOT:\n+        call_count = 0\n+        dot_count = 0\n+        next = leaves[-1]\n+        for leaf in leaves[-2::-1]:\n+            if leaf.type in OPENING_BRACKETS:\n+                if next.type not in CLOSING_BRACKETS:\n+                    return False\n+\n+                call_count += 1\n+            elif leaf.type == token.DOT:\n+                dot_count += 1\n+            elif leaf.type == token.NAME:\n+                if not (next.type == token.DOT or next.type in OPENING_BRACKETS):\n+                    return False\n+\n+            elif leaf.type not in CLOSING_BRACKETS:\n+                return False\n+\n+            if dot_count > 1 and call_count > 1:\n+                return False\n+\n+    return True\n+\n+\n def can_omit_invisible_parens(line: Line, line_length: int) -> bool:\n     \"\"\"Does `line` have a shape safe to reformat without optional parens around it?\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e15e69cf94e22018779d54ecc8335ef42720c169",
                                    "filename": "tests/cantfit.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/psf/black/blob/d638d56e0e383caeeff048e76b02b50b29367c3d/tests%2Fcantfit.py",
                                    "raw_url": "https://github.com/psf/black/raw/d638d56e0e383caeeff048e76b02b50b29367c3d/tests%2Fcantfit.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcantfit.py?ref=d638d56e0e383caeeff048e76b02b50b29367c3d",
                                    "patch": "@@ -28,6 +28,13 @@\n string_variable_name = (\n     \"a string that is waaaaaaaayyyyyyyy too long, even in parens, there's nothing you can do\"  # noqa\n )\n+for key in \"\"\"\n+    hostname\n+    port\n+    username\n+\"\"\".split():\n+    if key in self.connect_kwargs:\n+        raise ValueError(err.format(key))\n \n \n # output\n@@ -71,3 +78,10 @@\n     this_is_a_ridiculously_long_name_and_nobody_in_their_right_mind_would_use_one_like_it=0,\n )\n string_variable_name = \"a string that is waaaaaaaayyyyyyyy too long, even in parens, there's nothing you can do\"  # noqa\n+for key in \"\"\"\n+    hostname\n+    port\n+    username\n+\"\"\".split():\n+    if key in self.connect_kwargs:\n+        raise ValueError(err.format(key))"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 224,
                "created_at": "2018-05-18T17:29:49Z",
                "closed_at": "2018-05-21T22:21:23Z",
                "title": "black strips off parens in fmt: off section",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "808754af18f1cea2444c6facf63f3a86952c5d44",
                        "commit_date": "2018-05-21T22:21:14Z",
                        "parents": "86e1c3650741ce9641a14a89db6162f30786f093",
                        "stat": {
                            "total": 3,
                            "additions": 26,
                            "deletions": 23,
                            "files": [
                                {
                                    "sha": "9b42f20a10f7fe9b27348cd19afb44dd484a15f5",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/808754af18f1cea2444c6facf63f3a86952c5d44/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/808754af18f1cea2444c6facf63f3a86952c5d44/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=808754af18f1cea2444c6facf63f3a86952c5d44",
                                    "patch": "@@ -659,6 +659,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * fixed invalid code produced when standalone comments were present in a trailer\n   that was omitted from line splitting on a large expression (#237)\n \n+* fixed optional parentheses being removed within `# fmt: off` sections (#224)\n+\n \n ### 18.5b0\n "
                                },
                                {
                                    "sha": "9fbacc1048cd15fd7816ccdfb8dd3f6dd7252b9c",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 1,
                                    "changes": 7,
                                    "blob_url": "https://github.com/psf/black/blob/808754af18f1cea2444c6facf63f3a86952c5d44/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/808754af18f1cea2444c6facf63f3a86952c5d44/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=808754af18f1cea2444c6facf63f3a86952c5d44",
                                    "patch": "@@ -1820,7 +1820,7 @@ def is_split_before_delimiter(leaf: Leaf, previous: Leaf = None) -> int:\n     return 0\n \n \n-def generate_comments(leaf: Leaf) -> Iterator[Leaf]:\n+def generate_comments(leaf: LN) -> Iterator[Leaf]:\n     \"\"\"Clean the prefix of the `leaf` and generate comments from it, if any.\n \n     Comments in lib2to3 are shoved into the whitespace prefix.  This happens\n@@ -2337,6 +2337,11 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     Standardizes on visible parentheses for single-element tuples, and keeps\n     existing visible parentheses for other tuples and generator expressions.\n     \"\"\"\n+    try:\n+        list(generate_comments(node))\n+    except FormatOff:\n+        return  # This `node` has a prefix with `# fmt: off`, don't mess with parens.\n+\n     check_lpar = False\n     for child in list(node.children):\n         if check_lpar:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e76e6a7fcd671cbaf81682bc2c7d0f02e28e9b49",
                                    "filename": "tests/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 2,
                                    "changes": 17,
                                    "blob_url": "https://github.com/psf/black/blob/808754af18f1cea2444c6facf63f3a86952c5d44/tests%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/808754af18f1cea2444c6facf63f3a86952c5d44/tests%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffmtonoff.py?ref=808754af18f1cea2444c6facf63f3a86952c5d44",
                                    "patch": "@@ -50,6 +50,11 @@ def long_lines():\n         typedargslist.extend(\n             gen_annotated_params(ast_args.kwonlyargs, ast_args.kw_defaults, parameters, implicit_default=True)\n         )\n+        # fmt: off\n+        a = (\n+            unnecessary_bracket()\n+        )\n+        # fmt: on\n     _type_comment_re = re.compile(\n         r\"\"\"\n         ^\n@@ -69,8 +74,10 @@ def long_lines():\n             \\n?\n         )\n         $\n-        \"\"\", re.MULTILINE | re.VERBOSE\n+        \"\"\", # fmt: off\n+        re.MULTILINE | re.VERBOSE\n     )\n+    # fmt: on\n def single_literal_yapf_disable():\n     \"\"\"Black does not support this.\"\"\"\n     BAZ = {\n@@ -163,6 +170,11 @@ def long_lines():\n                 implicit_default=True,\n             )\n         )\n+        # fmt: off\n+        a = (\n+            unnecessary_bracket()\n+        )\n+        # fmt: on\n     _type_comment_re = re.compile(\n         r\"\"\"\n         ^\n@@ -182,9 +194,10 @@ def long_lines():\n             \\n?\n         )\n         $\n-        \"\"\",\n+        \"\"\",  # fmt: off\n         re.MULTILINE | re.VERBOSE,\n     )\n+    # fmt: on\n \n \n def single_literal_yapf_disable():"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 215,
                "created_at": "2018-05-16T21:16:38Z",
                "closed_at": "2018-05-17T02:20:41Z",
                "title": "Multiline string literals get wrapped in unnecessary parens",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "3ad0f5855c46410652b27b5e09c6f22314241757",
                        "commit_date": "2018-05-17T02:19:48Z",
                        "parents": "665ed8a2403161a987c49e1818f2376840723b96",
                        "stat": {
                            "total": 2,
                            "additions": 16,
                            "deletions": 14,
                            "files": [
                                {
                                    "sha": "3eb8ee4f131bb3bd234b457071895318932bf44c",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/3ad0f5855c46410652b27b5e09c6f22314241757/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/3ad0f5855c46410652b27b5e09c6f22314241757/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=3ad0f5855c46410652b27b5e09c6f22314241757",
                                    "patch": "@@ -653,6 +653,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * fixed non-deterministic formatting when multiple pairs of removable parentheses\n   were used (#183)\n \n+* fixed multiline strings being unnecessarily wrapped in optional\n+  parentheses in long assignments (#215)\n+\n * fixed not splitting long from-imports with only a single name\n \n * fixed Python 3.6+ file discovery by also looking at function calls with"
                                },
                                {
                                    "sha": "298597bcb30f8aad947056fc8583327a75814d5e",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 2,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/3ad0f5855c46410652b27b5e09c6f22314241757/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/3ad0f5855c46410652b27b5e09c6f22314241757/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=3ad0f5855c46410652b27b5e09c6f22314241757",
                                    "patch": "@@ -1315,7 +1315,7 @@ def visit_stmt(\n         The relevant Python language `keywords` for a given statement will be\n         NAME leaves within it. This methods puts those on a separate line.\n \n-        `parens` holds a set of string leaf values immeditely after which\n+        `parens` holds a set of string leaf values immediately after which\n         invisible parens should be put.\n         \"\"\"\n         normalize_invisible_parens(node, parens_after=parens)\n@@ -2361,7 +2361,7 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n                 rpar = Leaf(token.RPAR, \")\")\n                 index = child.remove() or 0\n                 node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))\n-            else:\n+            elif not (isinstance(child, Leaf) and is_multiline_string(child)):\n                 # wrap child in invisible parentheses\n                 lpar = Leaf(token.LPAR, \"\")\n                 rpar = Leaf(token.RPAR, \"\")\n@@ -2472,6 +2472,12 @@ def is_vararg(leaf: Leaf, within: Set[NodeType]) -> bool:\n     return p.type in within\n \n \n+def is_multiline_string(leaf: Leaf) -> bool:\n+    \"\"\"Return True if `leaf` is a multiline string that actually spans many lines.\"\"\"\n+    value = leaf.value.lstrip(\"furbFURB\")\n+    return value[:3] in {'\"\"\"', \"'''\"} and \"\\n\" in value\n+\n+\n def is_stub_suite(node: Node) -> bool:\n     \"\"\"Return True if `node` is a suite with a stub body.\"\"\"\n     if ("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e95bd21ca9edaf6af7049c06ff967b54883c92ae",
                                    "filename": "tests/comments3.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/3ad0f5855c46410652b27b5e09c6f22314241757/tests%2Fcomments3.py",
                                    "raw_url": "https://github.com/psf/black/raw/3ad0f5855c46410652b27b5e09c6f22314241757/tests%2Fcomments3.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomments3.py?ref=3ad0f5855c46410652b27b5e09c6f22314241757",
                                    "patch": "@@ -1,4 +1,7 @@\n def func():\n+    x = \"\"\"\n+    a really long string\n+    \"\"\"\n     lcomp3 = [\n         # This one is actually too long to fit in a single line.\n         element.split(\"\\n\", 1)[0]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 193,
                "created_at": "2018-05-07T21:25:50Z",
                "closed_at": "2018-05-16T05:13:24Z",
                "title": "can't format python2 valid syntax",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                        "commit_date": "2018-05-16T05:13:16Z",
                        "parents": "8b64e916f65f2c1023f9abd6243a904bfbcd8fe9",
                        "stat": {
                            "total": 68,
                            "additions": 242,
                            "deletions": 174,
                            "files": [
                                {
                                    "sha": "1570348ade6060f93d6192e9185233c09cf35d74",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -597,6 +597,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * math operators now use their respective priorities for delimiting multiline\n   expressions (#148)\n \n+* optional parentheses are now omitted on expressions that start or end\n+  with a bracket and only contain a single operator (#177)\n+\n * empty parentheses in a class definition are now removed (#145, #180)\n \n * string prefixes are now standardized to lowercase and `u` is removed\n@@ -621,6 +624,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n   where used both in function signatures with stars and function calls\n   with stars but the former would be reformatted to a single line.\n \n+* fixed crash on dealing with optional parentheses (#193)\n+\n * fixed crash when dead symlinks where encountered\n \n "
                                },
                                {
                                    "sha": "e7a7aa8585836c72a5d24dfa35fda0089462578a",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 25,
                                    "changes": 52,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -239,11 +239,8 @@ def reformat_one(\n                 src = src.resolve()\n                 if src in cache and cache[src] == get_cache_info(src):\n                     changed = Changed.CACHED\n-            if (\n-                changed is not Changed.CACHED\n-                and format_file_in_place(\n-                    src, line_length=line_length, fast=fast, write_back=write_back\n-                )\n+            if changed is not Changed.CACHED and format_file_in_place(\n+                src, line_length=line_length, fast=fast, write_back=write_back\n             ):\n                 changed = Changed.YES\n             if write_back == WriteBack.YES and changed is not Changed.NO:\n@@ -860,14 +857,11 @@ def is_def(self) -> bool:\n             second_leaf: Optional[Leaf] = self.leaves[1]\n         except IndexError:\n             second_leaf = None\n-        return (\n-            (first_leaf.type == token.NAME and first_leaf.value == \"def\")\n-            or (\n-                first_leaf.type == token.ASYNC\n-                and second_leaf is not None\n-                and second_leaf.type == token.NAME\n-                and second_leaf.value == \"def\"\n-            )\n+        return (first_leaf.type == token.NAME and first_leaf.value == \"def\") or (\n+            first_leaf.type == token.ASYNC\n+            and second_leaf is not None\n+            and second_leaf.type == token.NAME\n+            and second_leaf.value == \"def\"\n         )\n \n     @property\n@@ -1032,9 +1026,8 @@ def is_complex_subscript(self, leaf: Leaf) -> bool:\n             and subscript_start.type == syms.subscriptlist\n         ):\n             subscript_start = child_towards(subscript_start, leaf)\n-        return (\n-            subscript_start is not None\n-            and any(n.type in TEST_DESCENDANTS for n in subscript_start.pre_order())\n+        return subscript_start is not None and any(\n+            n.type in TEST_DESCENDANTS for n in subscript_start.pre_order()\n         )\n \n     def __str__(self) -> str:\n@@ -1999,8 +1992,9 @@ def right_hand_split(\n     # Since body is a new indent level, remove spurious leading whitespace.\n     if body_leaves:\n         normalize_prefix(body_leaves[0], inside_brackets=True)\n-    elif not head_leaves:\n-        # No `head` and no `body` means the split failed. `tail` has all content.\n+    if not head_leaves:\n+        # No `head` means the split failed. Either `tail` has all content or\n+        # the matching `opening_bracket` wasn't available on `line` anymore.\n         raise CannotSplit(\"No brackets found\")\n \n     # Build the new lines.\n@@ -2018,19 +2012,27 @@ def right_hand_split(\n         # the closing bracket is an optional paren\n         and closing_bracket.type == token.RPAR\n         and not closing_bracket.value\n-        # there are no delimiters or standalone comments in the body\n-        and not body.bracket_tracker.delimiters\n+        # there are no standalone comments in the body\n         and not line.contains_standalone_comments(0)\n         # and it's not an import (optional parens are the only thing we can split\n         # on in this case; attempting a split without them is a waste of time)\n         and not line.is_import\n     ):\n         omit = {id(closing_bracket), *omit}\n-        try:\n-            yield from right_hand_split(line, py36=py36, omit=omit)\n-            return\n-        except CannotSplit:\n-            pass\n+        delimiter_count = len(body.bracket_tracker.delimiters)\n+        if (\n+            delimiter_count == 0\n+            or delimiter_count == 1\n+            and (\n+                body.leaves[0].type in OPENING_BRACKETS\n+                or body.leaves[-1].type in CLOSING_BRACKETS\n+            )\n+        ):\n+            try:\n+                yield from right_hand_split(line, py36=py36, omit=omit)\n+                return\n+            except CannotSplit:\n+                pass\n \n     ensure_visible(opening_bracket)\n     ensure_visible(closing_bracket)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c3dfd3db042de048c26e43ad368d049557db4e8c",
                                    "filename": "tests/composition.py",
                                    "status": "modified",
                                    "additions": 109,
                                    "deletions": 1,
                                    "changes": 110,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fcomposition.py",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fcomposition.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomposition.py?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -33,7 +33,7 @@ def test(self) -> None:\n                     ):\n                         print(i)\n \n-    def omitting_trailers() -> None:\n+    def omitting_trailers(self) -> None:\n         get_collection(\n             hey_this_is_a_very_long_call, it_has_funny_attributes, really=True\n         )[OneLevelIndex]\n@@ -43,3 +43,111 @@ def omitting_trailers() -> None:\n         d[0][1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][\n             22\n         ]\n+\n+    def easy_asserts(self) -> None:\n+        assert {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        } == expected, \"Not what we expected\"\n+\n+        assert expected == {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        }, \"Not what we expected\"\n+\n+        assert expected == {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        }\n+\n+    def tricky_asserts(self) -> None:\n+        assert {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        } == expected(\n+            value, is_going_to_be=\"too long to fit in a single line\", srsly=True\n+        ), \"Not what we expected\"\n+\n+        assert {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        } == expected, (\n+            \"Not what we expected and the message is too long to fit in one line\"\n+        )\n+\n+        assert expected(\n+            value, is_going_to_be=\"too long to fit in a single line\", srsly=True\n+        ) == {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        }, \"Not what we expected\"\n+\n+        assert expected == {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        }, (\n+            \"Not what we expected and the message is too long to fit \"\n+            \"in one line because it's too long\"\n+        )\n+\n+        # This is weird but true.\n+        assert expectedexpectedexpectedexpectedexpectedexpectedexpectedexpectedexpect == {\n+            key1: value1,\n+            key2: value2,\n+            key3: value3,\n+            key4: value4,\n+            key5: value5,\n+            key6: value6,\n+            key7: value7,\n+            key8: value8,\n+            key9: value9,\n+        }"
                                },
                                {
                                    "sha": "4c03e432383c174127c803dea8076488a09a8daf",
                                    "filename": "tests/empty_lines.py",
                                    "status": "modified",
                                    "additions": 21,
                                    "deletions": 30,
                                    "changes": 51,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fempty_lines.py",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fempty_lines.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fempty_lines.py?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -123,29 +123,23 @@ def f():\n             return NO\n \n         if prevp.type == token.EQUAL:\n-            if (\n-                prevp.parent\n-                and prevp.parent.type in {\n-                    syms.typedargslist,\n-                    syms.varargslist,\n-                    syms.parameters,\n-                    syms.arglist,\n-                    syms.argument,\n-                }\n-            ):\n+            if prevp.parent and prevp.parent.type in {\n+                syms.typedargslist,\n+                syms.varargslist,\n+                syms.parameters,\n+                syms.arglist,\n+                syms.argument,\n+            }:\n                 return NO\n \n         elif prevp.type == token.DOUBLESTAR:\n-            if (\n-                prevp.parent\n-                and prevp.parent.type in {\n-                    syms.typedargslist,\n-                    syms.varargslist,\n-                    syms.parameters,\n-                    syms.arglist,\n-                    syms.dictsetmaker,\n-                }\n-            ):\n+            if prevp.parent and prevp.parent.type in {\n+                syms.typedargslist,\n+                syms.varargslist,\n+                syms.parameters,\n+                syms.arglist,\n+                syms.dictsetmaker,\n+            }:\n                 return NO\n \n \n@@ -183,14 +177,11 @@ def g():\n             return NO\n \n         if prevp.type == token.EQUAL:\n-            if (\n-                prevp.parent\n-                and prevp.parent.type in {\n-                    syms.typedargslist,\n-                    syms.varargslist,\n-                    syms.parameters,\n-                    syms.arglist,\n-                    syms.argument,\n-                }\n-            ):\n+            if prevp.parent and prevp.parent.type in {\n+                syms.typedargslist,\n+                syms.varargslist,\n+                syms.parameters,\n+                syms.arglist,\n+                syms.argument,\n+            }:\n                 return NO"
                                },
                                {
                                    "sha": "5ad905f0f616febc48d1b38c26e30dd8635a2903",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 6,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -167,11 +167,11 @@\n -what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(vars_to_remove)\n -what_is_up_with_those_new_coord_names = (coord_names | set(vars_to_create)) - set(vars_to_remove)\n -result = session.query(models.Customer.id).filter(models.Customer.account_id == account_id, models.Customer.email == email_address).order_by(models.Customer.id.asc(),).all()\n-+what_is_up_with_those_new_coord_names = (\n-+    (coord_names + set(vars_to_create)) + set(vars_to_remove)\n++what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(\n++    vars_to_remove\n +)\n-+what_is_up_with_those_new_coord_names = (\n-+    (coord_names | set(vars_to_create)) - set(vars_to_remove)\n++what_is_up_with_those_new_coord_names = (coord_names | set(vars_to_create)) - set(\n++    vars_to_remove\n +)\n +result = session.query(models.Customer.id).filter(\n +    models.Customer.account_id == account_id, models.Customer.email == email_address\n@@ -256,8 +256,8 @@\n -    ~ aaaaaaaaaaaaaaaa.a + aaaaaaaaaaaaaaaa.b - aaaaaaaaaaaaaaaa.c * aaaaaaaaaaaaaaaa.d @ aaaaaaaaaaaaaaaa.e | aaaaaaaaaaaaaaaa.f & aaaaaaaaaaaaaaaa.g % aaaaaaaaaaaaaaaa.h ^ aaaaaaaaaaaaaaaa.i << aaaaaaaaaaaaaaaa.k >> aaaaaaaaaaaaaaaa.l ** aaaaaaaaaaaaaaaa.m // aaaaaaaaaaaaaaaa.n\n +print(*lambda x: x)\n +assert not Test, \"Short message\"\n-+assert (\n-+    this is ComplexTest and not requirements.fit_in_a_single_line(force=False)\n++assert this is ComplexTest and not requirements.fit_in_a_single_line(\n++    force=False\n +), \"Short message\"\n +assert parens is TooMany\n +for (x,) in (1,), (2,), (3,):"
                                },
                                {
                                    "sha": "65833c12efcb30c327eb56da3c04d850992a8b75",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 6,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/5a47fd13cc4c9f43270dd12c37577244c1eabfcc/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=5a47fd13cc4c9f43270dd12c37577244c1eabfcc",
                                    "patch": "@@ -401,11 +401,11 @@ async def f():\n c = 1\n d = (1,) + a + (2,)\n e = (1,).count(1)\n-what_is_up_with_those_new_coord_names = (\n-    (coord_names + set(vars_to_create)) + set(vars_to_remove)\n+what_is_up_with_those_new_coord_names = (coord_names + set(vars_to_create)) + set(\n+    vars_to_remove\n )\n-what_is_up_with_those_new_coord_names = (\n-    (coord_names | set(vars_to_create)) - set(vars_to_remove)\n+what_is_up_with_those_new_coord_names = (coord_names | set(vars_to_create)) - set(\n+    vars_to_remove\n )\n result = session.query(models.Customer.id).filter(\n     models.Customer.account_id == account_id, models.Customer.email == email_address\n@@ -433,8 +433,8 @@ async def f():\n print(**{1: 3} if False else {x: x for x in range(3)})\n print(*lambda x: x)\n assert not Test, \"Short message\"\n-assert (\n-    this is ComplexTest and not requirements.fit_in_a_single_line(force=False)\n+assert this is ComplexTest and not requirements.fit_in_a_single_line(\n+    force=False\n ), \"Short message\"\n assert parens is TooMany\n for (x,) in (1,), (2,), (3,):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 185,
                "created_at": "2018-05-02T09:58:52Z",
                "closed_at": "2018-05-07T17:41:10Z",
                "title": "Trailing comma after from .. import",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "dc0c14240e7423d9ada002835dcc195f8c6d8797",
                        "commit_date": "2018-05-07T17:40:18Z",
                        "parents": "c6a6cfd3a293a69c29f93beca1c64bd857371e14",
                        "stat": {
                            "total": 3,
                            "additions": 15,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "fd2b75ed2c1cf9d5ffb3759f142b6855b0a23b9e",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 3,
                                    "changes": 11,
                                    "blob_url": "https://github.com/psf/black/blob/dc0c14240e7423d9ada002835dcc195f8c6d8797/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/dc0c14240e7423d9ada002835dcc195f8c6d8797/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=dc0c14240e7423d9ada002835dcc195f8c6d8797",
                                    "patch": "@@ -885,9 +885,14 @@ def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n                 self.remove_trailing_comma()\n                 return True\n \n-        # For parens let's check if it's safe to remove the comma.  If the\n-        # trailing one is the only one, we might mistakenly change a tuple\n-        # into a different type by removing the comma.\n+        # For parens let's check if it's safe to remove the comma.\n+        # Imports are always safe.\n+        if self.is_import:\n+            self.remove_trailing_comma()\n+            return True\n+\n+        # Otheriwsse, if the trailing one is the only one, we might mistakenly\n+        # change a tuple into a different type by removing the comma.\n         depth = closing.bracket_depth + 1\n         commas = 0\n         opening = closing.opening_bracket"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "cc17405aa3a6486b3c05e1985fc808a1ab41e76f",
                                    "filename": "tests/import_spacing.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/dc0c14240e7423d9ada002835dcc195f8c6d8797/tests%2Fimport_spacing.py",
                                    "raw_url": "https://github.com/psf/black/raw/dc0c14240e7423d9ada002835dcc195f8c6d8797/tests%2Fimport_spacing.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fimport_spacing.py?ref=dc0c14240e7423d9ada002835dcc195f8c6d8797",
                                    "patch": "@@ -2,6 +2,9 @@\n \n # flake8: noqa\n \n+from logging import (\n+    ERROR,\n+)\n import sys\n \n # This relies on each of the submodules having an __all__ variable.\n@@ -48,6 +51,7 @@\n \n # flake8: noqa\n \n+from logging import ERROR\n import sys\n \n # This relies on each of the submodules having an __all__ variable."
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 183,
                "created_at": "2018-05-01T03:36:30Z",
                "closed_at": "2018-05-07T18:15:06Z",
                "title": "Different code on second pass",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "cfb003f51c89802b5bca29018fde62c4501a8940",
                        "commit_date": "2018-05-07T18:13:55Z",
                        "parents": "dc0c14240e7423d9ada002835dcc195f8c6d8797",
                        "stat": {
                            "total": 14,
                            "additions": 64,
                            "deletions": 50,
                            "files": [
                                {
                                    "sha": "30990590403a52158789e1953f2bc522a64dcf06",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 2,
                                    "changes": 9,
                                    "blob_url": "https://github.com/psf/black/blob/cfb003f51c89802b5bca29018fde62c4501a8940/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/cfb003f51c89802b5bca29018fde62c4501a8940/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=cfb003f51c89802b5bca29018fde62c4501a8940",
                                    "patch": "@@ -538,9 +538,14 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ### 18.5a0 (unreleased)\n \n-* Slices are now formatted according to PEP 8 (#178)\n+* slices are now formatted according to PEP 8 (#178)\n \n-* Empty parentheses in a class definition are removed (#145, #180)\n+* empty parentheses in a class definition are now removed (#145, #180)\n+\n+* fixed an invalid trailing comma sometimes left in imports (#185)\n+\n+* fixed non-deterministic formatting when multiple pairs of removable parentheses\n+  were used (#183)\n \n \n ### 18.4a4"
                                },
                                {
                                    "sha": "34bdcf900dd50b2601a610d428006a1fdc06f59f",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 24,
                                    "deletions": 11,
                                    "changes": 35,
                                    "blob_url": "https://github.com/psf/black/blob/cfb003f51c89802b5bca29018fde62c4501a8940/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/cfb003f51c89802b5bca29018fde62c4501a8940/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=cfb003f51c89802b5bca29018fde62c4501a8940",
                                    "patch": "@@ -2187,17 +2187,7 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n     for child in list(node.children):\n         if check_lpar:\n             if child.type == syms.atom:\n-                if not (\n-                    is_empty_tuple(child)\n-                    or is_one_tuple(child)\n-                    or max_delimiter_priority_in_atom(child) >= COMMA_PRIORITY\n-                ):\n-                    first = child.children[0]\n-                    last = child.children[-1]\n-                    if first.type == token.LPAR and last.type == token.RPAR:\n-                        # make parentheses invisible\n-                        first.value = \"\"  # type: ignore\n-                        last.value = \"\"  # type: ignore\n+                maybe_make_parens_invisible_in_atom(child)\n             elif is_one_tuple(child):\n                 # wrap child in visible parentheses\n                 lpar = Leaf(token.LPAR, \"(\")\n@@ -2214,6 +2204,29 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:\n         check_lpar = isinstance(child, Leaf) and child.value in parens_after\n \n \n+def maybe_make_parens_invisible_in_atom(node: LN) -> bool:\n+    \"\"\"If it's safe, make the parens in the atom `node` invisible, recusively.\"\"\"\n+    if (\n+        node.type != syms.atom\n+        or is_empty_tuple(node)\n+        or is_one_tuple(node)\n+        or max_delimiter_priority_in_atom(node) >= COMMA_PRIORITY\n+    ):\n+        return False\n+\n+    first = node.children[0]\n+    last = node.children[-1]\n+    if first.type == token.LPAR and last.type == token.RPAR:\n+        # make parentheses invisible\n+        first.value = \"\"  # type: ignore\n+        last.value = \"\"  # type: ignore\n+        if len(node.children) > 1:\n+            maybe_make_parens_invisible_in_atom(node.children[1])\n+        return True\n+\n+    return False\n+\n+\n def is_empty_tuple(node: LN) -> bool:\n     \"\"\"Return True if `node` holds an empty tuple.\"\"\"\n     return ("
                                },
                                {
                                    "sha": "ede46a4507b418ddeed6103e989c9e37d533eb98",
                                    "filename": "docs/reference/reference_functions.rst",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/cfb003f51c89802b5bca29018fde62c4501a8940/docs%2Freference%2Freference_functions.rst",
                                    "raw_url": "https://github.com/psf/black/raw/cfb003f51c89802b5bca29018fde62c4501a8940/docs%2Freference%2Freference_functions.rst",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/docs%2Freference%2Freference_functions.rst?ref=cfb003f51c89802b5bca29018fde62c4501a8940",
                                    "patch": "@@ -98,6 +98,8 @@ Utilities\n \n .. autofunction:: black.make_comment\n \n+.. autofunction:: black.maybe_make_parens_invisible_in_atom\n+\n .. autofunction:: black.max_delimiter_priority_in_atom\n \n .. autofunction:: black.normalize_prefix"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "ad5e934adee206f7bd57a08758399573cd59950e",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 1,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/cfb003f51c89802b5bca29018fde62c4501a8940/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/cfb003f51c89802b5bca29018fde62c4501a8940/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=cfb003f51c89802b5bca29018fde62c4501a8940",
                                    "patch": "@@ -129,7 +129,7 @@\n  ]\n  slice[0]\n  slice[0:1]\n-@@ -123,88 +145,114 @@\n+@@ -123,91 +145,119 @@\n  numpy[-(c + 1) :, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]\n@@ -201,6 +201,9 @@\n +print(*[] or [1])\n  print(**{1: 3} if False else {x: x for x in range(3)})\n -print(* lambda x: x)\n+-assert(not Test),(\"Short message\")\n+-assert this is ComplexTest and not requirements.fit_in_a_single_line(force=False), \"Short message\"\n+-assert(((parens is TooMany)))\n -for x, in (1,), (2,), (3,): ...\n -for y in (): ...\n -for z in (i for i in (1, 2, 3)): ...\n@@ -242,6 +245,11 @@\n -    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa /\n -    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n +print(*lambda x: x)\n++assert not Test, \"Short message\"\n++assert (\n++    this is ComplexTest and not requirements.fit_in_a_single_line(force=False)\n++), \"Short message\"\n++assert parens is TooMany\n +for (x,) in (1,), (2,), (3,):\n +    ...\n +for y in ():"
                                },
                                {
                                    "sha": "2c92c0e8632fcf6dcf5cd638a356e2ca9d51324f",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/cfb003f51c89802b5bca29018fde62c4501a8940/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/cfb003f51c89802b5bca29018fde62c4501a8940/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=cfb003f51c89802b5bca29018fde62c4501a8940",
                                    "patch": "@@ -163,6 +163,9 @@ async def f():\n print(* [] or [1])\n print(**{1: 3} if False else {x: x for x in range(3)})\n print(* lambda x: x)\n+assert(not Test),(\"Short message\")\n+assert this is ComplexTest and not requirements.fit_in_a_single_line(force=False), \"Short message\"\n+assert(((parens is TooMany)))\n for x, in (1,), (2,), (3,): ...\n for y in (): ...\n for z in (i for i in (1, 2, 3)): ...\n@@ -419,6 +422,11 @@ async def f():\n print(*[] or [1])\n print(**{1: 3} if False else {x: x for x in range(3)})\n print(*lambda x: x)\n+assert not Test, \"Short message\"\n+assert (\n+    this is ComplexTest and not requirements.fit_in_a_single_line(force=False)\n+), \"Short message\"\n+assert parens is TooMany\n for (x,) in (1,), (2,), (3,):\n     ...\n for y in ():"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 154,
                "created_at": "2018-04-21T00:29:30Z",
                "closed_at": "2018-04-24T18:55:42Z",
                "title": "Adds blank lines after comment between decorators",
                "labels": "T: bug, F: comments, F: empty lines",
                "commits": [
                    {
                        "hash": "52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                        "commit_date": "2018-04-24T18:50:31Z",
                        "parents": "29e97d1d4a7717f1bd0ca35cacf2f2ce6d815b0c",
                        "stat": {
                            "total": 29,
                            "additions": 91,
                            "deletions": 62,
                            "files": [
                                {
                                    "sha": "e2f91effd32fed6093fc4c0f79057ce6bcaa0061",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 5,
                                    "changes": 11,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -230,11 +230,9 @@ are always reformatted to fit minimal space, this whitespace is lost.\n \n It will also insert proper spacing before and after function definitions.\n It's one line before and after inner functions and two lines before and\n-after module-level functions.  *Black* will put those empty lines also\n-between the function definition and any standalone comments that\n-immediately precede the given function.  If you want to comment on the\n-entire function, use a docstring or put a leading comment in the function\n-body.\n+after module-level functions.  *Black* will not put empty lines between\n+function/class definitions and standalone comments that immediately precede\n+the given function/class.\n \n \n ### Trailing commas\n@@ -532,6 +530,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed comment indentation when a standalone comment closes a block (#16, #32)\n \n+* fixed standalone comments receiving extra empty lines if immediately preceding\n+  a class, def, or decorator (#56, #154)\n+\n * fixed `--diff` not showing entire path (#130)\n \n * fixed parsing of complex expressions after star and double stars in"
                                },
                                {
                                    "sha": "11386d23f9df76ad9b57129ea2bb28eba4792f5d",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 4,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -1040,12 +1040,14 @@ def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\n                 # Don't insert empty lines before the first line in the file.\n                 return 0, 0\n \n-            if self.previous_line and self.previous_line.is_decorator:\n-                # Don't insert empty lines between decorators.\n+            if self.previous_line.is_decorator:\n                 return 0, 0\n \n-            if is_decorator and self.previous_line and self.previous_line.is_comment:\n-                # Don't insert empty lines between decorator comments.\n+            if (\n+                self.previous_line.is_comment\n+                and self.previous_line.depth == current_line.depth\n+                and before == 0\n+            ):\n                 return 0, 0\n \n             newlines = 2"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "44a4711a6aa0a9a7f0fe40992b5c1175a1d42b46",
                                    "filename": "tests/comments2.py",
                                    "status": "modified",
                                    "additions": 0,
                                    "deletions": 2,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fcomments2.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fcomments2.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomments2.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -161,8 +161,6 @@ def inline_comments_in_brackets_ruin_everything():\n     # add_compiler(compilers[(7.1, 64)])\n \n # Comment before function.\n-\n-\n def inline_comments_in_brackets_ruin_everything():\n     if typedargslist:\n         parameters.children = ["
                                },
                                {
                                    "sha": "d83b6b8ff47cc6f731aee0f7a3369f3d6e7db960",
                                    "filename": "tests/comments5.py",
                                    "status": "modified",
                                    "additions": 40,
                                    "deletions": 0,
                                    "changes": 40,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fcomments5.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fcomments5.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomments5.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -27,5 +27,45 @@\n except OSError:\n     print(\"problems\")\n \n+import sys\n+\n+\n+# leading function comment\n+def wat():\n+    ...\n+    # trailing function comment\n+\n+\n+# SECTION COMMENT\n+\n+\n+# leading 1\n+@deco1\n+# leading 2\n+@deco2(with_args=True)\n+# leading 3\n+@deco3\n+def decorated1():\n+    ...\n+\n+\n+# leading 1\n+@deco1\n+# leading 2\n+@deco2(with_args=True)\n+# leading function comment\n+def decorated1():\n+    ...\n+\n+\n+# Note: crappy but inevitable.  The current design of EmptyLineTracker doesn't\n+# allow this to work correctly.  The user will have to split those lines by\n+# hand.\n+some_instruction\n+# This comment should be split from `some_instruction` by two lines but isn't.\n+def g():\n+    ...\n+\n+\n if __name__ == \"__main__\":\n     main()"
                                },
                                {
                                    "sha": "05650156fc8646abc64275efe73fec682a183d05",
                                    "filename": "tests/comments6.py",
                                    "status": "removed",
                                    "additions": 0,
                                    "deletions": 8,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/29e97d1d4a7717f1bd0ca35cacf2f2ce6d815b0c/tests%2Fcomments6.py",
                                    "raw_url": "https://github.com/psf/black/raw/29e97d1d4a7717f1bd0ca35cacf2f2ce6d815b0c/tests%2Fcomments6.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomments6.py?ref=29e97d1d4a7717f1bd0ca35cacf2f2ce6d815b0c",
                                    "patch": "@@ -1,8 +0,0 @@\n-@property\n-# TODO: X\n-@property\n-# TODO: Y\n-# TODO: Z\n-@property\n-def foo():\n-    pass"
                                },
                                {
                                    "sha": "5b7ce928a783ed27c4fd23446680a4989a8a3c0c",
                                    "filename": "tests/empty_lines.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fempty_lines.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Fempty_lines.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fempty_lines.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -1,3 +1,7 @@\n+\"\"\"Docstring.\"\"\"\n+\n+\n+# leading comment\n def f():\n     NO = ''\n     SPACE = ' '\n@@ -44,9 +48,11 @@ def f():\n                 syms.dictsetmaker,\n             }:\n                 return NO\n+\n ###############################################################################\n # SECTION BECAUSE SECTIONS\n ###############################################################################\n+\n def g():\n     NO = ''\n     SPACE = ' '\n@@ -89,6 +95,10 @@ def g():\n # output\n \n \n+\"\"\"Docstring.\"\"\"\n+\n+\n+# leading comment\n def f():\n     NO = \"\"\n     SPACE = \" \""
                                },
                                {
                                    "sha": "a7b9bc744ed07148c3f7d445e22244b6ac90238c",
                                    "filename": "tests/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 0,
                                    "deletions": 2,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffmtonoff.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -119,8 +119,6 @@ async def coroutine(arg, exec=False):\n def function_signature_stress_test(number:int,no_annotation=None,text:str='default',* ,debug:bool=False,**kwargs) -> str:\n  return text[number:-1]\n # fmt: on\n-\n-\n def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h=\"\", i=r\"\"):\n     offset = attr.ib(default=attr.Factory(lambda: _r.uniform(10000, 200000)))\n     assert task._cancel_stack[:len(old_stack)] == old_stack"
                                },
                                {
                                    "sha": "dd3beed99abe0e5b6d7b3f37188fa41420f38493",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 0,
                                    "deletions": 8,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=52fda8b0e9e52e94aae6cb3170c9b1b492a2d8b4",
                                    "patch": "@@ -626,14 +626,6 @@ def test_check_diff_use_together(self) -> None:\n             )\n             self.assertEqual(result.exit_code, 1)\n \n-    @patch(\"black.dump_to_file\", dump_to_stderr)\n-    def test_comment_in_decorator(self) -> None:\n-        source, expected = read_data(\"comments6\")\n-        actual = fs(source)\n-        self.assertFormatEqual(expected, actual)\n-        black.assert_equivalent(source, actual)\n-        black.assert_stable(source, actual, line_length=ll)\n-\n \n if __name__ == \"__main__\":\n     unittest.main()"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 141,
                "created_at": "2018-04-18T00:07:04Z",
                "closed_at": "2018-04-24T19:22:13Z",
                "title": "Black does not follow line length rule with ternary expression",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "9d671bdbe13ab68cea1bba15001c43e90cf2c1a6",
                        "commit_date": "2018-04-24T19:21:56Z",
                        "parents": "92957a41e3f909c7b813b448f65cd437cf0139f2",
                        "stat": {
                            "total": 3,
                            "additions": 29,
                            "deletions": 26,
                            "files": [
                                {
                                    "sha": "4663176726b668c45fc80ff0b12430e651af230c",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=9d671bdbe13ab68cea1bba15001c43e90cf2c1a6",
                                    "patch": "@@ -540,6 +540,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed invalid splitting on comma in lambda arguments (#133)\n \n+* fixed missing splits of ternary expressions (#141)\n+\n ### 18.4a2\n \n * fixed parsing of unaligned standalone comments (#99, #112)"
                                },
                                {
                                    "sha": "5bf466af91c5e0eb03f293621ab00346dcfc00c3",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/psf/black/blob/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=9d671bdbe13ab68cea1bba15001c43e90cf2c1a6",
                                    "patch": "@@ -583,6 +583,7 @@ def show(cls, code: str) -> None:\n }\n COMPREHENSION_PRIORITY = 20\n COMMA_PRIORITY = 10\n+TERNARY_PRIORITY = 7\n LOGIC_PRIORITY = 5\n STRING_PRIORITY = 4\n COMPARATOR_PRIORITY = 3\n@@ -1602,6 +1603,14 @@ def is_split_before_delimiter(leaf: Leaf, previous: Leaf = None) -> int:\n     ):\n         return COMPREHENSION_PRIORITY\n \n+    if (\n+        leaf.type == token.NAME\n+        and leaf.value in {\"if\", \"else\"}\n+        and leaf.parent\n+        and leaf.parent.type == syms.test\n+    ):\n+        return TERNARY_PRIORITY\n+\n     if leaf.type == token.NAME and leaf.value in LOGIC_OPERATORS and leaf.parent:\n         return LOGIC_PRIORITY\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "da48a13a1936c62f1fde517473eebc140a6e33a7",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 3,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=9d671bdbe13ab68cea1bba15001c43e90cf2c1a6",
                                    "patch": "@@ -11,7 +11,7 @@\n  True\n  False\n  1\n-@@ -29,60 +29,78 @@\n+@@ -29,61 +29,83 @@\n  ~great\n  +value\n  -1\n@@ -37,8 +37,14 @@\n  (str or None) if True else (str or bytes or None)\n  str or None if (1 if True else 2) else str or bytes or None\n  (str or None) if (1 if True else 2) else (str or bytes or None)\n+-((super_long_variable_name or None) if (1 if super_long_test_name else 2) else (str or bytes or None))\n -{'2.7': dead, '3.7': (long_live or die_hard)}\n -{'2.7': dead, '3.7': (long_live or die_hard), **{'3.6': verygood}}\n++(\n++    (super_long_variable_name or None)\n++    if (1 if super_long_test_name else 2)\n++    else (str or bytes or None)\n++)\n +{\"2.7\": dead, \"3.7\": (long_live or die_hard)}\n +{\"2.7\": dead, \"3.7\": (long_live or die_hard), **{\"3.6\": verygood}}\n  {**a, **b, **c}\n@@ -110,7 +116,7 @@\n  call(**self.screen_kwargs)\n  call(b, **self.screen_kwargs)\n  lukasz.langa.pl\n-@@ -91,11 +109,11 @@\n+@@ -92,11 +114,11 @@\n  1.0 .real\n  ....__class__\n  list[str]\n@@ -123,7 +129,7 @@\n  ]\n  slice[0]\n  slice[0:1]\n-@@ -122,88 +140,122 @@\n+@@ -123,88 +145,122 @@\n  numpy[-(c + 1):, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]"
                                },
                                {
                                    "sha": "2c4d8ddbd75ca3d531a89c91231fc58b8dfeb9e2",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/9d671bdbe13ab68cea1bba15001c43e90cf2c1a6/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=9d671bdbe13ab68cea1bba15001c43e90cf2c1a6",
                                    "patch": "@@ -43,6 +43,7 @@\n (str or None) if True else (str or bytes or None)\n str or None if (1 if True else 2) else str or bytes or None\n (str or None) if (1 if True else 2) else (str or bytes or None)\n+((super_long_variable_name or None) if (1 if super_long_test_name else 2) else (str or bytes or None))\n {'2.7': dead, '3.7': (long_live or die_hard)}\n {'2.7': dead, '3.7': (long_live or die_hard), **{'3.6': verygood}}\n {**a, **b, **c}\n@@ -260,6 +261,11 @@ async def f():\n (str or None) if True else (str or bytes or None)\n str or None if (1 if True else 2) else str or bytes or None\n (str or None) if (1 if True else 2) else (str or bytes or None)\n+(\n+    (super_long_variable_name or None)\n+    if (1 if super_long_test_name else 2)\n+    else (str or bytes or None)\n+)\n {\"2.7\": dead, \"3.7\": (long_live or die_hard)}\n {\"2.7\": dead, \"3.7\": (long_live or die_hard), **{\"3.6\": verygood}}\n {**a, **b, **c}"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 133,
                "created_at": "2018-04-15T13:12:26Z",
                "closed_at": "2018-04-21T22:10:19Z",
                "title": "Failure to reformat a lamba expression",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "d73166c42b7de83249d17125cae3c2594a25b2c3",
                        "commit_date": "2018-04-21T22:08:36Z",
                        "parents": "b5b658da0683c3e0806461946d8b492784e26d97",
                        "stat": {
                            "total": 31,
                            "additions": 99,
                            "deletions": 68,
                            "files": [
                                {
                                    "sha": "5be349e42f28bbb5c5799ffd13f95de9bd984a2e",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/d73166c42b7de83249d17125cae3c2594a25b2c3/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/d73166c42b7de83249d17125cae3c2594a25b2c3/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=d73166c42b7de83249d17125cae3c2594a25b2c3",
                                    "patch": "@@ -516,9 +516,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * generalized star expression handling, including double stars; this\n   fixes multiplication making expressions \"unsafe\" for trailing commas (#132)\n \n-* fix parsing of complex expressions after star and double stars in\n+* fixed parsing of complex expressions after star and double stars in\n   function parameters (#2)\n \n+* fixed invalid splitting on comma in lambda arguments (#133)\n+\n ### 18.4a2\n \n * fixed parsing of unaligned standalone comments (#99, #112)"
                                },
                                {
                                    "sha": "58f7976aa60bc72b0ceb72b5a160a6107bf6637b",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 50,
                                    "deletions": 27,
                                    "changes": 77,
                                    "blob_url": "https://github.com/psf/black/blob/d73166c42b7de83249d17125cae3c2594a25b2c3/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/d73166c42b7de83249d17125cae3c2594a25b2c3/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=d73166c42b7de83249d17125cae3c2594a25b2c3",
                                    "patch": "@@ -609,6 +609,8 @@ class BracketTracker:\n     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)\n     delimiters: Dict[LeafID, Priority] = Factory(dict)\n     previous: Optional[Leaf] = None\n+    _for_loop_variable: bool = False\n+    _lambda_arguments: bool = False\n \n     def mark(self, leaf: Leaf) -> None:\n         \"\"\"Mark `leaf` with bracket-related metadata. Keep track of delimiters.\n@@ -628,6 +630,8 @@ def mark(self, leaf: Leaf) -> None:\n         if leaf.type == token.COMMENT:\n             return\n \n+        self.maybe_decrement_after_for_loop_variable(leaf)\n+        self.maybe_decrement_after_lambda_arguments(leaf)\n         if leaf.type in CLOSING_BRACKETS:\n             self.depth -= 1\n             opening_bracket = self.bracket_match.pop((self.depth, leaf.type))\n@@ -645,6 +649,8 @@ def mark(self, leaf: Leaf) -> None:\n             self.bracket_match[self.depth, BRACKET[leaf.type]] = leaf\n             self.depth += 1\n         self.previous = leaf\n+        self.maybe_increment_lambda_arguments(leaf)\n+        self.maybe_increment_for_loop_variable(leaf)\n \n     def any_open_brackets(self) -> bool:\n         \"\"\"Return True if there is an yet unmatched open bracket on the line.\"\"\"\n@@ -658,6 +664,50 @@ def max_delimiter_priority(self, exclude: Iterable[LeafID] = ()) -> int:\n         \"\"\"\n         return max(v for k, v in self.delimiters.items() if k not in exclude)\n \n+    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n+        \"\"\"In a for loop, or comprehension, the variables are often unpacks.\n+\n+        To avoid splitting on the comma in this situation, increase the depth of\n+        tokens between `for` and `in`.\n+        \"\"\"\n+        if leaf.type == token.NAME and leaf.value == \"for\":\n+            self.depth += 1\n+            self._for_loop_variable = True\n+            return True\n+\n+        return False\n+\n+    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\n+        \"\"\"See `maybe_increment_for_loop_variable` above for explanation.\"\"\"\n+        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n+            self.depth -= 1\n+            self._for_loop_variable = False\n+            return True\n+\n+        return False\n+\n+    def maybe_increment_lambda_arguments(self, leaf: Leaf) -> bool:\n+        \"\"\"In a lambda expression, there might be more than one argument.\n+\n+        To avoid splitting on the comma in this situation, increase the depth of\n+        tokens between `lambda` and `:`.\n+        \"\"\"\n+        if leaf.type == token.NAME and leaf.value == \"lambda\":\n+            self.depth += 1\n+            self._lambda_arguments = True\n+            return True\n+\n+        return False\n+\n+    def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:\n+        \"\"\"See `maybe_increment_lambda_arguments` above for explanation.\"\"\"\n+        if self._lambda_arguments and leaf.type == token.COLON:\n+            self.depth -= 1\n+            self._lambda_arguments = False\n+            return True\n+\n+        return False\n+\n \n @dataclass\n class Line:\n@@ -668,8 +718,6 @@ class Line:\n     comments: List[Tuple[Index, Leaf]] = Factory(list)\n     bracket_tracker: BracketTracker = Factory(BracketTracker)\n     inside_brackets: bool = False\n-    has_for: bool = False\n-    _for_loop_variable: bool = False\n \n     def append(self, leaf: Leaf, preformatted: bool = False) -> None:\n         \"\"\"Add a new `leaf` to the end of the line.\n@@ -690,10 +738,8 @@ def append(self, leaf: Leaf, preformatted: bool = False) -> None:\n             # imports, for which we only preserve newlines.\n             leaf.prefix += whitespace(leaf)\n         if self.inside_brackets or not preformatted:\n-            self.maybe_decrement_after_for_loop_variable(leaf)\n             self.bracket_tracker.mark(leaf)\n             self.maybe_remove_trailing_comma(leaf)\n-            self.maybe_increment_for_loop_variable(leaf)\n \n         if not self.append_comment(leaf):\n             self.leaves.append(leaf)\n@@ -840,29 +886,6 @@ def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n \n         return False\n \n-    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:\n-        \"\"\"In a for loop, or comprehension, the variables are often unpacks.\n-\n-        To avoid splitting on the comma in this situation, increase the depth of\n-        tokens between `for` and `in`.\n-        \"\"\"\n-        if leaf.type == token.NAME and leaf.value == \"for\":\n-            self.has_for = True\n-            self.bracket_tracker.depth += 1\n-            self._for_loop_variable = True\n-            return True\n-\n-        return False\n-\n-    def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:\n-        \"\"\"See `maybe_increment_for_loop_variable` above for explanation.\"\"\"\n-        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == \"in\":\n-            self.bracket_tracker.depth -= 1\n-            self._for_loop_variable = False\n-            return True\n-\n-        return False\n-\n     def append_comment(self, comment: Leaf) -> bool:\n         \"\"\"Add an inline or standalone comment to the line.\"\"\"\n         if ("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "9da00484c384d101bf8c088378ec8f4883c5572a",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 3,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/d73166c42b7de83249d17125cae3c2594a25b2c3/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/d73166c42b7de83249d17125cae3c2594a25b2c3/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=d73166c42b7de83249d17125cae3c2594a25b2c3",
                                    "patch": "@@ -11,7 +11,7 @@\n  True\n  False\n  1\n-@@ -29,59 +29,73 @@\n+@@ -29,60 +29,78 @@\n  ~great\n  +value\n  -1\n@@ -24,8 +24,14 @@\n  lambda a, b, c=True: a\n -lambda a, b, c=True, *, d=(1 << v2), e='str': a\n -lambda a, b, c=True, *vararg, d=(v1 << 2), e='str', **kwargs: a + b\n+-foo = (lambda port_id, ignore_missing: {\"port1\": port1_resource, \"port2\": port2_resource}[port_id])\n +lambda a, b, c=True, *, d=(1 << v2), e=\"str\": a\n +lambda a, b, c=True, *vararg, d=(v1 << 2), e=\"str\", **kwargs: a + b\n++foo = (\n++    lambda port_id, ignore_missing: {\"port1\": port1_resource, \"port2\": port2_resource}[\n++        port_id\n++    ]\n++)\n  1 if True else 2\n  str or None if True else str or bytes or None\n  (str or None) if True else (str or bytes or None)\n@@ -104,7 +110,7 @@\n  call(**self.screen_kwargs)\n  call(b, **self.screen_kwargs)\n  lukasz.langa.pl\n-@@ -90,11 +104,11 @@\n+@@ -91,11 +109,11 @@\n  1.0 .real\n  ....__class__\n  list[str]\n@@ -117,7 +123,7 @@\n  ]\n  slice[0]\n  slice[0:1]\n-@@ -121,88 +135,122 @@\n+@@ -122,88 +140,122 @@\n  numpy[-(c + 1):, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]"
                                },
                                {
                                    "sha": "c67505fc1fbf3d42370b2ab0b93bccabdbb29743",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/d73166c42b7de83249d17125cae3c2594a25b2c3/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/d73166c42b7de83249d17125cae3c2594a25b2c3/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=d73166c42b7de83249d17125cae3c2594a25b2c3",
                                    "patch": "@@ -37,6 +37,7 @@\n lambda a, b, c=True: a\n lambda a, b, c=True, *, d=(1 << v2), e='str': a\n lambda a, b, c=True, *vararg, d=(v1 << 2), e='str', **kwargs: a + b\n+foo = (lambda port_id, ignore_missing: {\"port1\": port1_resource, \"port2\": port2_resource}[port_id])\n 1 if True else 2\n str or None if True else str or bytes or None\n (str or None) if True else (str or bytes or None)\n@@ -249,6 +250,11 @@ async def f():\n lambda a, b, c=True: a\n lambda a, b, c=True, *, d=(1 << v2), e=\"str\": a\n lambda a, b, c=True, *vararg, d=(v1 << 2), e=\"str\", **kwargs: a + b\n+foo = (\n+    lambda port_id, ignore_missing: {\"port1\": port1_resource, \"port2\": port2_resource}[\n+        port_id\n+    ]\n+)\n 1 if True else 2\n str or None if True else str or bytes or None\n (str or None) if True else (str or bytes or None)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 132,
                "created_at": "2018-04-15T09:47:59Z",
                "closed_at": "2018-04-16T08:34:56Z",
                "title": "Trailing comma is removed in complex dict literal",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                        "commit_date": "2018-04-16T08:34:15Z",
                        "parents": "f294cc272c3aa5b8d3cd35b9f9283721b7e458d8",
                        "stat": {
                            "total": 32,
                            "additions": 159,
                            "deletions": 127,
                            "files": [
                                {
                                    "sha": "e65d9810c84658fef86561c17f252f0693177721",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -491,6 +491,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ## Change Log\n \n+### 18.4a3\n+\n+* generalized star expression handling, including double stars; this\n+  fixes multiplication making expressions \"unsafe\" for trailing commas (#132)\n+\n ### 18.4a2\n \n * fixed parsing of unaligned standalone comments (#99, #112)"
                                },
                                {
                                    "sha": "c3610c74b12a43ecf654a9e08cadaa3cc8f9a5f7",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 42,
                                    "deletions": 21,
                                    "changes": 63,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -522,7 +522,20 @@ def show(cls, code: str) -> None:\n     token.DOUBLESTAR,\n     token.DOUBLESLASH,\n }\n-VARARGS = {token.STAR, token.DOUBLESTAR}\n+STARS = {token.STAR, token.DOUBLESTAR}\n+VARARGS_PARENTS = {\n+    syms.arglist,\n+    syms.argument,  # double star in arglist\n+    syms.trailer,  # single argument to call\n+    syms.typedargslist,\n+    syms.varargslist,  # lambdas\n+}\n+UNPACKING_PARENTS = {\n+    syms.atom,  # single element of a list or set literal\n+    syms.dictsetmaker,\n+    syms.listmaker,\n+    syms.testlist_gexp,\n+}\n COMPREHENSION_PRIORITY = 20\n COMMA_PRIORITY = 10\n LOGIC_PRIORITY = 5\n@@ -1255,18 +1268,8 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n                     # that, too.\n                     return prevp.prefix\n \n-        elif prevp.type == token.DOUBLESTAR:\n-            if (\n-                prevp.parent\n-                and prevp.parent.type in {\n-                    syms.arglist,\n-                    syms.argument,\n-                    syms.dictsetmaker,\n-                    syms.parameters,\n-                    syms.typedargslist,\n-                    syms.varargslist,\n-                }\n-            ):\n+        elif prevp.type in STARS:\n+            if is_vararg(prevp, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n                 return NO\n \n         elif prevp.type == token.COLON:\n@@ -1275,7 +1278,7 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n \n         elif (\n             prevp.parent\n-            and prevp.parent.type in {syms.factor, syms.star_expr}\n+            and prevp.parent.type == syms.factor\n             and prevp.type in MATH_OPERATORS\n         ):\n             return NO\n@@ -1496,11 +1499,7 @@ def is_split_before_delimiter(leaf: Leaf, previous: Leaf = None) -> int:\n \n     Higher numbers are higher priority.\n     \"\"\"\n-    if (\n-        leaf.type in VARARGS\n-        and leaf.parent\n-        and leaf.parent.type in {syms.argument, syms.typedargslist, syms.dictsetmaker}\n-    ):\n+    if is_vararg(leaf, within=VARARGS_PARENTS | UNPACKING_PARENTS):\n         # * and ** might also be MATH_OPERATORS but in this case they are not.\n         # Don't treat them as a delimiter.\n         return 0\n@@ -1878,8 +1877,7 @@ def append_to_line(leaf: Leaf) -> Iterator[Line]:\n         lowest_depth = min(lowest_depth, leaf.bracket_depth)\n         if (\n             leaf.bracket_depth == lowest_depth\n-            and leaf.type == token.STAR\n-            or leaf.type == token.DOUBLESTAR\n+            and is_vararg(leaf, within=VARARGS_PARENTS)\n         ):\n             trailing_comma_safe = trailing_comma_safe and py36\n         leaf_priority = delimiters.get(id(leaf))\n@@ -2090,6 +2088,29 @@ def is_one_tuple(node: LN) -> bool:\n     )\n \n \n+def is_vararg(leaf: Leaf, within: Set[NodeType]) -> bool:\n+    \"\"\"Return True if `leaf` is a star or double star in a vararg or kwarg.\n+\n+    If `within` includes VARARGS_PARENTS, this applies to function signatures.\n+    If `within` includes COLLECTION_LIBERALS_PARENTS, it applies to right\n+    hand-side extended iterable unpacking (PEP 3132) and additional unpacking\n+    generalizations (PEP 448).\n+    \"\"\"\n+    if leaf.type not in STARS or not leaf.parent:\n+        return False\n+\n+    p = leaf.parent\n+    if p.type == syms.star_expr:\n+        # Star expressions are also used as assignment targets in extended\n+        # iterable unpacking (PEP 3132).  See what its parent is instead.\n+        if not p.parent:\n+            return False\n+\n+        p = p.parent\n+\n+    return p.type in within\n+\n+\n def max_delimiter_priority_in_atom(node: LN) -> int:\n     if node.type != syms.atom:\n         return 0"
                                },
                                {
                                    "sha": "8e9936dfaa50d28a024142a19fe8871ded38838c",
                                    "filename": "docs/reference/reference_functions.rst",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/docs%2Freference%2Freference_functions.rst",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/docs%2Freference%2Freference_functions.rst",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/docs%2Freference%2Freference_functions.rst?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -26,6 +26,8 @@ Assertions and checks\n \n .. autofunction:: black.is_python36\n \n+.. autofunction:: black.is_vararg\n+\n Formatting\n ----------\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "bc34c0ed496e4f289d7f468be5fef61eccd9b85e",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 7,
                                    "changes": 34,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -11,7 +11,7 @@\n  True\n  False\n  1\n-@@ -29,65 +29,74 @@\n+@@ -29,59 +29,73 @@\n  ~great\n  +value\n  -1\n@@ -48,6 +48,19 @@\n  [1, 2, 3, 4, 5, 6, 7, 8, 9, (10 or A), (11 or B), (12 or C)]\n -[1, 2, 3,]\n +[1, 2, 3]\n+ [*a]\n+ [*range(10)]\n+-[*a, 4, 5,]\n+-[4, *a, 5,]\n+-[this_is_a_very_long_variable_which_will_force_a_delimiter_split, element, another, *more]\n++[*a, 4, 5]\n++[4, *a, 5]\n++[\n++    this_is_a_very_long_variable_which_will_force_a_delimiter_split,\n++    element,\n++    another,\n++    *more,\n++]\n  {i for i in (1, 2, 3)}\n  {(i ** 2) for i in (1, 2, 3)}\n -{(i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c'))}\n@@ -87,10 +100,11 @@\n +    **kwargs\n +)  # note: no trailing comma pre-3.6\n  call(*gidgets[:2])\n+ call(a, *gidgets[:2])\n  call(**self.screen_kwargs)\n+ call(b, **self.screen_kwargs)\n  lukasz.langa.pl\n- call.me(maybe)\n- 1 .real\n+@@ -90,11 +104,11 @@\n  1.0 .real\n  ....__class__\n  list[str]\n@@ -103,7 +117,7 @@\n  ]\n  slice[0]\n  slice[0:1]\n-@@ -114,79 +123,113 @@\n+@@ -121,85 +135,119 @@\n  numpy[-(c + 1):, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]\n@@ -123,16 +137,16 @@\n +((i ** 2) for i, _ in ((1, \"a\"), (2, \"b\"), (3, \"c\")))\n  (((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3))\n  (*starred)\n--{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}  # no trailing comma, this file is not 3.6+\n+-{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}\n +{\n +    \"id\": \"1\",\n +    \"type\": \"type\",\n +    \"started_at\": now(),\n +    \"ended_at\": now() + timedelta(days=10),\n +    \"priority\": 1,\n +    \"import_session_id\": 1,\n-+    **kwargs\n-+}  # no trailing comma, this file is not 3.6+\n++    **kwargs,\n++}\n  a = (1,)\n  b = 1,\n  c = 1\n@@ -154,6 +168,12 @@\n +).all()\n  \u00d8 = set()\n  authors.\u0142ukasz.say_thanks()\n+ mapping = {\n+     A: 0.25 * (10.0 / 12),\n+     B: 0.1 * (10.0 / 12),\n+     C: 0.1 * (10.0 / 12),\n+     D: 0.1 * (10.0 / 12),\n+ }\n  \n +\n  def gen():"
                                },
                                {
                                    "sha": "9c071771f7676ba528d5f98c480e874f759f99a5",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 34,
                                    "deletions": 3,
                                    "changes": 37,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -54,6 +54,11 @@\n []\n [1, 2, 3, 4, 5, 6, 7, 8, 9, (10 or A), (11 or B), (12 or C)]\n [1, 2, 3,]\n+[*a]\n+[*range(10)]\n+[*a, 4, 5,]\n+[4, *a, 5,]\n+[this_is_a_very_long_variable_which_will_force_a_delimiter_split, element, another, *more]\n {i for i in (1, 2, 3)}\n {(i ** 2) for i in (1, 2, 3)}\n {(i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c'))}\n@@ -76,7 +81,9 @@\n call(arg, another, kwarg='hey', **kwargs)\n call(this_is_a_very_long_variable_which_will_force_a_delimiter_split, arg, another, kwarg='hey', **kwargs)  # note: no trailing comma pre-3.6\n call(*gidgets[:2])\n+call(a, *gidgets[:2])\n call(**self.screen_kwargs)\n+call(b, **self.screen_kwargs)\n lukasz.langa.pl\n call.me(maybe)\n 1 .real\n@@ -127,7 +134,7 @@\n ((i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c')))\n (((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3))\n (*starred)\n-{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}  # no trailing comma, this file is not 3.6+\n+{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}\n a = (1,)\n b = 1,\n c = 1\n@@ -138,6 +145,12 @@\n result = session.query(models.Customer.id).filter(models.Customer.account_id == account_id, models.Customer.email == email_address).order_by(models.Customer.id.asc(),).all()\n \u00d8 = set()\n authors.\u0142ukasz.say_thanks()\n+mapping = {\n+    A: 0.25 * (10.0 / 12),\n+    B: 0.1 * (10.0 / 12),\n+    C: 0.1 * (10.0 / 12),\n+    D: 0.1 * (10.0 / 12),\n+}\n \n def gen():\n     yield from outside_of_generator\n@@ -250,6 +263,16 @@ async def f():\n []\n [1, 2, 3, 4, 5, 6, 7, 8, 9, (10 or A), (11 or B), (12 or C)]\n [1, 2, 3]\n+[*a]\n+[*range(10)]\n+[*a, 4, 5]\n+[4, *a, 5]\n+[\n+    this_is_a_very_long_variable_which_will_force_a_delimiter_split,\n+    element,\n+    another,\n+    *more,\n+]\n {i for i in (1, 2, 3)}\n {(i ** 2) for i in (1, 2, 3)}\n {(i ** 2) for i, _ in ((1, \"a\"), (2, \"b\"), (3, \"c\"))}\n@@ -281,7 +304,9 @@ async def f():\n     **kwargs\n )  # note: no trailing comma pre-3.6\n call(*gidgets[:2])\n+call(a, *gidgets[:2])\n call(**self.screen_kwargs)\n+call(b, **self.screen_kwargs)\n lukasz.langa.pl\n call.me(maybe)\n 1 .real\n@@ -339,8 +364,8 @@ async def f():\n     \"ended_at\": now() + timedelta(days=10),\n     \"priority\": 1,\n     \"import_session_id\": 1,\n-    **kwargs\n-}  # no trailing comma, this file is not 3.6+\n+    **kwargs,\n+}\n a = (1,)\n b = 1,\n c = 1\n@@ -359,6 +384,12 @@ async def f():\n ).all()\n \u00d8 = set()\n authors.\u0142ukasz.say_thanks()\n+mapping = {\n+    A: 0.25 * (10.0 / 12),\n+    B: 0.1 * (10.0 / 12),\n+    C: 0.1 * (10.0 / 12),\n+    D: 0.1 * (10.0 / 12),\n+}\n \n \n def gen():"
                                },
                                {
                                    "sha": "08f9414a7b5041452573b7c6bc6e4c2f21e3cce3",
                                    "filename": "tests/function.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 0,
                                    "changes": 16,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Ffunction.py",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Ffunction.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffunction.py?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -74,6 +74,13 @@ def long_lines():\n         $\n         \"\"\", re.MULTILINE | re.VERBOSE\n     )\n+def trailing_comma():\n+    mapping = {\n+    A: 0.25 * (10.0 / 12),\n+    B: 0.1 * (10.0 / 12),\n+    C: 0.1 * (10.0 / 12),\n+    D: 0.1 * (10.0 / 12),\n+}\n \n # output\n \n@@ -198,3 +205,12 @@ def long_lines():\n         \"\"\",\n         re.MULTILINE | re.VERBOSE,\n     )\n+\n+\n+def trailing_comma():\n+    mapping = {\n+        A: 0.25 * (10.0 / 12),\n+        B: 0.1 * (10.0 / 12),\n+        C: 0.1 * (10.0 / 12),\n+        D: 0.1 * (10.0 / 12),\n+    }"
                                },
                                {
                                    "sha": "a4d23823c9d2164cfb0176d8544ca01e6d23fc47",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/a764f1bb3b459ee6f2e752e3d67793b119a2144a/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=a764f1bb3b459ee6f2e752e3d67793b119a2144a",
                                    "patch": "@@ -179,7 +179,7 @@ def test_expression_diff(self) -> None:\n             msg = (\n                 f\"Expected diff isn't equal to the actual. If you made changes \"\n                 f\"to expression.py and this is an anticipated difference, \"\n-                f\"overwrite tests/expression.diff with {dump}.\"\n+                f\"overwrite tests/expression.diff with {dump}\"\n             )\n             self.assertEqual(expected, actual, msg)\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 130,
                "created_at": "2018-04-14T21:58:59Z",
                "closed_at": "2018-04-23T19:35:30Z",
                "title": "--diff doesn't display full file path",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "06e95b1e9bcd43c4574840f8174ba4b2c5d281bd",
                        "commit_date": "2018-04-23T19:00:03Z",
                        "parents": "2e52a2b3ecc0fe025439c3db05a4457ab14f167b",
                        "stat": {
                            "total": 3,
                            "additions": 8,
                            "deletions": 5,
                            "files": [
                                {
                                    "sha": "9685172d7bf29fba584701b1a0bae670def97278",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=06e95b1e9bcd43c4574840f8174ba4b2c5d281bd",
                                    "patch": "@@ -518,6 +518,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * generalized star expression handling, including double stars; this\n   fixes multiplication making expressions \"unsafe\" for trailing commas (#132)\n \n+* fixed `--diff` not showing entire path (#130)\n+\n * fixed parsing of complex expressions after star and double stars in\n   function parameters (#2)\n "
                                },
                                {
                                    "sha": "eafc9e7be9251abaaee8fbef9b024e243b02a15c",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=06e95b1e9bcd43c4574840f8174ba4b2c5d281bd",
                                    "patch": "@@ -341,8 +341,8 @@ def format_file_in_place(\n         with open(src, \"w\", encoding=src_buffer.encoding) as f:\n             f.write(dst_contents)\n     elif write_back == write_back.DIFF:\n-        src_name = f\"{src.name}  (original)\"\n-        dst_name = f\"{src.name}  (formatted)\"\n+        src_name = f\"{src}  (original)\"\n+        dst_name = f\"{src}  (formatted)\"\n         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)\n         if lock:\n             lock.acquire()"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "6f0ffa364ac749e770e67549ee6e4420bfc8e739",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/06e95b1e9bcd43c4574840f8174ba4b2c5d281bd/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=06e95b1e9bcd43c4574840f8174ba4b2c5d281bd",
                                    "patch": "@@ -200,7 +200,7 @@ def test_expression_diff(self) -> None:\n             self.assertTrue(ff(tmp_file, write_back=black.WriteBack.DIFF))\n             sys.stdout.seek(0)\n             actual = sys.stdout.read()\n-            actual = actual.replace(tmp_file.name, \"<stdin>\")\n+            actual = actual.replace(str(tmp_file), \"<stdin>\")\n         finally:\n             sys.stdout = hold_stdout\n             os.unlink(tmp_file)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 112,
                "created_at": "2018-04-05T10:21:34Z",
                "closed_at": "2018-04-12T06:30:18Z",
                "title": "Cannot parse: 5:0:         # add_compiler(compiler)",
                "labels": "T: bug, F: comments",
                "commits": [
                    {
                        "hash": "9138a75b759ecb690d63924503f88bfbc82d4862",
                        "commit_date": "2018-04-12T06:22:22Z",
                        "parents": "1909d8cc6b6ef62ea285a69f6d8b25630d85f04a",
                        "stat": {
                            "total": 5,
                            "additions": 36,
                            "deletions": 31,
                            "files": [
                                {
                                    "sha": "d41da0cf49bcd8bed68b788bb8bf352fd7e75671",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/9138a75b759ecb690d63924503f88bfbc82d4862/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/9138a75b759ecb690d63924503f88bfbc82d4862/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=9138a75b759ecb690d63924503f88bfbc82d4862",
                                    "patch": "@@ -491,6 +491,10 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ## Change Log\n \n+### 18.4a2 (unreleased)\n+\n+* fixed parsing of unaligned standalone comments (#99, #112)\n+\n ### 18.4a1\n \n * added `--quiet` (#78)"
                                },
                                {
                                    "sha": "5cdd2e59e6156cecd8ffa3cebc822b4be20ef7ae",
                                    "filename": "blib2to3/pgen2/driver.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/9138a75b759ecb690d63924503f88bfbc82d4862/blib2to3%2Fpgen2%2Fdriver.py",
                                    "raw_url": "https://github.com/psf/black/raw/9138a75b759ecb690d63924503f88bfbc82d4862/blib2to3%2Fpgen2%2Fdriver.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Fdriver.py?ref=9138a75b759ecb690d63924503f88bfbc82d4862",
                                    "patch": "@@ -77,6 +77,12 @@ def parse_tokens(self, tokens, debug=False):\n                     self.logger.debug(\"Stop.\")\n                 break\n             prefix = \"\"\n+            if type == token.INDENT:\n+                if _prefix.startswith(value):\n+                    # Don't double-indent.  Since we're delaying the prefix that\n+                    # would normally belong to INDENT, we need to put the value\n+                    # at the end versus at the beginning.\n+                    _prefix = _prefix[len(value):] + value\n             if type in {token.INDENT, token.DEDENT}:\n                 prefix = _prefix\n             lineno, column = end"
                                },
                                {
                                    "sha": "bd9945498bd49adb40514134646de3d0a60c41b4",
                                    "filename": "blib2to3/pgen2/tokenize.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 4,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/9138a75b759ecb690d63924503f88bfbc82d4862/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "raw_url": "https://github.com/psf/black/raw/9138a75b759ecb690d63924503f88bfbc82d4862/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Ftokenize.py?ref=9138a75b759ecb690d63924503f88bfbc82d4862",
                                    "patch": "@@ -412,10 +412,6 @@ def generate_tokens(readline):\n                 yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n                 continue\n \n-            if column > indents[-1]:           # count indents\n-                indents.append(column)\n-                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n-\n             if line[pos] == '#':               # skip comments\n                 comment_token = line[pos:].rstrip('\\r\\n')\n                 nl_pos = pos + len(comment_token)\n@@ -425,6 +421,10 @@ def generate_tokens(readline):\n                         (lnum, nl_pos), (lnum, len(line)), line)\n                 continue\n \n+            if column > indents[-1]:           # count indents\n+                indents.append(column)\n+                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n+\n             while column < indents[-1]:        # count dedents\n                 if column not in indents:\n                     raise IndentationError("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "848ddb1ae8541a9dcf25b429e6481fe5da204cf4",
                                    "filename": "tests/comments2.py",
                                    "status": "modified",
                                    "additions": 17,
                                    "deletions": 1,
                                    "changes": 18,
                                    "blob_url": "https://github.com/psf/black/blob/9138a75b759ecb690d63924503f88bfbc82d4862/tests%2Fcomments2.py",
                                    "raw_url": "https://github.com/psf/black/raw/9138a75b759ecb690d63924503f88bfbc82d4862/tests%2Fcomments2.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fcomments2.py?ref=9138a75b759ecb690d63924503f88bfbc82d4862",
                                    "patch": "@@ -23,6 +23,14 @@\n     'Generator',\n ]\n \n+if 'PYTHON' in os.environ:\n+    add_compiler(compiler_from_env())\n+else:\n+    # for compiler in compilers.values():\n+         # add_compiler(compiler)\n+    add_compiler(compilers[(7.0, 32)])\n+    # add_compiler(compilers[(7.1, 64)])\n+\n # Comment before function.\n def inline_comments_in_brackets_ruin_everything():\n     if typedargslist:\n@@ -97,7 +105,7 @@ def inline_comments_in_brackets_ruin_everything():\n             # and round and round we go\n         # and round and round we go\n \n-    # let's return\n+   # let's return\n     return Node(\n         syms.simple_stmt,\n         [\n@@ -144,6 +152,14 @@ def inline_comments_in_brackets_ruin_everything():\n     \"Generator\",\n ]\n \n+if \"PYTHON\" in os.environ:\n+    add_compiler(compiler_from_env())\n+else:\n+    # for compiler in compilers.values():\n+    # add_compiler(compiler)\n+    add_compiler(compilers[(7.0, 32)])\n+# add_compiler(compilers[(7.1, 64)])\n+\n # Comment before function.\n \n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 111,
                "created_at": "2018-04-05T09:30:21Z",
                "closed_at": "2018-04-12T06:30:18Z",
                "title": "Formatting of unpacking inside dictionary literal",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "19d69b34e5e90589e4be4185852aab7135d59303",
                        "commit_date": "2018-04-12T06:22:22Z",
                        "parents": "e41844feb74fe266f4cc44f0b5ee57c03e138c2c",
                        "stat": {
                            "total": 2,
                            "additions": 26,
                            "deletions": 24,
                            "files": [
                                {
                                    "sha": "c68f703f7e45203b559965e2d6c335b3f96aa430",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/19d69b34e5e90589e4be4185852aab7135d59303/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/19d69b34e5e90589e4be4185852aab7135d59303/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=19d69b34e5e90589e4be4185852aab7135d59303",
                                    "patch": "@@ -495,6 +495,8 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n * fixed parsing of unaligned standalone comments (#99, #112)\n \n+* fixed placement of dictionary unpacking inside dictionary literals (#111)\n+\n ### 18.4a1\n \n * added `--quiet` (#78)"
                                },
                                {
                                    "sha": "537ba599807ffea932a223884a40488d22c8ab45",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/19d69b34e5e90589e4be4185852aab7135d59303/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/19d69b34e5e90589e4be4185852aab7135d59303/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=19d69b34e5e90589e4be4185852aab7135d59303",
                                    "patch": "@@ -1498,7 +1498,7 @@ def is_split_before_delimiter(leaf: Leaf, previous: Leaf = None) -> int:\n     if (\n         leaf.type in VARARGS\n         and leaf.parent\n-        and leaf.parent.type in {syms.argument, syms.typedargslist}\n+        and leaf.parent.type in {syms.argument, syms.typedargslist, syms.dictsetmaker}\n     ):\n         # * and ** might also be MATH_OPERATORS but in this case they are not.\n         # Don't treat them as a delimiter."
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "02bdc2ced3076c26d58ab48fd586f593b7b80962",
                                    "filename": "tests/expression.diff",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 1,
                                    "changes": 12,
                                    "blob_url": "https://github.com/psf/black/blob/19d69b34e5e90589e4be4185852aab7135d59303/tests%2Fexpression.diff",
                                    "raw_url": "https://github.com/psf/black/raw/19d69b34e5e90589e4be4185852aab7135d59303/tests%2Fexpression.diff",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.diff?ref=19d69b34e5e90589e4be4185852aab7135d59303",
                                    "patch": "@@ -103,7 +103,7 @@\n  ]\n  slice[0]\n  slice[0:1]\n-@@ -114,78 +123,104 @@\n+@@ -114,79 +123,113 @@\n  numpy[-(c + 1):, d]\n  numpy[:, l[-2]]\n  numpy[:, ::-1]\n@@ -123,6 +123,16 @@\n +((i ** 2) for i, _ in ((1, \"a\"), (2, \"b\"), (3, \"c\")))\n  (((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3))\n  (*starred)\n+-{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}  # no trailing comma, this file is not 3.6+\n++{\n++    \"id\": \"1\",\n++    \"type\": \"type\",\n++    \"started_at\": now(),\n++    \"ended_at\": now() + timedelta(days=10),\n++    \"priority\": 1,\n++    \"import_session_id\": 1,\n++    **kwargs\n++}  # no trailing comma, this file is not 3.6+\n  a = (1,)\n  b = 1,\n  c = 1"
                                },
                                {
                                    "sha": "357cbb63d248cf4497ab3c2527f950a9d1e75bf7",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/19d69b34e5e90589e4be4185852aab7135d59303/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/19d69b34e5e90589e4be4185852aab7135d59303/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=19d69b34e5e90589e4be4185852aab7135d59303",
                                    "patch": "@@ -127,6 +127,7 @@\n ((i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c')))\n (((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3))\n (*starred)\n+{\"id\": \"1\",\"type\": \"type\",\"started_at\": now(),\"ended_at\": now() + timedelta(days=10),\"priority\": 1,\"import_session_id\": 1,**kwargs}  # no trailing comma, this file is not 3.6+\n a = (1,)\n b = 1,\n c = 1\n@@ -331,6 +332,15 @@ async def f():\n ((i ** 2) for i, _ in ((1, \"a\"), (2, \"b\"), (3, \"c\")))\n (((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3))\n (*starred)\n+{\n+    \"id\": \"1\",\n+    \"type\": \"type\",\n+    \"started_at\": now(),\n+    \"ended_at\": now() + timedelta(days=10),\n+    \"priority\": 1,\n+    \"import_session_id\": 1,\n+    **kwargs\n+}  # no trailing comma, this file is not 3.6+\n a = (1,)\n b = 1,\n c = 1"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 95,
                "created_at": "2018-03-31T00:39:05Z",
                "closed_at": "2018-04-01T07:17:46Z",
                "title": "Error formatting 3 files in trio",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "2f260514f67f154d1c8f59bc9ea899406d8a2cfd",
                        "commit_date": "2018-04-01T07:08:09Z",
                        "parents": "4787294622f9053525fb2373db207dd4a35520dc",
                        "stat": {
                            "total": 1,
                            "additions": 38,
                            "deletions": 37,
                            "files": [
                                {
                                    "sha": "e5570252de1582a80fa39ec3e030ff5628dfa261",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=2f260514f67f154d1c8f59bc9ea899406d8a2cfd",
                                    "patch": "@@ -426,6 +426,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * fixed 18.3a4 regression: don't crash and burn on empty lines with\n   trailing whitespace (#80)\n \n+* fixed 18.3a4 regression: `# yapf: disable` usage as trailing comment\n+  would cause Black to not emit the rest of the file (#95)\n+\n * when CTRL+C is pressed while formatting many files, Black no longer\n   freaks out with a flurry of asyncio-related exceptions\n "
                                },
                                {
                                    "sha": "2c61880dda24a23dc897523f9bd37c9dd29d7ac8",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 1,
                                    "changes": 11,
                                    "blob_url": "https://github.com/psf/black/blob/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=2f260514f67f154d1c8f59bc9ea899406d8a2cfd",
                                    "patch": "@@ -1134,6 +1134,10 @@ def visit_unformatted(self, node: LN) -> Iterator[Line]:\n                 yield from self.line()\n                 yield from self.visit(node)\n \n+            if node.type == token.ENDMARKER:\n+                # somebody decided not to put a final `# fmt: on`\n+                yield from self.line()\n+\n     def __attrs_post_init__(self) -> None:\n         \"\"\"You are in a twisty little maze of passages.\"\"\"\n         v = self.visit_stmt\n@@ -1537,7 +1541,12 @@ def generate_comments(leaf: Leaf) -> Iterator[Leaf]:\n             raise FormatOn(consumed)\n \n         if comment in {\"# fmt: off\", \"# yapf: disable\"}:\n-            raise FormatOff(consumed)\n+            if comment_type == STANDALONE_COMMENT:\n+                raise FormatOff(consumed)\n+\n+            prev = preceding_leaf(leaf)\n+            if not prev or prev.type in WHITESPACE:  # standalone comment in disguise\n+                raise FormatOff(consumed)\n \n         nlines = 0\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "16c392595c0cf32a4b963d44cb574ce7ee54d38f",
                                    "filename": "tests/fmtonoff.py",
                                    "status": "modified",
                                    "additions": 24,
                                    "deletions": 0,
                                    "changes": 24,
                                    "blob_url": "https://github.com/psf/black/blob/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/tests%2Ffmtonoff.py",
                                    "raw_url": "https://github.com/psf/black/raw/2f260514f67f154d1c8f59bc9ea899406d8a2cfd/tests%2Ffmtonoff.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffmtonoff.py?ref=2f260514f67f154d1c8f59bc9ea899406d8a2cfd",
                                    "patch": "@@ -71,6 +71,18 @@ def long_lines():\n         $\n         \"\"\", re.MULTILINE | re.VERBOSE\n     )\n+def single_literal_yapf_disable():\n+    \"\"\"Black does not support this.\"\"\"\n+    BAZ = {\n+        (1, 2, 3, 4),\n+        (5, 6, 7, 8),\n+        (9, 10, 11, 12),\n+    }  # yapf: disable\n+# fmt: off\n+# No formatting to the end of the file\n+l=[1,2,3]\n+d={'a':1,\n+   'b':2}\n \n # output\n \n@@ -175,3 +187,15 @@ def long_lines():\n         \"\"\",\n         re.MULTILINE | re.VERBOSE,\n     )\n+\n+\n+def single_literal_yapf_disable():\n+    \"\"\"Black does not support this.\"\"\"\n+    BAZ = {(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 12)}  # yapf: disable\n+\n+\n+# fmt: off\n+# No formatting to the end of the file\n+l=[1,2,3]\n+d={'a':1,\n+   'b':2}"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 80,
                "created_at": "2018-03-26T20:33:33Z",
                "closed_at": "2018-03-27T01:41:54Z",
                "title": "Cannot format if at least one space on empty line in file",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "fc869039ebcc0c0ff922ea9b2713480c119e5341",
                        "commit_date": "2018-03-27T01:41:25Z",
                        "parents": "611737f9cc186d3e6463ef774fdbda4f77055d4c",
                        "stat": {
                            "total": 12,
                            "additions": 33,
                            "deletions": 21,
                            "files": [
                                {
                                    "sha": "1e28d5299aae4251a4048a0165f026db7e9cf1ca",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/psf/black/blob/fc869039ebcc0c0ff922ea9b2713480c119e5341/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/fc869039ebcc0c0ff922ea9b2713480c119e5341/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=fc869039ebcc0c0ff922ea9b2713480c119e5341",
                                    "patch": "@@ -333,6 +333,12 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ## Change Log\n \n+### 18.3a5 (unreleased)\n+\n+* fixed 18.3a4 regression: don't crash and burn on empty lines with\n+  trailing whitespace (#80)\n+\n+\n ### 18.3a4\n \n * `# fmt: off` and `# fmt: on` are implemented (#5)"
                                },
                                {
                                    "sha": "b6bbf4ec7dde2d912690be28a938bfb7f0742cc0",
                                    "filename": "blib2to3/pgen2/tokenize.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 12,
                                    "changes": 24,
                                    "blob_url": "https://github.com/psf/black/blob/fc869039ebcc0c0ff922ea9b2713480c119e5341/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "raw_url": "https://github.com/psf/black/raw/fc869039ebcc0c0ff922ea9b2713480c119e5341/blib2to3%2Fpgen2%2Ftokenize.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/blib2to3%2Fpgen2%2Ftokenize.py?ref=fc869039ebcc0c0ff922ea9b2713480c119e5341",
                                    "patch": "@@ -430,24 +430,24 @@ def generate_tokens(readline):\n                 yield stashed\n                 stashed = None\n \n+            if line[pos] in '\\r\\n':            # skip blank lines\n+                yield (NL, line[pos:], (lnum, pos), (lnum, len(line)), line)\n+                continue\n+\n             if column > indents[-1]:           # count indents\n                 indents.append(column)\n                 yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n \n-            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n-                if line[pos] == '#':\n-                    comment_token = line[pos:].rstrip('\\r\\n')\n-                    nl_pos = pos + len(comment_token)\n-                    yield (COMMENT, comment_token,\n-                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n-                    yield (NL, line[nl_pos:],\n-                           (lnum, nl_pos), (lnum, len(line)), line)\n-                else:\n-                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n-                           (lnum, pos), (lnum, len(line)), line)\n+            if line[pos] == '#':               # skip comments\n+                comment_token = line[pos:].rstrip('\\r\\n')\n+                nl_pos = pos + len(comment_token)\n+                yield (COMMENT, comment_token,\n+                        (lnum, pos), (lnum, pos + len(comment_token)), line)\n+                yield (NL, line[nl_pos:],\n+                        (lnum, nl_pos), (lnum, len(line)), line)\n                 continue\n \n-            while column < indents[-1]:         # count dedents\n+            while column < indents[-1]:        # count dedents\n                 if column not in indents:\n                     raise IndentationError(\n                         \"unindent does not match any outer indentation level\","
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "387e441b4d3dbd7a6813ce53e7668ca11e8c58d2",
                                    "filename": "tests/function.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/psf/black/blob/fc869039ebcc0c0ff922ea9b2713480c119e5341/tests%2Ffunction.py",
                                    "raw_url": "https://github.com/psf/black/raw/fc869039ebcc0c0ff922ea9b2713480c119e5341/tests%2Ffunction.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffunction.py?ref=fc869039ebcc0c0ff922ea9b2713480c119e5341",
                                    "patch": "@@ -34,6 +34,7 @@ def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h=\"\", i=r''\n def spaces_types(a: int = 1, b: tuple = (), c: list = [], d: dict = {}, e: bool = True, f: int = -1, g: int = 1 if False else 2, h: str = \"\", i: str = r''): ...\n def spaces2(result= _core.Value(None)):\n  ...\n+    # EMPTY LINE WITH WHITESPACE (this comment will be removed)\n def example(session):\n     result = session.query(models.Customer.id).filter(\n         models.Customer.account_id == account_id,"
                                },
                                {
                                    "sha": "759bda5af3dbea3c83036853b443cc4ada8de43d",
                                    "filename": "tests/test_black.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/fc869039ebcc0c0ff922ea9b2713480c119e5341/tests%2Ftest_black.py",
                                    "raw_url": "https://github.com/psf/black/raw/fc869039ebcc0c0ff922ea9b2713480c119e5341/tests%2Ftest_black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ftest_black.py?ref=fc869039ebcc0c0ff922ea9b2713480c119e5341",
                                    "patch": "@@ -17,6 +17,7 @@\n fs = partial(black.format_str, line_length=ll)\n THIS_FILE = Path(__file__)\n THIS_DIR = THIS_FILE.parent\n+EMPTY_LINE = '# EMPTY LINE WITH WHITESPACE' + ' (this comment will be removed)'\n \n \n def dump_to_stderr(*output: str) -> str:\n@@ -33,6 +34,7 @@ def read_data(name: str) -> Tuple[str, str]:\n         lines = test.readlines()\n     result = _input\n     for line in lines:\n+        line = line.replace(EMPTY_LINE, '')\n         if line.rstrip() == '# output':\n             result = _output\n             continue"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 74,
                "created_at": "2018-03-25T09:58:06Z",
                "closed_at": "2018-03-27T05:56:18Z",
                "title": "Empty lines within functions and class body definitions are not removed",
                "labels": "T: bug, F: empty lines",
                "commits": [
                    {
                        "hash": "e5f8251704c22b143b79474905c6c4b7e10ddb47",
                        "commit_date": "2018-03-27T05:55:56Z",
                        "parents": "1f445a01c8c073058ccd6ca6ceba8f527e6cbf15",
                        "stat": {
                            "total": 6,
                            "additions": 19,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "182e2649dcf24c3f3342098552e685132e574702",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 4,
                                    "changes": 11,
                                    "blob_url": "https://github.com/psf/black/blob/e5f8251704c22b143b79474905c6c4b7e10ddb47/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/e5f8251704c22b143b79474905c6c4b7e10ddb47/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=e5f8251704c22b143b79474905c6c4b7e10ddb47",
                                    "patch": "@@ -218,10 +218,10 @@ always emit an extra empty line after ``return``, ``raise``, ``break``,\n ``continue``, and ``yield``.  This is to make changes in control flow\n more prominent to readers of your code.\n \n-*Black* will allow single empty lines left by the original editors,\n-except when they're added within parenthesized expressions.  Since such\n-expressions are always reformatted to fit minimal space, this whitespace\n-is lost.\n+*Black* will allow single empty lines inside functions, and single and\n+double empty lines on module level left by the original editors, except\n+when they're within parenthesized expressions.  Since such expressions\n+are always reformatted to fit minimal space, this whitespace is lost.\n \n It will also insert proper spacing before and after function definitions.\n It's one line before and after inner functions and two lines before and\n@@ -338,6 +338,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * fixed 18.3a4 regression: don't crash and burn on empty lines with\n   trailing whitespace (#80)\n \n+* only allow up to two empty lines on module level and only single empty\n+  lines within functions (#74)\n+\n \n ### 18.3a4\n "
                                },
                                {
                                    "sha": "da7af03aebf3f5511a93f12c61283c6603541c03",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/e5f8251704c22b143b79474905c6c4b7e10ddb47/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/e5f8251704c22b143b79474905c6c4b7e10ddb47/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=e5f8251704c22b143b79474905c6c4b7e10ddb47",
                                    "patch": "@@ -754,13 +754,13 @@ def maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\n \n     def _maybe_empty_lines(self, current_line: Line) -> Tuple[int, int]:\n         max_allowed = 1\n-        if current_line.is_comment and current_line.depth == 0:\n+        if current_line.depth == 0:\n             max_allowed = 2\n         if current_line.leaves:\n             # Consume the first leaf's extra newlines.\n             first_leaf = current_line.leaves[0]\n             before = first_leaf.prefix.count('\\n')\n-            before = min(before, max(before, max_allowed))\n+            before = min(before, max_allowed)\n             first_leaf.prefix = ''\n         else:\n             before = 0"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "ec04337670dfd0b22211422d59419ac0e761fff4",
                                    "filename": "tests/empty_lines.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/e5f8251704c22b143b79474905c6c4b7e10ddb47/tests%2Fempty_lines.py",
                                    "raw_url": "https://github.com/psf/black/raw/e5f8251704c22b143b79474905c6c4b7e10ddb47/tests%2Fempty_lines.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fempty_lines.py?ref=e5f8251704c22b143b79474905c6c4b7e10ddb47",
                                    "patch": "@@ -12,15 +12,19 @@ def f():\n     if t == token.COMMENT:  # another trailing comment\n         return DOUBLESPACE\n \n+\n     assert p is not None, f\"INTERNAL ERROR: hand-made leaf without parent: {leaf!r}\"\n \n+\n     prev = leaf.prev_sibling\n     if not prev:\n         prevp = preceding_leaf(p)\n         if not prevp or prevp.type in OPENING_BRACKETS:\n \n+\n             return NO\n \n+\n         if prevp.type == token.EQUAL:\n             if prevp.parent and prevp.parent.type in {\n                 syms.typedargslist,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 60,
                "created_at": "2018-03-22T13:02:32Z",
                "closed_at": "2018-03-23T06:53:37Z",
                "title": "Space removed before () when used as default function argument",
                "labels": "T: bug",
                "commits": [
                    {
                        "hash": "cf6f577928f5a1f4e98b02b8e723311cae830305",
                        "commit_date": "2018-03-23T06:52:21Z",
                        "parents": "7bd6f3cb2ff11d385dbe433e2b03e9f7c94be33e",
                        "stat": {
                            "total": 10,
                            "additions": 41,
                            "deletions": 31,
                            "files": [
                                {
                                    "sha": "014780bd1af7fad5a0c437b11504ca6435780b81",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/cf6f577928f5a1f4e98b02b8e723311cae830305/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/cf6f577928f5a1f4e98b02b8e723311cae830305/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=cf6f577928f5a1f4e98b02b8e723311cae830305",
                                    "patch": "@@ -311,6 +311,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * automatic detection of deprecated Python 2 forms of print statements\n   and exec statements in the formatted file (#49)\n \n+* use proper spaces for complex expressions in default values of typed\n+  function arguments (#60)\n+\n * only return exit code 1 when --check is used (#50)\n \n * don't remove single trailing commas from square bracket indexing"
                                },
                                {
                                    "sha": "7ca6bc6eaa47e91b1f55cf54832c4a950671c1b4",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 10,
                                    "changes": 23,
                                    "blob_url": "https://github.com/psf/black/blob/cf6f577928f5a1f4e98b02b8e723311cae830305/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/cf6f577928f5a1f4e98b02b8e723311cae830305/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=cf6f577928f5a1f4e98b02b8e723311cae830305",
                                    "patch": "@@ -419,7 +419,7 @@ def any_open_brackets(self) -> bool:\n         \"\"\"Returns True if there is an yet unmatched open bracket on the line.\"\"\"\n         return bool(self.bracket_match)\n \n-    def max_priority(self, exclude: Iterable[LeafID] =()) -> int:\n+    def max_priority(self, exclude: Iterable[LeafID] = ()) -> int:\n         \"\"\"Returns the highest priority of a delimiter found on the line.\n \n         Values are consistent with what `is_delimiter()` returns.\n@@ -885,14 +885,17 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n             return SPACE if prevp.type == token.COMMA else NO\n \n         if prevp.type == token.EQUAL:\n-            if prevp.parent and prevp.parent.type in {\n-                syms.arglist,\n-                syms.argument,\n-                syms.parameters,\n-                syms.typedargslist,\n-                syms.varargslist,\n-            }:\n-                return NO\n+            if prevp.parent:\n+                if prevp.parent.type in {\n+                    syms.arglist, syms.argument, syms.parameters, syms.varargslist\n+                }:\n+                    return NO\n+\n+                elif prevp.parent.type == syms.typedargslist:\n+                    # A bit hacky: if the equal sign has whitespace, it means we\n+                    # previously found it's a typed argument.  So, we're using\n+                    # that, too.\n+                    return prevp.prefix\n \n         elif prevp.type == token.DOUBLESTAR:\n             if prevp.parent and prevp.parent.type in {\n@@ -938,7 +941,7 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n         if not prev or prev.type != token.COMMA:\n             return NO\n \n-    if p.type == syms.varargslist:\n+    elif p.type == syms.varargslist:\n         # lambdas\n         if t == token.RPAR:\n             return NO"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "08ab2b80a576f52a04fa6c9847bdfc90f6429e75",
                                    "filename": "tests/function.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 0,
                                    "changes": 15,
                                    "blob_url": "https://github.com/psf/black/blob/cf6f577928f5a1f4e98b02b8e723311cae830305/tests%2Ffunction.py",
                                    "raw_url": "https://github.com/psf/black/raw/cf6f577928f5a1f4e98b02b8e723311cae830305/tests%2Ffunction.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Ffunction.py?ref=cf6f577928f5a1f4e98b02b8e723311cae830305",
                                    "patch": "@@ -31,6 +31,7 @@ def function_signature_stress_test(number:int,no_annotation=None,text:str=\"defau\n def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h=\"\", i=r''):\n  offset = attr.ib(default=attr.Factory( lambda: _r.uniform(10000, 200000)))\n  assert task._cancel_stack[:len(old_stack)] == old_stack\n+def spaces_types(a: int = 1, b: tuple = (), c: list = [], d: dict = {}, e: bool = True, f: int = -1, g: int = 1 if False else 2, h: str = \"\", i: str = r''): ...\n def spaces2(result= _core.Value(None)):\n  ...\n def example(session):\n@@ -123,6 +124,20 @@ def spaces(a=1, b=(), c=[], d={}, e=True, f=-1, g=1 if False else 2, h=\"\", i=r''\n     assert task._cancel_stack[:len(old_stack)] == old_stack\n \n \n+def spaces_types(\n+    a: int = 1,\n+    b: tuple = (),\n+    c: list = [],\n+    d: dict = {},\n+    e: bool = True,\n+    f: int = -1,\n+    g: int = 1 if False else 2,\n+    h: str = \"\",\n+    i: str = r'',\n+):\n+    ...\n+\n+\n def spaces2(result=_core.Value(None)):\n     ...\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 59,
                "created_at": "2018-03-22T12:56:03Z",
                "closed_at": "2018-03-22T23:34:23Z",
                "title": "Internal error on a complex variable type annotation",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "a970a205bcea73672e85468836b477d3262ee75e",
                        "commit_date": "2018-03-22T23:33:50Z",
                        "parents": "c9f8983936a3e4bf02ae4f71e9ccd0006baa5169",
                        "stat": {
                            "total": 2,
                            "additions": 18,
                            "deletions": 16,
                            "files": [
                                {
                                    "sha": "877b632f3a54c3a5893290c1447c6221f73c1f68",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 1,
                                    "changes": 8,
                                    "blob_url": "https://github.com/psf/black/blob/a970a205bcea73672e85468836b477d3262ee75e/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/a970a205bcea73672e85468836b477d3262ee75e/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=a970a205bcea73672e85468836b477d3262ee75e",
                                    "patch": "@@ -510,10 +510,16 @@ def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:\n         ):\n             return False\n \n-        if closing.type == token.RSQB or closing.type == token.RBRACE:\n+        if closing.type == token.RBRACE:\n             self.leaves.pop()\n             return True\n \n+        if closing.type == token.RSQB:\n+            comma = self.leaves[-1]\n+            if comma.parent and comma.parent.type == syms.listmaker:\n+                self.leaves.pop()\n+                return True\n+\n         # For parens let's check if it's safe to remove the comma.  If the\n         # trailing one is the only one, we might mistakenly change a tuple\n         # into a different type by removing the comma."
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "79e7c7ecef70327bba591e6f3057baa6dcb980b4",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 1,
                                    "changes": 10,
                                    "blob_url": "https://github.com/psf/black/blob/a970a205bcea73672e85468836b477d3262ee75e/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/a970a205bcea73672e85468836b477d3262ee75e/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=a970a205bcea73672e85468836b477d3262ee75e",
                                    "patch": "@@ -53,6 +53,7 @@\n (1, 2, 3)\n []\n [1, 2, 3, 4, 5, 6, 7, 8, 9, (10 or A), (11 or B), (12 or C)]\n+[1, 2, 3,]\n {i for i in (1, 2, 3)}\n {(i ** 2) for i in (1, 2, 3)}\n {(i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c'))}\n@@ -84,7 +85,10 @@\n list[str]\n dict[str, int]\n tuple[str, ...]\n-tuple[str, int, float, dict[str, int]]\n+tuple[str, int, float, dict[str, int],]\n+very_long_variable_name_filters: t.List[\n+    t.Tuple[str, t.Union[str, t.List[t.Optional[str]]]],\n+]\n slice[0]\n slice[0:1]\n slice[0:1:2]\n@@ -207,6 +211,7 @@ async def f():\n (1, 2, 3)\n []\n [1, 2, 3, 4, 5, 6, 7, 8, 9, (10 or A), (11 or B), (12 or C)]\n+[1, 2, 3]\n {i for i in (1, 2, 3)}\n {(i ** 2) for i in (1, 2, 3)}\n {(i ** 2) for i, _ in ((1, 'a'), (2, 'b'), (3, 'c'))}\n@@ -248,6 +253,9 @@ async def f():\n dict[str, int]\n tuple[str, ...]\n tuple[str, int, float, dict[str, int]]\n+very_long_variable_name_filters: t.List[\n+    t.Tuple[str, t.Union[str, t.List[t.Optional[str]]]],\n+]\n slice[0]\n slice[0:1]\n slice[0:1:2]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 55,
                "created_at": "2018-03-21T22:22:22Z",
                "closed_at": "2018-03-22T01:36:45Z",
                "title": "Cannot parse dict comprehension with negative number",
                "labels": "T: bug, C: invalid code",
                "commits": [
                    {
                        "hash": "92b377556e24616d5980a9010cf558da7fa35d28",
                        "commit_date": "2018-03-22T01:34:16Z",
                        "parents": "c7c8c4f5018844d69edbc961c922b29f0d6ceeaf",
                        "stat": {
                            "total": 1,
                            "additions": 16,
                            "deletions": 15,
                            "files": [
                                {
                                    "sha": "aba30ab2fb0483483e75bb32f28d7bd6185ae2ca",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/psf/black/blob/92b377556e24616d5980a9010cf558da7fa35d28/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/92b377556e24616d5980a9010cf558da7fa35d28/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=92b377556e24616d5980a9010cf558da7fa35d28",
                                    "patch": "@@ -293,6 +293,11 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n \n ## Change Log\n \n+### 18.3a4 (unreleased)\n+\n+* don't omit whitespace if the previous factor leaf wasn't a math\n+  operator (#55)\n+\n ### 18.3a3\n \n * don't remove single empty lines outside of bracketed expressions"
                                },
                                {
                                    "sha": "0dd763073622d2ec1d0234b46cf5cd4e42bfe607",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 1,
                                    "changes": 7,
                                    "blob_url": "https://github.com/psf/black/blob/92b377556e24616d5980a9010cf558da7fa35d28/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/92b377556e24616d5980a9010cf558da7fa35d28/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=92b377556e24616d5980a9010cf558da7fa35d28",
                                    "patch": "@@ -340,6 +340,7 @@ def visit_default(self, node: LN) -> Iterator[T]:\n     token.AMPER,\n     token.PERCENT,\n     token.CIRCUMFLEX,\n+    token.TILDE,\n     token.LEFTSHIFT,\n     token.RIGHTSHIFT,\n     token.DOUBLESTAR,\n@@ -888,7 +889,11 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n             if prevp.parent and prevp.parent.type in {syms.subscript, syms.sliceop}:\n                 return NO\n \n-        elif prevp.parent and prevp.parent.type in {syms.factor, syms.star_expr}:\n+        elif (\n+            prevp.parent\n+            and prevp.parent.type in {syms.factor, syms.star_expr}\n+            and prevp.type in MATH_OPERATORS\n+        ):\n             return NO\n \n     elif prev.type in OPENING_BRACKETS:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c18942ec3522f058dfd7f173d669bfb07385e932",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/psf/black/blob/92b377556e24616d5980a9010cf558da7fa35d28/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/92b377556e24616d5980a9010cf558da7fa35d28/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=92b377556e24616d5980a9010cf558da7fa35d28",
                                    "patch": "@@ -63,6 +63,8 @@\n [((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3)]\n {i: 0 for i in (1, 2, 3)}\n {i: j for i, j in ((1, 'a'), (2, 'b'), (3, 'c'))}\n+{a: b * 2 for a, b in dictionary.items()}\n+{a: b * -2 for a, b in dictionary.items()}\n {k: v for k, v in this_is_a_very_long_variable_which_will_cause_a_trailing_comma_which_breaks_the_comprehension}\n Python3 > Python2 > COBOL\n Life is Life\n@@ -214,6 +216,8 @@ async def f():\n [((i ** 2) + j) for i in (1, 2, 3) for j in (1, 2, 3)]\n {i: 0 for i in (1, 2, 3)}\n {i: j for i, j in ((1, 'a'), (2, 'b'), (3, 'c'))}\n+{a: b * 2 for a, b in dictionary.items()}\n+{a: b * -2 for a, b in dictionary.items()}\n {\n     k: v\n     for k, v in this_is_a_very_long_variable_which_will_cause_a_trailing_comma_which_breaks_the_comprehension"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            },
            {
                "id": 46,
                "created_at": "2018-03-21T04:23:39Z",
                "closed_at": "2018-03-22T01:36:45Z",
                "title": "Extra space in kwarg unpacking",
                "labels": "T: bug, good first issue",
                "commits": [
                    {
                        "hash": "df7aacb43eb16f91adcb558905625d75ee804753",
                        "commit_date": "2018-03-22T01:35:25Z",
                        "parents": "92b377556e24616d5980a9010cf558da7fa35d28",
                        "stat": {
                            "total": 6,
                            "additions": 18,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "91c9384696c07b097dff54e55e1823376d32771e",
                                    "filename": "README.md",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/psf/black/blob/df7aacb43eb16f91adcb558905625d75ee804753/README.md",
                                    "raw_url": "https://github.com/psf/black/raw/df7aacb43eb16f91adcb558905625d75ee804753/README.md",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/README.md?ref=df7aacb43eb16f91adcb558905625d75ee804753",
                                    "patch": "@@ -298,6 +298,9 @@ More details can be found in [CONTRIBUTING](CONTRIBUTING.md).\n * don't omit whitespace if the previous factor leaf wasn't a math\n   operator (#55)\n \n+* omit extra space in kwarg unpacking if it's the first argument (#46)\n+\n+\n ### 18.3a3\n \n * don't remove single empty lines outside of bracketed expressions"
                                },
                                {
                                    "sha": "d3e0761e273a58fe4b7318827e12bbd853f53eea",
                                    "filename": "black.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 6,
                                    "changes": 13,
                                    "blob_url": "https://github.com/psf/black/blob/df7aacb43eb16f91adcb558905625d75ee804753/black.py",
                                    "raw_url": "https://github.com/psf/black/raw/df7aacb43eb16f91adcb558905625d75ee804753/black.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/black.py?ref=df7aacb43eb16f91adcb558905625d75ee804753",
                                    "patch": "@@ -867,21 +867,22 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901\n \n         if prevp.type == token.EQUAL:\n             if prevp.parent and prevp.parent.type in {\n-                syms.typedargslist,\n-                syms.varargslist,\n-                syms.parameters,\n                 syms.arglist,\n                 syms.argument,\n+                syms.parameters,\n+                syms.typedargslist,\n+                syms.varargslist,\n             }:\n                 return NO\n \n         elif prevp.type == token.DOUBLESTAR:\n             if prevp.parent and prevp.parent.type in {\n-                syms.typedargslist,\n-                syms.varargslist,\n-                syms.parameters,\n                 syms.arglist,\n+                syms.argument,\n                 syms.dictsetmaker,\n+                syms.parameters,\n+                syms.typedargslist,\n+                syms.varargslist,\n             }:\n                 return NO\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "91e5465440799856479837e4f73e56ec2fe7e83e",
                                    "filename": "tests/expression.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/psf/black/blob/df7aacb43eb16f91adcb558905625d75ee804753/tests%2Fexpression.py",
                                    "raw_url": "https://github.com/psf/black/raw/df7aacb43eb16f91adcb558905625d75ee804753/tests%2Fexpression.py",
                                    "contents_url": "https://api.github.com/repos/psf/black/contents/tests%2Fexpression.py?ref=df7aacb43eb16f91adcb558905625d75ee804753",
                                    "patch": "@@ -75,6 +75,7 @@\n call(arg, another, kwarg='hey', **kwargs)\n call(this_is_a_very_long_variable_which_will_force_a_delimiter_split, arg, another, kwarg='hey', **kwargs)  # note: no trailing comma pre-3.6\n call(*gidgets[:2])\n+call(**self.screen_kwargs)\n lukasz.langa.pl\n call.me(maybe)\n 1 .real\n@@ -237,6 +238,7 @@ async def f():\n     **kwargs\n )  # note: no trailing comma pre-3.6\n call(*gidgets[:2])\n+call(**self.screen_kwargs)\n lukasz.langa.pl\n call.me(maybe)\n 1 .real"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv install\npipenv install click==8.0.2\npipenv install pytest==6.2.5",
                "testSteps": "pipenv run python setup.py test",
                "testStepsFull": "pipenv run python setup.py test"
            }
        ],
        "installSteps": "pipenv --python 3.7\npipenv install setuptools==68.0.0\npipenv run pip install -r test_requirements.txt\npipenv install click==8.0.2\npipenv install pytest==6.2.5\npipenv run pip install -e .[d]\n"
    },
    {
        "_id": "6489714ed97326f51535995b",
        "username": "cookiecutter",
        "repository": "cookiecutter",
        "issues": [
            {
                "id": 1513,
                "created_at": "2021-04-10T21:02:09Z",
                "closed_at": "2021-04-29T21:40:23Z",
                "title": "Error reading yaml file using `poyo` - `ValueError: Parent of ChildMixin instance needs to be a Container.`",
                "labels": "bug, duplicate",
                "commits": [
                    {
                        "hash": "cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb",
                        "commit_date": "2021-04-22T22:01:32Z",
                        "parents": "2fd137f2c2d508cd0d194301101da7e85df6994e",
                        "stat": {
                            "total": 1,
                            "additions": 16,
                            "deletions": 15,
                            "files": [
                                {
                                    "sha": "d93501f36862cecb57d20cb104f1022078d84d10",
                                    "filename": "cookiecutter/config.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/cookiecutter%2Fconfig.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/cookiecutter%2Fconfig.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/cookiecutter%2Fconfig.py?ref=cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb",
                                    "patch": "@@ -45,7 +45,7 @@ def merge_configs(default, overwrite):\n         # Make sure to preserve existing items in\n         # nested dicts, for example `abbreviations`\n         if isinstance(v, dict):\n-            new_config[k] = merge_configs(default[k], v)\n+            new_config[k] = merge_configs(default.get(k, {}), v)\n         else:\n             new_config[k] = v\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c46ac4a60d7e8b8c9a81601c24d6dac697ef2842",
                                    "filename": "tests/test-config/valid-config.yaml",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest-config%2Fvalid-config.yaml",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest-config%2Fvalid-config.yaml",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Ftest-config%2Fvalid-config.yaml?ref=cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb",
                                    "patch": "@@ -2,6 +2,12 @@ default_context:\n     full_name: \"Firstname Lastname\"\n     email: \"firstname.lastname@gmail.com\"\n     github_username: \"example\"\n+    project:\n+        description: \"description\"\n+        tags:\n+            - \"first\"\n+            - \"second\"\n+            - \"third\"\n cookiecutters_dir: \"/home/example/some-path-to-templates\"\n replay_dir: \"/home/example/some-path-to-replay-files\"\n abbreviations:"
                                },
                                {
                                    "sha": "a0e81f26619c88184c6987c611f28c9a1b77f8cd",
                                    "filename": "tests/test_get_config.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest_get_config.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest_get_config.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Ftest_get_config.py?ref=cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb",
                                    "patch": "@@ -59,6 +59,10 @@ def test_get_config():\n             'full_name': 'Firstname Lastname',\n             'email': 'firstname.lastname@gmail.com',\n             'github_username': 'example',\n+            'project': {\n+                'description': 'description',\n+                'tags': ['first', 'second', 'third',],\n+            },\n         },\n         'abbreviations': {\n             'gh': 'https://github.com/{0}.git',"
                                },
                                {
                                    "sha": "560c1d873c50a924be5dbaac798e67a94beaf40e",
                                    "filename": "tests/test_get_user_config.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest_get_user_config.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb/tests%2Ftest_get_user_config.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Ftest_get_user_config.py?ref=cc92e3cc00a3d2acd1ef6d38cf5731478dade3cb",
                                    "patch": "@@ -46,6 +46,10 @@ def custom_config():\n             'full_name': 'Firstname Lastname',\n             'email': 'firstname.lastname@gmail.com',\n             'github_username': 'example',\n+            'project': {\n+                'description': 'description',\n+                'tags': ['first', 'second', 'third',],\n+            },\n         },\n         'cookiecutters_dir': '/home/example/some-path-to-templates',\n         'replay_dir': '/home/example/some-path-to-replay-files',"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_get_config.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 18,
                "created_at": "2013-08-16T10:36:09Z",
                "closed_at": "2013-08-19T19:07:25Z",
                "title": "cookiecutter throws unicode exceptions on image files",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "0e38a7068375e065f00a6a6ce29459e91261f11b",
                        "commit_date": "2013-08-18T13:16:22Z",
                        "parents": "312bcc9f78f1ffe0c0fa2c483d1dea5a101c5871",
                        "stat": {
                            "total": 12,
                            "additions": 57,
                            "deletions": 45,
                            "files": [
                                {
                                    "sha": "a20d6bd722a36ccd337e467bb54e02306be0352a",
                                    "filename": "cookiecutter/generate.py",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 10,
                                    "changes": 37,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/cookiecutter%2Fgenerate.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/cookiecutter%2Fgenerate.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/cookiecutter%2Fgenerate.py?ref=0e38a7068375e065f00a6a6ce29459e91261f11b",
                                    "patch": "@@ -10,10 +10,12 @@\n \n import logging\n import os\n+import shutil\n import sys\n \n from jinja2 import FileSystemLoader, Template\n from jinja2.environment import Environment\n+from binaryornot.check import is_binary\n \n from .exceptions import NonTemplatedInputDirException\n from .utils import make_sure_path_exists, unicode_open\n@@ -26,6 +28,7 @@\n     import json\n     from collections import OrderedDict\n \n+\n def generate_context(config_file='cookiecutter.json'):\n     \"\"\"\n     Generates the context for a Cookiecutter project template.\n@@ -55,7 +58,7 @@ def generate_files(template_dir, context=None):\n     :param input_dir: Project template input directory.\n     :paramtype input_dir: directory\n     \"\"\"\n-    \n+\n     logging.debug('Generating project from {0}...'.format(template_dir))\n \n     context = context or {}\n@@ -68,6 +71,7 @@ def generate_files(template_dir, context=None):\n     if output_dir == template_dir:\n         raise NonTemplatedInputDirException\n \n+    logging.debug(\"output_dir is {0}\".format(output_dir))\n     make_sure_path_exists(output_dir)\n \n     for root, dirs, files in os.walk(template_dir):\n@@ -82,18 +86,31 @@ def generate_files(template_dir, context=None):\n             make_sure_path_exists(rendered_dirname)\n \n         for f in files:\n-            # Render the file\n+            logging.debug(\"f is {0}\".format(f))\n             infile = os.path.join(root, f)\n-            tmpl = env.get_template(infile)\n-            rendered_file = tmpl.render(**context)\n+            logging.debug(\"infile is {0}\".format(infile))\n \n             # Write it to the corresponding place in output_dir\n             outfile = infile.replace(template_dir, output_dir, 1)\n+            logging.debug(\"outfile is {0}\".format(outfile))\n+\n+            # Just copy over binary files. Don't render.\n+            logging.debug(\"Check {0} to see if it's a binary\".format(infile))\n+            if is_binary(infile):\n+                logging.debug(\"Copying binary {0} to {1} without rendering\"\n+                              .format(infile, outfile))\n+                shutil.copyfile(infile, outfile)\n+\n+            else:\n+\n+                # Render the file\n+                tmpl = env.get_template(infile)\n+                rendered_file = tmpl.render(**context)\n \n-            # Render the output filename before writing\n-            name_tmpl = Template(outfile)\n-            rendered_name = name_tmpl.render(**context)\n-            logging.debug(\"Writing {0}\".format(rendered_name))\n+                # Render the output filename before writing\n+                name_tmpl = Template(outfile)\n+                rendered_name = name_tmpl.render(**context)\n+                logging.debug(\"Writing {0}\".format(rendered_name))\n \n-            with unicode_open(rendered_name, 'w') as fh:\n-                fh.write(rendered_file)\n+                with unicode_open(rendered_name, 'w') as fh:\n+                    fh.write(rendered_file)"
                                },
                                {
                                    "sha": "2367ba0e3172162570ef00c5c74f0f0dac1a17a0",
                                    "filename": "setup.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/setup.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/setup.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/setup.py?ref=0e38a7068375e065f00a6a6ce29459e91261f11b",
                                    "patch": "@@ -17,7 +17,7 @@\n readme = open('README.rst').read()\n history = open('HISTORY.rst').read().replace('.. :changelog:', '')\n \n-requirements = ['jinja2>=2.4']\n+requirements = ['binaryornot>=0.1.1', 'jinja2>=2.4']\n \n if sys.version_info[:2] < (2, 7):\n     requirements.append('argparse')"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e04d51eb4d78d2b7b78cfb858dbe44922a3f1ab9",
                                    "filename": "tests/input{{binary_test}}/logo.png",
                                    "status": "added",
                                    "additions": 0,
                                    "deletions": 0,
                                    "changes": 0,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Flogo.png",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Flogo.png",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Finput%7B%7Bbinary_test%7D%7D%2Flogo.png?ref=0e38a7068375e065f00a6a6ce29459e91261f11b"
                                },
                                {
                                    "sha": "43b29487c991f3adcda38e75d06cf6b1b4a1fa6b",
                                    "filename": "tests/input{{binary_test}}/readme.txt",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Freadme.txt",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Freadme.txt",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Finput%7B%7Bbinary_test%7D%7D%2Freadme.txt?ref=0e38a7068375e065f00a6a6ce29459e91261f11b",
                                    "patch": "@@ -0,0 +1 @@\n+I eat {{ binary_test }}\n\\ No newline at end of file"
                                },
                                {
                                    "sha": "b058f1cd1b998892d9d6e69c2a3bfcd4b04b42ca",
                                    "filename": "tests/input{{binary_test}}/some_font.otf",
                                    "status": "added",
                                    "additions": 0,
                                    "deletions": 0,
                                    "changes": 0,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Fsome_font.otf",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Finput%7B%7Bbinary_test%7D%7D%2Fsome_font.otf",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Finput%7B%7Bbinary_test%7D%7D%2Fsome_font.otf?ref=0e38a7068375e065f00a6a6ce29459e91261f11b"
                                },
                                {
                                    "sha": "2960f4b52753d26d9e45218aebe3341cdf284ea9",
                                    "filename": "tests/test_generate.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 1,
                                    "changes": 17,
                                    "blob_url": "https://github.com/cookiecutter/cookiecutter/blob/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Ftest_generate.py",
                                    "raw_url": "https://github.com/cookiecutter/cookiecutter/raw/0e38a7068375e065f00a6a6ce29459e91261f11b/tests%2Ftest_generate.py",
                                    "contents_url": "https://api.github.com/repos/cookiecutter/cookiecutter/contents/tests%2Ftest_generate.py?ref=0e38a7068375e065f00a6a6ce29459e91261f11b",
                                    "patch": "@@ -40,12 +40,25 @@ def test_generate_files(self):\n         simple_text = open('tests/inputpizza/simple.txt', 'rt').read()\n         self.assertEqual(simple_text, 'I eat pizza')\n \n+    def test_generate_files_binaries(self):\n+        generate.generate_files(\n+            context={'binary_test': 'binary_files'},\n+            template_dir='tests/input{{binary_test}}'\n+        )\n+        self.assertTrue(os.path.isfile('tests/inputbinary_files/logo.png'))\n+        self.assertTrue(os.path.isfile('tests/inputbinary_files/readme.txt'))\n+        self.assertTrue(\n+            os.path.isfile('tests/inputbinary_files/some_font.otf')\n+        )\n+\n     def test_generate_context(self):\n         context = generate.generate_context(config_file='tests/json/test.json')\n         self.assertEqual(context, {\"test\": {\"1\": 2}})\n \n     def test_output_folder(self):\n-        context = generate.generate_context(config_file='tests/json2/stuff.json')\n+        context = generate.generate_context(\n+            config_file='tests/json2/stuff.json'\n+        )\n         logging.debug('Context is {0}'.format(context))\n         generate.generate_files(\n             context=context,\n@@ -70,6 +83,8 @@ def tearDown(self):\n             shutil.rmtree('tests/inputpizza')\n         if os.path.exists('tests/inputgreen'):\n             shutil.rmtree('tests/inputgreen')\n+        if os.path.exists('tests/inputbinary_files'):\n+            shutil.rmtree('tests/inputbinary_files')\n \n if __name__ == '__main__':\n     unittest.main()"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\npipenv install pytest\npipenv install pytest-mock\npipenv install pytest-cov\npipenv install freezegun\npipenv run python setup.py install",
                "testSteps": "pipenv run pytest -- tests/test_generate.py",
                "testStepsFull": "pipenv run pytest"
            }
        ],
        "installSteps": "pipenv --python 3.8\npipenv install -r test_requirements.txt\npipenv run python setup.py install"
    },
    {
        "_id": "64898133d97326f515359960",
        "username": "pandas-dev",
        "repository": "pandas",
        "issues": [
            {
                "id": 16466,
                "created_at": "2017-05-23T22:51:31Z",
                "closed_at": "2017-06-01T10:57:28Z",
                "title": "BUG: fixed wrong order of ordered labels in pd.cut()",
                "labels": "Bug, Blocker, Categorical",
                "commits": [
                    {
                        "hash": "d419be4333dcb8cf643bdd04c7c4e990feae49f9",
                        "commit_date": "2017-06-01T10:56:27Z",
                        "parents": "e3ee1869ce955df5d3daa59d59e08749471e5be5",
                        "stat": {
                            "total": 4,
                            "additions": 14,
                            "deletions": 10,
                            "files": [
                                {
                                    "sha": "379249b6e55d67bbdb6e46391db0baf0c6dd8684",
                                    "filename": "doc/source/whatsnew/v0.20.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 3,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d419be4333dcb8cf643bdd04c7c4e990feae49f9/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d419be4333dcb8cf643bdd04c7c4e990feae49f9/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt?ref=d419be4333dcb8cf643bdd04c7c4e990feae49f9",
                                    "patch": "@@ -46,11 +46,9 @@ Bug Fixes\n - Passing an invalid engine to :func:`read_csv` now raises an informative\n   ``ValueError`` rather than ``UnboundLocalError``. (:issue:`16511`)\n - Bug in :func:`unique` on an array of tuples (:issue:`16519`)\n-\n-\n+- Bug in :func:`cut`` when ``labels`` are set, resulting in incorrect label ordering (:issue:`16459`)\n - Fixed a compatibility issue with IPython 6.0's tab completion showing deprecation warnings on Categoricals (:issue:`16409`)\n \n-\n Conversion\n ^^^^^^^^^^\n "
                                },
                                {
                                    "sha": "d8398023a5083332e846a20d5fe2bd34582926cd",
                                    "filename": "pandas/core/reshape/tile.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d419be4333dcb8cf643bdd04c7c4e990feae49f9/pandas%2Fcore%2Freshape%2Ftile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d419be4333dcb8cf643bdd04c7c4e990feae49f9/pandas%2Fcore%2Freshape%2Ftile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Freshape%2Ftile.py?ref=d419be4333dcb8cf643bdd04c7c4e990feae49f9",
                                    "patch": "@@ -254,7 +254,7 @@ def _bins_to_cuts(x, bins, right=True, labels=None,\n                 raise ValueError('Bin labels must be one fewer than '\n                                  'the number of bin edges')\n         if not is_categorical_dtype(labels):\n-            labels = Categorical(labels, ordered=True)\n+            labels = Categorical(labels, categories=labels, ordered=True)\n \n         np.putmask(ids, na_mask, 0)\n         result = algos.take_nd(labels, ids - 1)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "542af321632cf2d317c67e5da0c602bdb5b98916",
                                    "filename": "pandas/tests/reshape/test_tile.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d419be4333dcb8cf643bdd04c7c4e990feae49f9/pandas%2Ftests%2Freshape%2Ftest_tile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d419be4333dcb8cf643bdd04c7c4e990feae49f9/pandas%2Ftests%2Freshape%2Ftest_tile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Freshape%2Ftest_tile.py?ref=d419be4333dcb8cf643bdd04c7c4e990feae49f9",
                                    "patch": "@@ -211,6 +211,7 @@ def test_cut_pass_labels(self):\n \n         result = cut(arr, bins, labels=labels)\n         exp = Categorical(['Medium'] + 4 * ['Small'] + ['Medium', 'Large'],\n+                          categories=labels,\n                           ordered=True)\n         tm.assert_categorical_equal(result, exp)\n \n@@ -219,6 +220,13 @@ def test_cut_pass_labels(self):\n         exp = Categorical.from_codes([1] + 4 * [0] + [1, 2], labels)\n         tm.assert_categorical_equal(result, exp)\n \n+\t# issue 16459\n+        labels = ['Good', 'Medium', 'Bad']\n+        result = cut(arr, 3, labels=labels)\n+        exp = cut(arr, 3, labels=Categorical(labels, categories=labels,\n+                                             ordered=True))\n+        tm.assert_categorical_equal(result, exp)\n+\n     def test_qcut_include_lowest(self):\n         values = np.arange(10)\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/reshape/test_tile.py"
            },
            {
                "id": 16362,
                "created_at": "2017-05-15T18:28:09Z",
                "closed_at": "2017-05-24T23:21:18Z",
                "title": "BUG: Transpose construction/block error with certain dtypes",
                "labels": "Bug, Regression",
                "commits": [
                    {
                        "hash": "05d0667169e4b770cfad94f4a19c1d6ae9a98536",
                        "commit_date": "2017-05-24T23:20:51Z",
                        "parents": "85080aaf332711dbaebf4b4b266df053ccc6b52c",
                        "stat": {
                            "total": 2,
                            "additions": 16,
                            "deletions": 14,
                            "files": [
                                {
                                    "sha": "b8810f5ee314c23012fc781860448da1e57f48a4",
                                    "filename": "doc/source/whatsnew/v0.20.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/05d0667169e4b770cfad94f4a19c1d6ae9a98536/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/05d0667169e4b770cfad94f4a19c1d6ae9a98536/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.2.txt?ref=05d0667169e4b770cfad94f4a19c1d6ae9a98536",
                                    "patch": "@@ -86,6 +86,7 @@ Reshaping\n - Bug in ``DataFrame.stack`` with unsorted levels in MultiIndex columns (:issue:`16323`)\n - Bug in ``pd.wide_to_long()`` where no error was raised when ``i`` was not a unique identifier (:issue:`16382`)\n - Bug in ``Series.isin(..)`` with a list of tuples (:issue:`16394`)\n+- Bug in construction of a ``DataFrame`` with mixed dtypes including an all-NaT column. (:issue:`16395`)\n \n \n Numeric"
                                },
                                {
                                    "sha": "fd61813a57c9818f2f81ddf1ca2952ac0e47922a",
                                    "filename": "pandas/core/dtypes/cast.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/05d0667169e4b770cfad94f4a19c1d6ae9a98536/pandas%2Fcore%2Fdtypes%2Fcast.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/05d0667169e4b770cfad94f4a19c1d6ae9a98536/pandas%2Fcore%2Fdtypes%2Fcast.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fdtypes%2Fcast.py?ref=05d0667169e4b770cfad94f4a19c1d6ae9a98536",
                                    "patch": "@@ -837,7 +837,7 @@ def try_timedelta(v):\n         try:\n             return to_timedelta(v)._values.reshape(shape)\n         except:\n-            return v\n+            return v.reshape(shape)\n \n     inferred_type = lib.infer_datetimelike_array(_ensure_object(v))\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "767e99d98cf29f87af019ff7116016329b5b3aa0",
                                    "filename": "pandas/tests/dtypes/test_cast.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 1,
                                    "changes": 13,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/05d0667169e4b770cfad94f4a19c1d6ae9a98536/pandas%2Ftests%2Fdtypes%2Ftest_cast.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/05d0667169e4b770cfad94f4a19c1d6ae9a98536/pandas%2Ftests%2Fdtypes%2Ftest_cast.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fdtypes%2Ftest_cast.py?ref=05d0667169e4b770cfad94f4a19c1d6ae9a98536",
                                    "patch": "@@ -9,7 +9,7 @@\n from datetime import datetime, timedelta, date\n import numpy as np\n \n-from pandas import Timedelta, Timestamp, DatetimeIndex\n+from pandas import Timedelta, Timestamp, DatetimeIndex, DataFrame, NaT\n \n from pandas.core.dtypes.cast import (\n     maybe_downcast_to_dtype,\n@@ -213,6 +213,17 @@ def test_maybe_convert_scalar(self):\n         result = maybe_convert_scalar(Timedelta('1 day 1 min'))\n         assert result == Timedelta('1 day 1 min').value\n \n+    def test_maybe_infer_to_datetimelike(self):\n+        # GH16362\n+        # pandas=0.20.1 raises IndexError: tuple index out of range\n+        result = DataFrame(np.array([[NaT, 'a', 'b', 0],\n+                                     [NaT, 'b', 'c', 1]]))\n+        assert result.size == 8\n+        # this construction was fine\n+        result = DataFrame(np.array([[NaT, 'a', 0],\n+                                     [NaT, 'b', 1]]))\n+        assert result.size == 6\n+\n \n class TestConvert(object):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/dtypes/test_cast.py"
            },
            {
                "id": 16114,
                "created_at": "2017-04-24T18:26:30Z",
                "closed_at": "2017-04-25T02:11:01Z",
                "title": "Bug: dtype lost in `Series.to_frame` for empty series with integer name in 0.20.0rc1",
                "labels": "Bug, Dtype Conversions, Regression",
                "commits": [
                    {
                        "hash": "4ca4fca839ced3483f37b3ae2434033e2721e1d5",
                        "commit_date": "2017-04-25T02:10:39Z",
                        "parents": "008e9ec4d2df01dc7325d626a196c0128f6976be",
                        "stat": {
                            "total": 1,
                            "additions": 11,
                            "deletions": 10,
                            "files": [
                                {
                                    "sha": "983a6ef3e045adb835ff4912f18265d19016c316",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/4ca4fca839ced3483f37b3ae2434033e2721e1d5/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/4ca4fca839ced3483f37b3ae2434033e2721e1d5/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=4ca4fca839ced3483f37b3ae2434033e2721e1d5",
                                    "patch": "@@ -296,7 +296,7 @@ def __init__(self, data=None, index=None, columns=None, dtype=None,\n                 if columns is None:\n                     columns = data_columns\n                 mgr = self._init_dict(data, index, columns, dtype=dtype)\n-            elif getattr(data, 'name', None):\n+            elif getattr(data, 'name', None) is not None:\n                 mgr = self._init_dict({data.name: data}, index, columns,\n                                       dtype=dtype)\n             else:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f6cdb37a2477a93ae3ee1049cca56a694a76abd5",
                                    "filename": "pandas/tests/frame/test_constructors.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/4ca4fca839ced3483f37b3ae2434033e2721e1d5/pandas%2Ftests%2Fframe%2Ftest_constructors.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/4ca4fca839ced3483f37b3ae2434033e2721e1d5/pandas%2Ftests%2Fframe%2Ftest_constructors.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_constructors.py?ref=4ca4fca839ced3483f37b3ae2434033e2721e1d5",
                                    "patch": "@@ -1888,6 +1888,15 @@ def test_from_records_len0_with_columns(self):\n         self.assertEqual(len(result), 0)\n         self.assertEqual(result.index.name, 'foo')\n \n+    def test_to_frame_with_falsey_names(self):\n+        # GH 16114\n+        result = Series(name=0).to_frame().dtypes\n+        expected = Series({0: np.float64})\n+        tm.assert_series_equal(result, expected)\n+\n+        result = DataFrame(Series(name=0)).dtypes\n+        tm.assert_series_equal(result, expected)\n+\n \n class TestDataFrameConstructorWithDatetimeTZ(tm.TestCase, TestData):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_constructors.py"
            },
            {
                "id": 15955,
                "created_at": "2017-04-09T14:20:46Z",
                "closed_at": "2017-04-10T12:10:24Z",
                "title": "BUG: Fix MultiIndex names handling in pd.concat",
                "labels": "Bug, MultiIndex",
                "commits": [
                    {
                        "hash": "9cb2c2db0dd763bb9e6586d3103a564875ed25d5",
                        "commit_date": "2017-04-10T12:08:23Z",
                        "parents": "b4701a6dcb432ba6c5c5b757f4956ae59d282781",
                        "stat": {
                            "total": 1,
                            "additions": 27,
                            "deletions": 26,
                            "files": [
                                {
                                    "sha": "e8170b4bf2113ff43a9896237fb0a4b9f3e528b3",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=9cb2c2db0dd763bb9e6586d3103a564875ed25d5",
                                    "patch": "@@ -1241,6 +1241,7 @@ Indexing\n - Bug in creating a ``MultiIndex`` with tuples and not passing a list of names; this will now raise ``ValueError`` (:issue:`15110`)\n - Bug in the HTML display with with a ``MultiIndex`` and truncation (:issue:`14882`)\n - Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)\n+- Bug in ``pd.concat()`` where the names of ``MultiIndex`` of resulting ``DataFrame`` are not handled correctly when ``None`` is presented in the names of ``MultiIndex`` of input ``DataFrame`` (:issue:`15787`)\n \n I/O\n ^^^"
                                },
                                {
                                    "sha": "a3cb54ca97071e45f92587ba4cc479435ef117f6",
                                    "filename": "pandas/indexes/api.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/pandas%2Findexes%2Fapi.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/pandas%2Findexes%2Fapi.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fapi.py?ref=9cb2c2db0dd763bb9e6586d3103a564875ed25d5",
                                    "patch": "@@ -107,7 +107,7 @@ def _get_consensus_names(indexes):\n     # find the non-none names, need to tupleify to make\n     # the set hashable, then reverse on return\n     consensus_names = set([tuple(i.names) for i in indexes\n-                           if all(n is not None for n in i.names)])\n+                           if any(n is not None for n in i.names)])\n     if len(consensus_names) == 1:\n         return list(list(consensus_names)[0])\n     return [None] * indexes[0].nlevels"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c61f2a3dc80661260e226246a7d09798b8ec4627",
                                    "filename": "pandas/tests/tools/test_concat.py",
                                    "status": "modified",
                                    "additions": 24,
                                    "deletions": 0,
                                    "changes": 24,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/pandas%2Ftests%2Ftools%2Ftest_concat.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9cb2c2db0dd763bb9e6586d3103a564875ed25d5/pandas%2Ftests%2Ftools%2Ftest_concat.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftools%2Ftest_concat.py?ref=9cb2c2db0dd763bb9e6586d3103a564875ed25d5",
                                    "patch": "@@ -1048,6 +1048,30 @@ def test_concat_multiindex_with_tz(self):\n         result = concat([df, df])\n         tm.assert_frame_equal(result, expected)\n \n+    def test_concat_multiindex_with_none_in_index_names(self):\n+        # GH 15787\n+        index = pd.MultiIndex.from_product([[1], range(5)],\n+                                           names=['level1', None])\n+        df = pd.DataFrame({'col': range(5)}, index=index, dtype=np.int32)\n+\n+        result = concat([df, df], keys=[1, 2], names=['level2'])\n+        index = pd.MultiIndex.from_product([[1, 2], [1], range(5)],\n+                                           names=['level2', 'level1', None])\n+        expected = pd.DataFrame({'col': list(range(5)) * 2},\n+                                index=index, dtype=np.int32)\n+        assert_frame_equal(result, expected)\n+\n+        result = concat([df, df[:2]], keys=[1, 2], names=['level2'])\n+        level2 = [1] * 5 + [2] * 2\n+        level1 = [1] * 7\n+        no_name = list(range(5)) + list(range(2))\n+        tuples = list(zip(level2, level1, no_name))\n+        index = pd.MultiIndex.from_tuples(tuples,\n+                                          names=['level2', 'level1', None])\n+        expected = pd.DataFrame({'col': no_name}, index=index,\n+                                dtype=np.int32)\n+        assert_frame_equal(result, expected)\n+\n     def test_concat_keys_and_levels(self):\n         df = DataFrame(np.random.randn(1, 3))\n         df2 = DataFrame(np.random.randn(1, 4))"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/tools/test_concat.py"
            },
            {
                "id": 15845,
                "created_at": "2017-03-30T06:57:03Z",
                "closed_at": "2017-04-19T22:29:29Z",
                "title": "na_position doesn't work for sort_index() with MultiIndex",
                "labels": "Bug, MultiIndex",
                "commits": [
                    {
                        "hash": "dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                        "commit_date": "2017-04-19T22:28:56Z",
                        "parents": "f114af045f68ed960cba02d234a959301ad97a79",
                        "stat": {
                            "total": 2,
                            "additions": 81,
                            "deletions": 79,
                            "files": [
                                {
                                    "sha": "f64f592e109a1d37d2f01bf4d8b52cad5c2b6341",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/dd5cef560b2fc30aaad04e74134b3f52b64425ce/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/dd5cef560b2fc30aaad04e74134b3f52b64425ce/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                                    "patch": "@@ -1564,6 +1564,7 @@ Indexing\n - Bug in the HTML display with with a ``MultiIndex`` and truncation (:issue:`14882`)\n - Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)\n - Bug in ``pd.concat()`` where the names of ``MultiIndex`` of resulting ``DataFrame`` are not handled correctly when ``None`` is presented in the names of ``MultiIndex`` of input ``DataFrame`` (:issue:`15787`)\n+- Bug in ``DataFrame.sort_index()`` and ``Series.sort_index()`` where ``na_position`` doesn't work with a ``MultiIndex`` (:issue:`14784`)\n \n I/O\n ^^^"
                                },
                                {
                                    "sha": "7fbfa7962c2c6167906da4865b439be6fdce3625",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                                    "patch": "@@ -3352,7 +3352,8 @@ def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\n             # make sure that the axis is lexsorted to start\n             # if not we need to reconstruct to get the correct indexer\n             labels = labels._sort_levels_monotonic()\n-            indexer = lexsort_indexer(labels.labels, orders=ascending,\n+            indexer = lexsort_indexer(labels._get_labels_for_sorting(),\n+                                      orders=ascending,\n                                       na_position=na_position)\n         else:\n             from pandas.core.sorting import nargsort"
                                },
                                {
                                    "sha": "92baf9d289cd25b73af7595c7bb31f93685deb91",
                                    "filename": "pandas/core/indexes/multi.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 0,
                                    "changes": 16,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Findexes%2Fmulti.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Findexes%2Fmulti.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Findexes%2Fmulti.py?ref=dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                                    "patch": "@@ -1635,6 +1635,22 @@ def reorder_levels(self, order):\n     def __getslice__(self, i, j):\n         return self.__getitem__(slice(i, j))\n \n+    def _get_labels_for_sorting(self):\n+        \"\"\"\n+        we categorizing our labels by using the\n+        available catgories (all, not just observed)\n+        excluding any missing ones (-1); this is in preparation\n+        for sorting, where we need to disambiguate that -1 is not\n+        a valid valid\n+        \"\"\"\n+        from pandas.core.categorical import Categorical\n+\n+        return [Categorical.from_codes(label,\n+                                       np.arange(np.array(label).max() + 1,\n+                                                 dtype=label.dtype),\n+                                       ordered=True)\n+                for label in self.labels]\n+\n     def sortlevel(self, level=0, ascending=True, sort_remaining=True):\n         \"\"\"\n         Sort MultiIndex at the requested level. The result will respect the"
                                },
                                {
                                    "sha": "e0364ad629c5dfd9ad4e061f332891b7ef95094f",
                                    "filename": "pandas/core/series.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Fseries.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Fcore%2Fseries.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fseries.py?ref=dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                                    "patch": "@@ -1753,7 +1753,9 @@ def sort_index(self, axis=0, level=None, ascending=True, inplace=False,\n         elif isinstance(index, MultiIndex):\n             from pandas.core.sorting import lexsort_indexer\n             labels = index._sort_levels_monotonic()\n-            indexer = lexsort_indexer(labels.labels, orders=ascending)\n+            indexer = lexsort_indexer(labels._get_labels_for_sorting(),\n+                                      orders=ascending,\n+                                      na_position=na_position)\n         else:\n             from pandas.core.sorting import nargsort\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "99c468b2561679c2e83bc771fe4c1702924361af",
                                    "filename": "pandas/tests/test_multilevel.py",
                                    "status": "modified",
                                    "additions": 57,
                                    "deletions": 0,
                                    "changes": 57,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Ftests%2Ftest_multilevel.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/dd5cef560b2fc30aaad04e74134b3f52b64425ce/pandas%2Ftests%2Ftest_multilevel.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_multilevel.py?ref=dd5cef560b2fc30aaad04e74134b3f52b64425ce",
                                    "patch": "@@ -2634,3 +2634,60 @@ def test_sort_non_lexsorted(self):\n \n         with pytest.raises(UnsortedIndexError):\n             result.loc[pd.IndexSlice['B':'C', 'a':'c'], :]\n+\n+    def test_sort_index_nan(self):\n+        # GH 14784\n+        # incorrect sorting w.r.t. nans\n+        tuples = [[12, 13], [np.nan, np.nan], [np.nan, 3], [1, 2]]\n+        mi = MultiIndex.from_tuples(tuples)\n+\n+        df = DataFrame(np.arange(16).reshape(4, 4),\n+                       index=mi, columns=list('ABCD'))\n+        s = Series(np.arange(4), index=mi)\n+\n+        df2 = DataFrame({\n+            'date': pd.to_datetime([\n+                '20121002', '20121007', '20130130', '20130202', '20130305',\n+                '20121002', '20121207', '20130130', '20130202', '20130305',\n+                '20130202', '20130305'\n+            ]),\n+            'user_id': [1, 1, 1, 1, 1, 3, 3, 3, 5, 5, 5, 5],\n+            'whole_cost': [1790, np.nan, 280, 259, np.nan, 623, 90, 312,\n+                           np.nan, 301, 359, 801],\n+            'cost': [12, 15, 10, 24, 39, 1, 0, np.nan, 45, 34, 1, 12]\n+        }).set_index(['date', 'user_id'])\n+\n+        # sorting frame, default nan position is last\n+        result = df.sort_index()\n+        expected = df.iloc[[3, 0, 2, 1], :]\n+        tm.assert_frame_equal(result, expected)\n+\n+        # sorting frame, nan position last\n+        result = df.sort_index(na_position='last')\n+        expected = df.iloc[[3, 0, 2, 1], :]\n+        tm.assert_frame_equal(result, expected)\n+\n+        # sorting frame, nan position first\n+        result = df.sort_index(na_position='first')\n+        expected = df.iloc[[1, 2, 3, 0], :]\n+        tm.assert_frame_equal(result, expected)\n+\n+        # sorting frame with removed rows\n+        result = df2.dropna().sort_index()\n+        expected = df2.sort_index().dropna()\n+        tm.assert_frame_equal(result, expected)\n+\n+        # sorting series, default nan position is last\n+        result = s.sort_index()\n+        expected = s.iloc[[3, 0, 2, 1]]\n+        tm.assert_series_equal(result, expected)\n+\n+        # sorting series, nan position last\n+        result = s.sort_index(na_position='last')\n+        expected = s.iloc[[3, 0, 2, 1]]\n+        tm.assert_series_equal(result, expected)\n+\n+        # sorting series, nan position first\n+        result = s.sort_index(na_position='first')\n+        expected = s.iloc[[1, 2, 3, 0]]\n+        tm.assert_series_equal(result, expected)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_multilevel.py"
            },
            {
                "id": 15822,
                "created_at": "2017-03-28T07:02:08Z",
                "closed_at": "2017-03-28T16:49:16Z",
                "title": "get localized Timestamp using \".at\" cannot get the time zone info",
                "labels": "Bug, Indexing, Regression, Timezones",
                "commits": [
                    {
                        "hash": "a9406057b5f48d579d9a9136a183a594c4b1f758",
                        "commit_date": "2017-03-28T16:48:41Z",
                        "parents": "1dab800b412be3613e8f666eb1be88458b631312",
                        "stat": {
                            "total": 1,
                            "additions": 27,
                            "deletions": 26,
                            "files": [
                                {
                                    "sha": "51c3d5578ae6c8706a49d88fc15649e5456289b1",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/a9406057b5f48d579d9a9136a183a594c4b1f758/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/a9406057b5f48d579d9a9136a183a594c4b1f758/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=a9406057b5f48d579d9a9136a183a594c4b1f758",
                                    "patch": "@@ -881,6 +881,7 @@ Bug Fixes\n \n - Compat for 32-bit platforms for ``.qcut/cut``; bins will now be ``int64`` dtype (:issue:`14866`)\n \n+- Bug in ``.at`` when selecting from a tz-aware column (:issue:`15822`)\n - Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)\n - Bug in ``.replace()`` may result in incorrect dtypes. (:issue:`12747`, :issue:`15765`)\n "
                                },
                                {
                                    "sha": "90baa1aff4857c3acdffdd1212dd7e0b47ed09a9",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 1,
                                    "changes": 11,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/a9406057b5f48d579d9a9136a183a594c4b1f758/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/a9406057b5f48d579d9a9136a183a594c4b1f758/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=a9406057b5f48d579d9a9136a183a594c4b1f758",
                                    "patch": "@@ -1918,7 +1918,16 @@ def get_value(self, index, col, takeable=False):\n \n         series = self._get_item_cache(col)\n         engine = self.index._engine\n-        return engine.get_value(series.get_values(), index)\n+\n+        try:\n+            return engine.get_value(series._values, index)\n+        except TypeError:\n+\n+            # we cannot handle direct indexing\n+            # use positional\n+            col = self.columns.get_loc(col)\n+            index = self.index.get_loc(index)\n+            return self.get_value(index, col, takeable=True)\n \n     def set_value(self, index, col, value, takeable=False):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "0eeaec3e00fa62cb0e6722dc9cf9cfb5e7807c4d",
                                    "filename": "pandas/tests/indexing/test_scalar.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 0,
                                    "changes": 15,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/a9406057b5f48d579d9a9136a183a594c4b1f758/pandas%2Ftests%2Findexing%2Ftest_scalar.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/a9406057b5f48d579d9a9136a183a594c4b1f758/pandas%2Ftests%2Findexing%2Ftest_scalar.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexing%2Ftest_scalar.py?ref=a9406057b5f48d579d9a9136a183a594c4b1f758",
                                    "patch": "@@ -154,3 +154,18 @@ def test_at_to_fail(self):\n         # Check that we get the correct value in the KeyError\n         self.assertRaisesRegexp(KeyError, r\"\\['y'\\] not in index\",\n                                 lambda: df[['x', 'y', 'z']])\n+\n+    def test_at_with_tz(self):\n+        # gh-15822\n+        df = DataFrame({'name': ['John', 'Anderson'],\n+                        'date': [Timestamp(2017, 3, 13, 13, 32, 56),\n+                                 Timestamp(2017, 2, 16, 12, 10, 3)]})\n+        df['date'] = df['date'].dt.tz_localize('Asia/Shanghai')\n+\n+        expected = Timestamp('2017-03-13 13:32:56+0800', tz='Asia/Shanghai')\n+\n+        result = df.loc[0, 'date']\n+        assert result == expected\n+\n+        result = df.at[0, 'date']\n+        assert result == expected"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexing/test_scalar.py"
            },
            {
                "id": 15676,
                "created_at": "2017-03-13T18:27:44Z",
                "closed_at": "2017-03-14T14:06:40Z",
                "title": "BUG: merge_asof only merges by first column of 'by=' list",
                "labels": "Bug, Reshaping",
                "commits": [
                    {
                        "hash": "2621b31c7dbd68126867266d2b2e32d3e5e222d5",
                        "commit_date": "2017-03-14T14:05:41Z",
                        "parents": "c7c74ad7b2fc33f68e59a7a4f677ce48c2829b18",
                        "stat": {
                            "total": 7,
                            "additions": 61,
                            "deletions": 54,
                            "files": [
                                {
                                    "sha": "3548cbf6eb4a749ff19866a61126836b8ae13b4d",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2621b31c7dbd68126867266d2b2e32d3e5e222d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2621b31c7dbd68126867266d2b2e32d3e5e222d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=2621b31c7dbd68126867266d2b2e32d3e5e222d5",
                                    "patch": "@@ -884,6 +884,7 @@ Bug Fixes\n - Bug in the HTML display with with a ``MultiIndex`` and truncation (:issue:`14882`)\n \n \n+- Bug in ``pd.merge_asof()`` where ``left_index`` or ``right_index`` caused a failure when multiple ``by`` was specified (:issue:`15676`)\n - Bug in ``pd.merge_asof()`` where ``left_index``/``right_index`` together caused a failure when ``tolerance`` was specified (:issue:`15135`)\n - Bug in ``DataFrame.pivot_table()`` where ``dropna=True`` would not drop all-NaN columns when the columns was a ``category`` dtype (:issue:`15193`)\n "
                                },
                                {
                                    "sha": "261884bba54bdf09a3d1dde1be637955aaded7a6",
                                    "filename": "pandas/tools/merge.py",
                                    "status": "modified",
                                    "additions": 18,
                                    "deletions": 7,
                                    "changes": 25,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2621b31c7dbd68126867266d2b2e32d3e5e222d5/pandas%2Ftools%2Fmerge.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2621b31c7dbd68126867266d2b2e32d3e5e222d5/pandas%2Ftools%2Fmerge.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Fmerge.py?ref=2621b31c7dbd68126867266d2b2e32d3e5e222d5",
                                    "patch": "@@ -1165,14 +1165,17 @@ def _validate_specification(self):\n         if self.left_by is not None and self.right_by is None:\n             raise MergeError('missing right_by')\n \n-        # add by to our key-list so we can have it in the\n+        # add 'by' to our key-list so we can have it in the\n         # output as a key\n         if self.left_by is not None:\n             if not is_list_like(self.left_by):\n                 self.left_by = [self.left_by]\n             if not is_list_like(self.right_by):\n                 self.right_by = [self.right_by]\n \n+            if len(self.left_by) != len(self.right_by):\n+                raise MergeError('left_by and right_by must be same length')\n+\n             self.left_on = self.left_by + list(self.left_on)\n             self.right_on = self.right_by + list(self.right_on)\n \n@@ -1264,13 +1267,21 @@ def flip(xs):\n \n         # a \"by\" parameter requires special handling\n         if self.left_by is not None:\n-            if len(self.left_join_keys) > 2:\n-                # get tuple representation of values if more than one\n-                left_by_values = flip(self.left_join_keys[0:-1])\n-                right_by_values = flip(self.right_join_keys[0:-1])\n+            # remove 'on' parameter from values if one existed\n+            if self.left_index and self.right_index:\n+                left_by_values = self.left_join_keys\n+                right_by_values = self.right_join_keys\n+            else:\n+                left_by_values = self.left_join_keys[0:-1]\n+                right_by_values = self.right_join_keys[0:-1]\n+\n+            # get tuple representation of values if more than one\n+            if len(left_by_values) == 1:\n+                left_by_values = left_by_values[0]\n+                right_by_values = right_by_values[0]\n             else:\n-                left_by_values = self.left_join_keys[0]\n-                right_by_values = self.right_join_keys[0]\n+                left_by_values = flip(left_by_values)\n+                right_by_values = flip(right_by_values)\n \n             # upcast 'by' parameter because HashTable is limited\n             by_type = _get_cython_type_upcast(left_by_values.dtype)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c9460cc74c94aaa7e764fcac4c79279a19a80b51",
                                    "filename": "pandas/tests/tools/test_merge_asof.py",
                                    "status": "modified",
                                    "additions": 35,
                                    "deletions": 0,
                                    "changes": 35,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2621b31c7dbd68126867266d2b2e32d3e5e222d5/pandas%2Ftests%2Ftools%2Ftest_merge_asof.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2621b31c7dbd68126867266d2b2e32d3e5e222d5/pandas%2Ftests%2Ftools%2Ftest_merge_asof.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftools%2Ftest_merge_asof.py?ref=2621b31c7dbd68126867266d2b2e32d3e5e222d5",
                                    "patch": "@@ -368,6 +368,41 @@ def test_multiby_heterogeneous_types(self):\n                                by=['ticker', 'exch'])\n         assert_frame_equal(result, expected)\n \n+    def test_multiby_indexed(self):\n+        # GH15676\n+        left = pd.DataFrame([\n+            [pd.to_datetime('20160602'), 1, 'a'],\n+            [pd.to_datetime('20160602'), 2, 'a'],\n+            [pd.to_datetime('20160603'), 1, 'b'],\n+            [pd.to_datetime('20160603'), 2, 'b']],\n+            columns=['time', 'k1', 'k2']).set_index('time')\n+\n+        right = pd.DataFrame([\n+            [pd.to_datetime('20160502'), 1, 'a', 1.0],\n+            [pd.to_datetime('20160502'), 2, 'a', 2.0],\n+            [pd.to_datetime('20160503'), 1, 'b', 3.0],\n+            [pd.to_datetime('20160503'), 2, 'b', 4.0]],\n+            columns=['time', 'k1', 'k2', 'value']).set_index('time')\n+\n+        expected = pd.DataFrame([\n+            [pd.to_datetime('20160602'), 1, 'a', 1.0],\n+            [pd.to_datetime('20160602'), 2, 'a', 2.0],\n+            [pd.to_datetime('20160603'), 1, 'b', 3.0],\n+            [pd.to_datetime('20160603'), 2, 'b', 4.0]],\n+            columns=['time', 'k1', 'k2', 'value']).set_index('time')\n+\n+        result = pd.merge_asof(left,\n+                               right,\n+                               left_index=True,\n+                               right_index=True,\n+                               by=['k1', 'k2'])\n+\n+        assert_frame_equal(expected, result)\n+\n+        with self.assertRaises(MergeError):\n+            pd.merge_asof(left, right, left_index=True, right_index=True,\n+                          left_by=['k1', 'k2'], right_by=['k1'])\n+\n     def test_basic2(self):\n \n         expected = self.read_data('asof2.csv')"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/tools/test_merge_asof.py"
            },
            {
                "id": 15604,
                "created_at": "2017-03-07T18:32:43Z",
                "closed_at": "2017-03-08T13:37:19Z",
                "title": "pandas.Series.sort_values(ascending=[False]) behaves as ascending=True",
                "labels": "Bug, Reshaping, Error Reporting",
                "commits": [
                    {
                        "hash": "c9d4e0b01f8b47e7c04fb132081dce607c05757c",
                        "commit_date": "2017-03-08T13:24:45Z",
                        "parents": "8daf677b04b6797e7db894b85da7f6e5a4d356c5",
                        "stat": {
                            "total": 1,
                            "additions": 31,
                            "deletions": 30,
                            "files": [
                                {
                                    "sha": "a7169640759e30b9b185b0440677cb13ddbdc1f7",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c9d4e0b01f8b47e7c04fb132081dce607c05757c/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c9d4e0b01f8b47e7c04fb132081dce607c05757c/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=c9d4e0b01f8b47e7c04fb132081dce607c05757c",
                                    "patch": "@@ -228,7 +228,7 @@ Other enhancements\n - ``pd.TimedeltaIndex`` now has a custom datetick formatter specifically designed for nanosecond level precision (:issue:`8711`)\n - ``pd.types.concat.union_categoricals`` gained the ``ignore_ordered`` argument to allow ignoring the ordered attribute of unioned categoricals (:issue:`13410`). See the :ref:`categorical union docs <categorical.union>` for more information.\n - ``pandas.io.json.json_normalize()`` with an empty ``list`` will return an empty ``DataFrame`` (:issue:`15534`)\n-\n+- ``Series.sort_values`` accepts a one element list of bool for consistency with the behavior of ``DataFrame.sort_values`` (:issue:`15604`)\n .. _ISO 8601 duration: https://en.wikipedia.org/wiki/ISO_8601#Durations\n \n "
                                },
                                {
                                    "sha": "f23e90effdabf9990a3ac6e593dd830700471c37",
                                    "filename": "pandas/core/series.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c9d4e0b01f8b47e7c04fb132081dce607c05757c/pandas%2Fcore%2Fseries.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c9d4e0b01f8b47e7c04fb132081dce607c05757c/pandas%2Fcore%2Fseries.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fseries.py?ref=c9d4e0b01f8b47e7c04fb132081dce607c05757c",
                                    "patch": "@@ -14,6 +14,7 @@\n import numpy.ma as ma\n \n from pandas.types.common import (_coerce_to_dtype, is_categorical_dtype,\n+                                 is_bool,\n                                  is_integer, is_integer_dtype,\n                                  is_float_dtype,\n                                  is_extension_type, is_datetimetz,\n@@ -1719,6 +1720,15 @@ def _try_kind_sort(arr):\n \n         argsorted = _try_kind_sort(arr[good])\n \n+        if is_list_like(ascending):\n+            if len(ascending) != 1:\n+                raise ValueError('Length of ascending (%d) must be 1 '\n+                                 'for Series' % (len(ascending)))\n+            ascending = ascending[0]\n+\n+        if not is_bool(ascending):\n+            raise ValueError('ascending must be boolean')\n+\n         if not ascending:\n             argsorted = argsorted[::-1]\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "590a530a847bd55e36b39fb5741134ed9e0a2cb9",
                                    "filename": "pandas/tests/series/test_sorting.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 0,
                                    "changes": 19,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c9d4e0b01f8b47e7c04fb132081dce607c05757c/pandas%2Ftests%2Fseries%2Ftest_sorting.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c9d4e0b01f8b47e7c04fb132081dce607c05757c/pandas%2Ftests%2Fseries%2Ftest_sorting.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fseries%2Ftest_sorting.py?ref=c9d4e0b01f8b47e7c04fb132081dce607c05757c",
                                    "patch": "@@ -64,6 +64,25 @@ def test_sort_values(self):\n         ordered = ts.sort_values(ascending=False, na_position='first')\n         assert_almost_equal(expected, ordered.valid().values)\n \n+        # ascending=[False] should behave the same as ascending=False\n+        ordered = ts.sort_values(ascending=[False])\n+        expected = ts.sort_values(ascending=False)\n+        assert_series_equal(expected, ordered)\n+        ordered = ts.sort_values(ascending=[False], na_position='first')\n+        expected = ts.sort_values(ascending=False, na_position='first')\n+        assert_series_equal(expected, ordered)\n+\n+        self.assertRaises(ValueError,\n+                          lambda: ts.sort_values(ascending=None))\n+        self.assertRaises(ValueError,\n+                          lambda: ts.sort_values(ascending=[]))\n+        self.assertRaises(ValueError,\n+                          lambda: ts.sort_values(ascending=[1, 2, 3]))\n+        self.assertRaises(ValueError,\n+                          lambda: ts.sort_values(ascending=[False, False]))\n+        self.assertRaises(ValueError,\n+                          lambda: ts.sort_values(ascending='foobar'))\n+\n         # inplace=True\n         ts = self.ts.copy()\n         ts.sort_values(ascending=False, inplace=True)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/series/test_sorting.py"
            },
            {
                "id": 15488,
                "created_at": "2017-02-23T18:36:33Z",
                "closed_at": "2017-03-07T13:28:16Z",
                "title": "Repr-ing SparseDataFrame after setting a value",
                "labels": "Bug, Indexing, Sparse",
                "commits": [
                    {
                        "hash": "38a34be9108fc76b68e57860506f428d8d67e002",
                        "commit_date": "2017-03-07T13:27:43Z",
                        "parents": "fdee92214dedf87f351f1ae0613d9f25061359b0",
                        "stat": {
                            "total": 4,
                            "additions": 17,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "e459c854dfab91d1506a73a34a7aa83311214321",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/38a34be9108fc76b68e57860506f428d8d67e002/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/38a34be9108fc76b68e57860506f428d8d67e002/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=38a34be9108fc76b68e57860506f428d8d67e002",
                                    "patch": "@@ -698,7 +698,7 @@ Bug Fixes\n \n - Bug in ``to_sql`` when writing a DataFrame with numeric index names (:issue:`15404`).\n - Bug in ``Series.iloc`` where a ``Categorical`` object for list-like indexes input was returned, where a ``Series`` was expected. (:issue:`14580`)\n-\n+- Bug in repr-formatting a ``SparseDataFrame`` after a value was set on (a copy of) one of its series (:issue:`15488`)\n \n \n - Bug in  groupby operations with timedelta64 when passing ``numeric_only=False`` (:issue:`5724`)"
                                },
                                {
                                    "sha": "622c4cd3bbcc7054295822191d83e3098ca13d06",
                                    "filename": "pandas/formats/format.py",
                                    "status": "modified",
                                    "additions": 0,
                                    "deletions": 3,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/38a34be9108fc76b68e57860506f428d8d67e002/pandas%2Fformats%2Fformat.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/38a34be9108fc76b68e57860506f428d8d67e002/pandas%2Fformats%2Fformat.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fformats%2Fformat.py?ref=38a34be9108fc76b68e57860506f428d8d67e002",
                                    "patch": "@@ -716,9 +716,6 @@ def to_html(self, classes=None, notebook=False, border=None):\n     def _get_formatted_column_labels(self, frame):\n         from pandas.core.index import _sparsify\n \n-        def is_numeric_dtype(dtype):\n-            return issubclass(dtype.type, np.number)\n-\n         columns = frame.columns\n \n         if isinstance(columns, MultiIndex):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "ba870a2c338016bae50df5d362397b84585390f6",
                                    "filename": "pandas/tests/sparse/test_format.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 0,
                                    "changes": 12,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/38a34be9108fc76b68e57860506f428d8d67e002/pandas%2Ftests%2Fsparse%2Ftest_format.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/38a34be9108fc76b68e57860506f428d8d67e002/pandas%2Ftests%2Fsparse%2Ftest_format.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fsparse%2Ftest_format.py?ref=38a34be9108fc76b68e57860506f428d8d67e002",
                                    "patch": "@@ -116,3 +116,15 @@ def test_sparse_frame(self):\n \n         with option_context(\"display.max_rows\", 3):\n             self.assertEqual(repr(sparse), repr(df))\n+\n+    def test_sparse_repr_after_set(self):\n+        # GH 15488\n+        sdf = pd.SparseDataFrame([[np.nan, 1], [2, np.nan]])\n+        res = sdf.copy()\n+\n+        # Ignore the warning\n+        with pd.option_context('mode.chained_assignment', None):\n+            sdf[0][1] = 2  # This line triggers the bug\n+\n+        repr(sdf)\n+        tm.assert_sp_frame_equal(sdf, res)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/sparse/test_format.py"
            },
            {
                "id": 15463,
                "created_at": "2017-02-20T23:27:52Z",
                "closed_at": "2017-02-23T13:24:28Z",
                "title": "Series.rolling(3).quantile(10) segmentation fault",
                "labels": "Bug, Reshaping, Numeric Operations",
                "commits": [
                    {
                        "hash": "b94186d4c58ee055656a84f55618be537db0095a",
                        "commit_date": "2017-02-23T13:23:15Z",
                        "parents": "03eca9dad6c911d7df12377839e8eb3bb6028d98",
                        "stat": {
                            "total": 4,
                            "additions": 23,
                            "deletions": 19,
                            "files": [
                                {
                                    "sha": "fa24c973a754916bea56b6e1f45a28abb90e4fc7",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b94186d4c58ee055656a84f55618be537db0095a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b94186d4c58ee055656a84f55618be537db0095a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=b94186d4c58ee055656a84f55618be537db0095a",
                                    "patch": "@@ -539,7 +539,7 @@ Bug Fixes\n - Bug in using ``__deepcopy__`` on empty NDFrame objects (:issue:`15370`)\n - Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`, :issue:`15424`)\n - Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a numpy array (:issue:`15434`)\n-\n+- Bug in ``Rolling.quantile`` function that caused a segmentation fault when called with a quantile value outside of the range [0, 1] (:issue:`15463`)\n \n \n - Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)"
                                },
                                {
                                    "sha": "005d42c9f68be420a854eb54ab97d4825a28d1a4",
                                    "filename": "pandas/window.pyx",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 2,
                                    "changes": 7,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b94186d4c58ee055656a84f55618be537db0095a/pandas%2Fwindow.pyx",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b94186d4c58ee055656a84f55618be537db0095a/pandas%2Fwindow.pyx",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fwindow.pyx?ref=b94186d4c58ee055656a84f55618be537db0095a",
                                    "patch": "@@ -134,8 +134,8 @@ cdef class WindowIndexer:\n         bint is_variable\n \n     def get_data(self):\n-        return (self.start, self.end, <int64_t>self.N, \n-                <int64_t>self.win, <int64_t>self.minp, \n+        return (self.start, self.end, <int64_t>self.N,\n+                <int64_t>self.win, <int64_t>self.minp,\n                 self.is_variable)\n \n \n@@ -1285,6 +1285,9 @@ def roll_quantile(ndarray[float64_t, cast=True] input, int64_t win,\n         ndarray[int64_t] start, end\n         ndarray[double_t] output\n \n+    if quantile < 0.0 or quantile > 1.0:\n+        raise ValueError(\"quantile value {0} not in [0, 1]\".format(quantile))\n+\n     # we use the Fixed/Variable Indexer here as the\n     # actual skiplist ops outweigh any window computation costs\n     start, end, N, win, minp, is_variable = get_window_indexer("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "3f2973a9834caece3b7b2b0e2f3a70c432681725",
                                    "filename": "pandas/tests/test_window.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 1,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b94186d4c58ee055656a84f55618be537db0095a/pandas%2Ftests%2Ftest_window.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b94186d4c58ee055656a84f55618be537db0095a/pandas%2Ftests%2Ftest_window.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_window.py?ref=b94186d4c58ee055656a84f55618be537db0095a",
                                    "patch": "@@ -1063,7 +1063,7 @@ def test_rolling_max(self):\n                               window=3, min_periods=5)\n \n     def test_rolling_quantile(self):\n-        qs = [.1, .5, .9]\n+        qs = [0.0, .1, .5, .9, 1.0]\n \n         def scoreatpercentile(a, per):\n             values = np.sort(a, axis=0)\n@@ -1084,6 +1084,18 @@ def alt(x):\n \n             self._check_moment_func(f, alt, name='quantile', quantile=q)\n \n+    def test_rolling_quantile_param(self):\n+        ser = Series([0.0, .1, .5, .9, 1.0])\n+\n+        with self.assertRaises(ValueError):\n+            ser.rolling(3).quantile(-0.1)\n+\n+        with self.assertRaises(ValueError):\n+            ser.rolling(3).quantile(10.0)\n+\n+        with self.assertRaises(TypeError):\n+            ser.rolling(3).quantile('foo')\n+\n     def test_rolling_apply(self):\n         # suppress warnings about empty slices, as we are deliberately testing\n         # with a 0-length Series"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_window.py"
            },
            {
                "id": 15447,
                "created_at": "2017-02-17T21:55:02Z",
                "closed_at": "2017-03-07T21:17:20Z",
                "title": "Wrong result of pandas.sparse.series.SparseSeries.loc with indexer of length 1",
                "labels": "Bug, Indexing, Sparse, MultiIndex",
                "commits": [
                    {
                        "hash": "c52ff68a536fafc0204c5afea57abb943a6c37ce",
                        "commit_date": "2017-03-07T21:14:59Z",
                        "parents": "a347ecb574f4e53f43400ad50b507c481ce12edb",
                        "stat": {
                            "total": 20,
                            "additions": 81,
                            "deletions": 61,
                            "files": [
                                {
                                    "sha": "ece9ff4a1adffadca9cf94b18349f7735a7255d4",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c52ff68a536fafc0204c5afea57abb943a6c37ce/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c52ff68a536fafc0204c5afea57abb943a6c37ce/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=c52ff68a536fafc0204c5afea57abb943a6c37ce",
                                    "patch": "@@ -672,6 +672,10 @@ Bug Fixes\n - Bug in ``Rolling.quantile`` function that caused a segmentation fault when called with a quantile value outside of the range [0, 1] (:issue:`15463`)\n \n \n+- Bug in ``SparseSeries.reindex`` on single level with list of length 1 (:issue:`15447`)\n+\n+\n+\n - Bug in the display of ``.info()`` where a qualifier (+) would always be displayed with a ``MultiIndex`` that contains only non-strings (:issue:`15245`)\n \n - Bug in ``.asfreq()``, where frequency was not set for empty ``Series` (:issue:`14320`)"
                                },
                                {
                                    "sha": "c3dd089e8409af5e12cf8e32bb647f201739d500",
                                    "filename": "pandas/sparse/series.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 19,
                                    "changes": 24,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c52ff68a536fafc0204c5afea57abb943a6c37ce/pandas%2Fsparse%2Fseries.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c52ff68a536fafc0204c5afea57abb943a6c37ce/pandas%2Fsparse%2Fseries.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsparse%2Fseries.py?ref=c52ff68a536fafc0204c5afea57abb943a6c37ce",
                                    "patch": "@@ -32,7 +32,7 @@\n                                         _coo_to_sparse_series)\n \n \n-_shared_doc_kwargs = dict(klass='SparseSeries',\n+_shared_doc_kwargs = dict(axes='index', klass='SparseSeries',\n                           axes_single_arg=\"{0, 'index'}\")\n \n # -----------------------------------------------------------------------------\n@@ -570,27 +570,13 @@ def copy(self, deep=True):\n         return self._constructor(new_data, sparse_index=self.sp_index,\n                                  fill_value=self.fill_value).__finalize__(self)\n \n+    @Appender(generic._shared_docs['reindex'] % _shared_doc_kwargs)\n     def reindex(self, index=None, method=None, copy=True, limit=None,\n                 **kwargs):\n-        \"\"\"\n-        Conform SparseSeries to new Index\n-\n-        See Series.reindex docstring for general behavior\n \n-        Returns\n-        -------\n-        reindexed : SparseSeries\n-        \"\"\"\n-        new_index = _ensure_index(index)\n-\n-        if self.index.equals(new_index):\n-            if copy:\n-                return self.copy()\n-            else:\n-                return self\n-        return self._constructor(self._data.reindex(new_index, method=method,\n-                                                    limit=limit, copy=copy),\n-                                 index=new_index).__finalize__(self)\n+        return super(SparseSeries, self).reindex(index=index, method=method,\n+                                                 copy=copy, limit=limit,\n+                                                 **kwargs)\n \n     def sparse_reindex(self, new_index):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "1a0782c0a3db9a9151beec0b1d44d79d4687173b",
                                    "filename": "pandas/tests/sparse/test_indexing.py",
                                    "status": "modified",
                                    "additions": 52,
                                    "deletions": 1,
                                    "changes": 53,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c52ff68a536fafc0204c5afea57abb943a6c37ce/pandas%2Ftests%2Fsparse%2Ftest_indexing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c52ff68a536fafc0204c5afea57abb943a6c37ce/pandas%2Ftests%2Fsparse%2Ftest_indexing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fsparse%2Ftest_indexing.py?ref=c52ff68a536fafc0204c5afea57abb943a6c37ce",
                                    "patch": "@@ -366,7 +366,7 @@ def test_reindex(self):\n         exp = orig.reindex(['A', 'E', 'C', 'D']).to_sparse()\n         tm.assert_sp_series_equal(res, exp)\n \n-    def test_reindex_fill_value(self):\n+    def test_fill_value_reindex(self):\n         orig = pd.Series([1, np.nan, 0, 3, 0], index=list('ABCDE'))\n         sparse = orig.to_sparse(fill_value=0)\n \n@@ -397,6 +397,23 @@ def test_reindex_fill_value(self):\n         exp = orig.reindex(['A', 'E', 'C', 'D']).to_sparse(fill_value=0)\n         tm.assert_sp_series_equal(res, exp)\n \n+    def test_reindex_fill_value(self):\n+        floats = pd.Series([1., 2., 3.]).to_sparse()\n+        result = floats.reindex([1, 2, 3], fill_value=0)\n+        expected = pd.Series([2., 3., 0], index=[1, 2, 3]).to_sparse()\n+        tm.assert_sp_series_equal(result, expected)\n+\n+    def test_reindex_nearest(self):\n+        s = pd.Series(np.arange(10, dtype='float64')).to_sparse()\n+        target = [0.1, 0.9, 1.5, 2.0]\n+        actual = s.reindex(target, method='nearest')\n+        expected = pd.Series(np.around(target), target).to_sparse()\n+        tm.assert_sp_series_equal(expected, actual)\n+\n+        actual = s.reindex(target, method='nearest', tolerance=0.2)\n+        expected = pd.Series([0, 1, np.nan, 2], target).to_sparse()\n+        tm.assert_sp_series_equal(expected, actual)\n+\n     def tests_indexing_with_sparse(self):\n         # GH 13985\n \n@@ -504,6 +521,11 @@ def test_loc(self):\n         exp = orig.loc[[1, 3, 4, 5]].to_sparse()\n         tm.assert_sp_series_equal(result, exp)\n \n+        # single element list (GH 15447)\n+        result = sparse.loc[['A']]\n+        exp = orig.loc[['A']].to_sparse()\n+        tm.assert_sp_series_equal(result, exp)\n+\n         # dense array\n         result = sparse.loc[orig % 2 == 1]\n         exp = orig.loc[orig % 2 == 1].to_sparse()\n@@ -537,6 +559,35 @@ def test_loc_slice(self):\n                                   orig.loc['A':'B'].to_sparse())\n         tm.assert_sp_series_equal(sparse.loc[:'B'], orig.loc[:'B'].to_sparse())\n \n+    def test_reindex(self):\n+        # GH 15447\n+        orig = self.orig\n+        sparse = self.sparse\n+\n+        res = sparse.reindex([('A', 0), ('C', 1)])\n+        exp = orig.reindex([('A', 0), ('C', 1)]).to_sparse()\n+        tm.assert_sp_series_equal(res, exp)\n+\n+        # On specific level:\n+        res = sparse.reindex(['A', 'C', 'B'], level=0)\n+        exp = orig.reindex(['A', 'C', 'B'], level=0).to_sparse()\n+        tm.assert_sp_series_equal(res, exp)\n+\n+        # single element list (GH 15447)\n+        res = sparse.reindex(['A'], level=0)\n+        exp = orig.reindex(['A'], level=0).to_sparse()\n+        tm.assert_sp_series_equal(res, exp)\n+\n+        with tm.assertRaises(TypeError):\n+            # Incomplete keys are not accepted for reindexing:\n+            sparse.reindex(['A', 'C'])\n+\n+        # \"copy\" argument:\n+        res = sparse.reindex(sparse.index, copy=True)\n+        exp = orig.reindex(orig.index, copy=True).to_sparse()\n+        tm.assert_sp_series_equal(res, exp)\n+        self.assertIsNot(sparse, res)\n+\n \n class TestSparseDataFrameIndexing(tm.TestCase):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/sparse/test_indexing.py"
            },
            {
                "id": 15428,
                "created_at": "2017-02-16T21:53:44Z",
                "closed_at": "2017-03-08T13:43:24Z",
                "title": "BUG: pd.cut with bins=1 and input all 0s",
                "labels": "Bug, Algos",
                "commits": [
                    {
                        "hash": "d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
                        "commit_date": "2017-03-08T13:40:24Z",
                        "parents": "11c947997e0f7f91a4170ad7ddcc90124b7f5f2a",
                        "stat": {
                            "total": 8,
                            "additions": 91,
                            "deletions": 83,
                            "files": [
                                {
                                    "sha": "bf778f6065010b259735a5df4b30ecdb44ccb90d",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
                                    "patch": "@@ -698,8 +698,8 @@ Bug Fixes\n - Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`, :issue:`15424`)\n - Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a numpy array (:issue:`15434`)\n - Bug in ``Rolling.quantile`` function that caused a segmentation fault when called with a quantile value outside of the range [0, 1] (:issue:`15463`)\n-\n-\n+- Bug in ``pd.cut()`` with a single bin on an all 0s array (:issue:`15428`)\n+- Bug in ``pd.qcut()`` with a single quantile and an array with identical values (:issue:`15431`)\n - Bug in ``SparseSeries.reindex`` on single level with list of length 1 (:issue:`15447`)\n \n "
                                },
                                {
                                    "sha": "ccd8c2478e8a510960100701df1ba85361645e6b",
                                    "filename": "pandas/tools/tile.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/pandas%2Ftools%2Ftile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/pandas%2Ftools%2Ftile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftile.py?ref=d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
                                    "patch": "@@ -104,8 +104,8 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,\n         mn, mx = [mi + 0.0 for mi in rng]\n \n         if mn == mx:  # adjust end points before binning\n-            mn -= .001 * abs(mn)\n-            mx += .001 * abs(mx)\n+            mn -= .001 * abs(mn) if mn != 0 else .001\n+            mx += .001 * abs(mx) if mx != 0 else .001\n             bins = np.linspace(mn, mx, bins + 1, endpoint=True)\n         else:  # adjust end points after binning\n             bins = np.linspace(mn, mx, bins + 1, endpoint=True)\n@@ -206,7 +206,7 @@ def _bins_to_cuts(x, bins, right=True, labels=None,\n                          \"valid options are: raise, drop\")\n \n     unique_bins = algos.unique(bins)\n-    if len(unique_bins) < len(bins):\n+    if len(unique_bins) < len(bins) and len(bins) != 2:\n         if duplicates == 'raise':\n             raise ValueError(\"Bin edges must be unique: {}.\\nYou \"\n                              \"can drop duplicate edges by setting \""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "11b242bc06e15b0f2cb8209357723a41bb028a23",
                                    "filename": "pandas/tests/tools/test_tile.py",
                                    "status": "modified",
                                    "additions": 78,
                                    "deletions": 3,
                                    "changes": 81,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/pandas%2Ftests%2Ftools%2Ftest_tile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d32acaa7fbe95a96a7118a32324beea1e2e8ae32/pandas%2Ftests%2Ftools%2Ftest_tile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftools%2Ftest_tile.py?ref=d32acaa7fbe95a96a7118a32324beea1e2e8ae32",
                                    "patch": "@@ -3,7 +3,7 @@\n import numpy as np\n from pandas.compat import zip\n \n-from pandas import Series, Index\n+from pandas import Series, Index, Categorical\n import pandas.util.testing as tm\n from pandas.util.testing import assertRaisesRegexp\n import pandas.core.common as com\n@@ -239,7 +239,6 @@ def test_qcut_binning_issues(self):\n             self.assertTrue(ep <= sn)\n \n     def test_cut_return_categorical(self):\n-        from pandas import Categorical\n         s = Series([0, 1, 2, 3, 4, 5, 6, 7, 8])\n         res = cut(s, 3)\n         exp = Series(Categorical.from_codes([0, 0, 0, 1, 1, 1, 2, 2, 2],\n@@ -249,7 +248,6 @@ def test_cut_return_categorical(self):\n         tm.assert_series_equal(res, exp)\n \n     def test_qcut_return_categorical(self):\n-        from pandas import Categorical\n         s = Series([0, 1, 2, 3, 4, 5, 6, 7, 8])\n         res = qcut(s, [0, 0.333, 0.666, 1])\n         exp = Series(Categorical.from_codes([0, 0, 0, 1, 1, 1, 2, 2, 2],\n@@ -285,6 +283,60 @@ def test_qcut_duplicates_bin(self):\n         # invalid\n         self.assertRaises(ValueError, qcut, values, 3, duplicates='foo')\n \n+    def test_single_quantile(self):\n+        # issue 15431\n+        expected = Series([0, 0])\n+\n+        s = Series([9., 9.])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0, 0], [\"[9, 9]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n+        s = Series([-9., -9.])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0, 0], [\"[-9, -9]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n+        s = Series([0., 0.])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0, 0], [\"[0, 0]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n+        expected = Series([0])\n+\n+        s = Series([9])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0], [\"[9, 9]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n+        s = Series([-9])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0], [\"[-9, -9]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n+        s = Series([0])\n+        result = qcut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+        result = qcut(s, 1)\n+        exp_lab = Series(Categorical.from_codes([0], [\"[0, 0]\"],\n+                                                ordered=True))\n+        tm.assert_series_equal(result, exp_lab)\n+\n     def test_single_bin(self):\n         # issue 14652\n         expected = Series([0, 0])\n@@ -297,6 +349,29 @@ def test_single_bin(self):\n         result = cut(s, 1, labels=False)\n         tm.assert_series_equal(result, expected)\n \n+        expected = Series([0])\n+\n+        s = Series([9])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n+        s = Series([-9])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n+        # issue 15428\n+        expected = Series([0, 0])\n+\n+        s = Series([0., 0.])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n+        expected = Series([0])\n+\n+        s = Series([0])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n     def test_datetime_cut(self):\n         # GH 14714\n         # testing for time data to be present as series"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/tools/test_tile.py"
            },
            {
                "id": 15426,
                "created_at": "2017-02-16T15:48:12Z",
                "closed_at": "2017-02-27T15:46:31Z",
                "title": "BUG: timezone lost in groupby-agg with cython functions",
                "labels": "Bug, Groupby, Timezones",
                "commits": [
                    {
                        "hash": "6c17f67aafd7de8af96032aa415fc798fa3b73ca",
                        "commit_date": "2017-02-27T15:45:54Z",
                        "parents": "fb7dc7dcbde1d81dea28b1b83e1c3bd171a7e73d",
                        "stat": {
                            "total": 3,
                            "additions": 47,
                            "deletions": 44,
                            "files": [
                                {
                                    "sha": "f337d4404abfca3227e927e128c18314d8090d03",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6c17f67aafd7de8af96032aa415fc798fa3b73ca/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6c17f67aafd7de8af96032aa415fc798fa3b73ca/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=6c17f67aafd7de8af96032aa415fc798fa3b73ca",
                                    "patch": "@@ -622,6 +622,7 @@ Bug Fixes\n \n - Bug in ``DataFrame.to_stata()`` and ``StataWriter`` which produces incorrectly formatted files to be produced for some locales (:issue:`13856`)\n - Bug in ``pd.concat()`` in which concatting with an empty dataframe with ``join='inner'`` was being improperly handled (:issue:`15328`)\n+- Bug in ``groupby.agg()`` incorrectly localizing timezone on ``datetime`` (:issue:`15426`, :issue:`10668`)\n \n \n "
                                },
                                {
                                    "sha": "8cc3fe41f73c8aa3ce89959ca2c3882b70775079",
                                    "filename": "pandas/types/cast.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftypes%2Fcast.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftypes%2Fcast.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftypes%2Fcast.py?ref=6c17f67aafd7de8af96032aa415fc798fa3b73ca",
                                    "patch": "@@ -133,7 +133,8 @@ def trans(x):  # noqa\n                 if dtype.tz:\n                     # convert to datetime and change timezone\n                     from pandas import to_datetime\n-                    result = to_datetime(result).tz_localize(dtype.tz)\n+                    result = to_datetime(result).tz_localize('utc')\n+                    result = result.tz_convert(dtype.tz)\n \n     except:\n         pass"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "cb739546a23127d971e0d215ac1a90163b431bbf",
                                    "filename": "pandas/tests/groupby/test_aggregate.py",
                                    "status": "modified",
                                    "additions": 30,
                                    "deletions": 1,
                                    "changes": 31,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftests%2Fgroupby%2Ftest_aggregate.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftests%2Fgroupby%2Ftest_aggregate.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fgroupby%2Ftest_aggregate.py?ref=6c17f67aafd7de8af96032aa415fc798fa3b73ca",
                                    "patch": "@@ -6,7 +6,7 @@\n \"\"\"\n \n from __future__ import print_function\n-from datetime import datetime\n+from datetime import datetime, timedelta\n from functools import partial\n \n import numpy as np\n@@ -738,3 +738,32 @@ def test_agg_over_numpy_arrays(self):\n                                 columns=expected_column)\n \n         assert_frame_equal(result, expected)\n+\n+    def test_agg_timezone_round_trip(self):\n+        # GH 15426\n+        ts = pd.Timestamp(\"2016-01-01 12:00:00\", tz='US/Pacific')\n+        df = pd.DataFrame({'a': 1, 'b': [ts + timedelta(minutes=nn)\n+                                         for nn in range(10)]})\n+\n+        result1 = df.groupby('a')['b'].agg(np.min).iloc[0]\n+        result2 = df.groupby('a')['b'].agg(lambda x: np.min(x)).iloc[0]\n+        result3 = df.groupby('a')['b'].min().iloc[0]\n+\n+        assert result1 == ts\n+        assert result2 == ts\n+        assert result3 == ts\n+\n+        dates = [pd.Timestamp(\"2016-01-0%d 12:00:00\" % i, tz='US/Pacific')\n+                 for i in range(1, 5)]\n+        df = pd.DataFrame({'A': ['a', 'b'] * 2, 'B': dates})\n+        grouped = df.groupby('A')\n+\n+        ts = df['B'].iloc[0]\n+        assert ts == grouped.nth(0)['B'].iloc[0]\n+        assert ts == grouped.head(1)['B'].iloc[0]\n+        assert ts == grouped.first()['B'].iloc[0]\n+        assert ts == grouped.apply(lambda x: x.iloc[0])[0]\n+\n+        ts = df['B'].iloc[2]\n+        assert ts == grouped.last()['B'].iloc[0]\n+        assert ts == grouped.apply(lambda x: x.iloc[-1])[0]"
                                },
                                {
                                    "sha": "70f69cc7d5701828aeb73bcc2826e25ae8a3d643",
                                    "filename": "pandas/tests/types/test_cast.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 1,
                                    "changes": 12,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftests%2Ftypes%2Ftest_cast.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6c17f67aafd7de8af96032aa415fc798fa3b73ca/pandas%2Ftests%2Ftypes%2Ftest_cast.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftypes%2Ftest_cast.py?ref=6c17f67aafd7de8af96032aa415fc798fa3b73ca",
                                    "patch": "@@ -8,7 +8,7 @@\n from datetime import datetime\n import numpy as np\n \n-from pandas import Timedelta, Timestamp\n+from pandas import Timedelta, Timestamp, DatetimeIndex\n from pandas.types.cast import (_possibly_downcast_to_dtype,\n                                _possibly_convert_objects,\n                                _infer_dtype_from_scalar,\n@@ -71,6 +71,16 @@ def test_datetimelikes_nan(self):\n         res = _possibly_downcast_to_dtype(arr, 'timedelta64[ns]')\n         tm.assert_numpy_array_equal(res, exp)\n \n+    def test_datetime_with_timezone(self):\n+        # GH 15426\n+        ts = Timestamp(\"2016-01-01 12:00:00\", tz='US/Pacific')\n+        exp = DatetimeIndex([ts, ts])\n+        res = _possibly_downcast_to_dtype(exp, exp.dtype)\n+        tm.assert_index_equal(res, exp)\n+\n+        res = _possibly_downcast_to_dtype(exp.asi8, exp.dtype)\n+        tm.assert_index_equal(res, exp)\n+\n \n class TestInferDtype(tm.TestCase):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/groupby/test_aggregate.py\npipenv run python -m pytest pandas/tests/types/test_cast.py"
            },
            {
                "id": 15424,
                "created_at": "2017-02-16T15:39:14Z",
                "closed_at": "2017-02-20T14:44:01Z",
                "title": "Indexing MultiIndex with NDFrame indexer fails if index of indexer does not contain 0",
                "labels": "Bug, Indexing, MultiIndex",
                "commits": [
                    {
                        "hash": "821be3991cca866a5cc9cf3407cd9f68c66c0306",
                        "commit_date": "2017-02-20T14:43:17Z",
                        "parents": "be4a63fe791e27c2f8a9ae4f3a419ccc255c1b5b",
                        "stat": {
                            "total": 7,
                            "additions": 62,
                            "deletions": 55,
                            "files": [
                                {
                                    "sha": "9e71b9a11c8eb49209ecde6cdee2b87b6d28efb0",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/821be3991cca866a5cc9cf3407cd9f68c66c0306/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/821be3991cca866a5cc9cf3407cd9f68c66c0306/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=821be3991cca866a5cc9cf3407cd9f68c66c0306",
                                    "patch": "@@ -501,7 +501,8 @@ Bug Fixes\n - Bug in ``pd.tools.hashing.hash_pandas_object()`` in which hashing of categoricals depended on the ordering of categories, instead of just their values. (:issue:`15143`)\n - Bug in ``.groupby(..).resample()`` when passed the ``on=`` kwarg. (:issue:`15021`)\n \n-- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`)\n+- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`, :issue:`15424`)\n+- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a numpy array (:issue:`15434`)\n \n \n "
                                },
                                {
                                    "sha": "6f490875742ca40b9cde9b05b89f60d55c6118e7",
                                    "filename": "pandas/core/indexing.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 6,
                                    "changes": 25,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/821be3991cca866a5cc9cf3407cd9f68c66c0306/pandas%2Fcore%2Findexing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/821be3991cca866a5cc9cf3407cd9f68c66c0306/pandas%2Fcore%2Findexing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Findexing.py?ref=821be3991cca866a5cc9cf3407cd9f68c66c0306",
                                    "patch": "@@ -1521,15 +1521,28 @@ def _getitem_axis(self, key, axis=0):\n             return self._getbool_axis(key, axis=axis)\n         elif is_list_like_indexer(key):\n \n-            # GH 7349\n-            # possibly convert a list-like into a nested tuple\n-            # but don't convert a list-like of tuples\n+            # convert various list-like indexers\n+            # to a list of keys\n+            # we will use the *values* of the object\n+            # and NOT the index if its a PandasObject\n             if isinstance(labels, MultiIndex):\n+\n+                if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:\n+                    # Series, or 0,1 ndim ndarray\n+                    # GH 14730\n+                    key = list(key)\n+                elif isinstance(key, ABCDataFrame):\n+                    # GH 15438\n+                    raise NotImplementedError(\"Indexing a MultiIndex with a \"\n+                                              \"DataFrame key is not \"\n+                                              \"implemented\")\n+                elif hasattr(key, 'ndim') and key.ndim > 1:\n+                    raise NotImplementedError(\"Indexing a MultiIndex with a \"\n+                                              \"multidimensional key is not \"\n+                                              \"implemented\")\n+\n                 if (not isinstance(key, tuple) and len(key) > 1 and\n                         not isinstance(key[0], tuple)):\n-                    if isinstance(key, ABCSeries):\n-                        # GH 14730\n-                        key = list(key)\n                     key = tuple([key])\n \n             # an iterable multi-selection"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b40f0b8cd99765d16387a14d69735b0830844606",
                                    "filename": "pandas/tests/indexing/test_multiindex.py",
                                    "status": "modified",
                                    "additions": 34,
                                    "deletions": 0,
                                    "changes": 34,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/821be3991cca866a5cc9cf3407cd9f68c66c0306/pandas%2Ftests%2Findexing%2Ftest_multiindex.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/821be3991cca866a5cc9cf3407cd9f68c66c0306/pandas%2Ftests%2Findexing%2Ftest_multiindex.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexing%2Ftest_multiindex.py?ref=821be3991cca866a5cc9cf3407cd9f68c66c0306",
                                    "patch": "@@ -158,12 +158,46 @@ def test_loc_getitem_series(self):\n         result = x.loc[[1, 3]]\n         tm.assert_series_equal(result, expected)\n \n+        # GH15424\n+        y1 = Series([1, 3], index=[1, 2])\n+        result = x.loc[y1]\n+        tm.assert_series_equal(result, expected)\n+\n         empty = Series(data=[], dtype=np.float64)\n         expected = Series([], index=MultiIndex(\n             levels=index.levels, labels=[[], []], dtype=np.float64))\n         result = x.loc[empty]\n         tm.assert_series_equal(result, expected)\n \n+    def test_loc_getitem_array(self):\n+        # GH15434\n+        # passing an array as a key with a MultiIndex\n+        index = MultiIndex.from_product([[1, 2, 3], ['A', 'B', 'C']])\n+        x = Series(index=index, data=range(9), dtype=np.float64)\n+        y = np.array([1, 3])\n+        expected = Series(\n+            data=[0, 1, 2, 6, 7, 8],\n+            index=MultiIndex.from_product([[1, 3], ['A', 'B', 'C']]),\n+            dtype=np.float64)\n+        result = x.loc[y]\n+        tm.assert_series_equal(result, expected)\n+\n+        # empty array:\n+        empty = np.array([])\n+        expected = Series([], index=MultiIndex(\n+            levels=index.levels, labels=[[], []], dtype=np.float64))\n+        result = x.loc[empty]\n+        tm.assert_series_equal(result, expected)\n+\n+        # 0-dim array (scalar):\n+        scalar = np.int64(1)\n+        expected = Series(\n+            data=[0, 1, 2],\n+            index=['A', 'B', 'C'],\n+            dtype=np.float64)\n+        result = x.loc[scalar]\n+        tm.assert_series_equal(result, expected)\n+\n     def test_iloc_getitem_multiindex(self):\n         mi_labels = DataFrame(np.random.randn(4, 3),\n                               columns=[['i', 'i', 'j'], ['A', 'A', 'B']],"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexing/test_multiindex.py"
            },
            {
                "id": 15420,
                "created_at": "2017-02-16T04:01:20Z",
                "closed_at": "2017-02-24T19:57:32Z",
                "title": "rank incorrectly orders ordered categories",
                "labels": "Bug, Categorical",
                "commits": [
                    {
                        "hash": "3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
                        "commit_date": "2017-02-24T19:56:12Z",
                        "parents": "924c16667ee3db5d025c0963f99a778de8aad398",
                        "stat": {
                            "total": 1,
                            "additions": 106,
                            "deletions": 105,
                            "files": [
                                {
                                    "sha": "0b501adba503992242a56424e3ae94fb9347f9f7",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
                                    "patch": "@@ -578,6 +578,7 @@ Bug Fixes\n \n \n \n+- Bug in ``.rank()`` which incorrectly ranks ordered categories (:issue:`15420`)\n \n \n "
                                },
                                {
                                    "sha": "b11927a80fb2e2b9275724a17eff25ed397bda58",
                                    "filename": "pandas/core/algorithms.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 1,
                                    "changes": 5,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Fcore%2Falgorithms.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Fcore%2Falgorithms.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Falgorithms.py?ref=3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
                                    "patch": "@@ -973,6 +973,10 @@ def _hashtable_algo(f, values, return_dtype=None):\n def _get_data_algo(values, func_map):\n \n     f = None\n+\n+    if is_categorical_dtype(values):\n+        values = values._values_for_rank()\n+\n     if is_float_dtype(values):\n         f = func_map['float64']\n         values = _ensure_float64(values)\n@@ -988,7 +992,6 @@ def _get_data_algo(values, func_map):\n     elif is_unsigned_integer_dtype(values):\n         f = func_map['uint64']\n         values = _ensure_uint64(values)\n-\n     else:\n         values = _ensure_object(values)\n "
                                },
                                {
                                    "sha": "b88a6b171b316c6da97f45ee45f2f40d7c824c2c",
                                    "filename": "pandas/core/categorical.py",
                                    "status": "modified",
                                    "additions": 22,
                                    "deletions": 0,
                                    "changes": 22,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Fcore%2Fcategorical.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Fcore%2Fcategorical.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fcategorical.py?ref=3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
                                    "patch": "@@ -1404,6 +1404,28 @@ def sort_values(self, inplace=False, ascending=True, na_position='last'):\n             return self._constructor(values=codes, categories=self.categories,\n                                      ordered=self.ordered, fastpath=True)\n \n+    def _values_for_rank(self):\n+        \"\"\"\n+        For correctly ranking ordered categorical data. See GH#15420\n+\n+        Ordered categorical data should be ranked on the basis of\n+        codes with -1 translated to NaN.\n+\n+        Returns\n+        -------\n+        numpy array\n+\n+        \"\"\"\n+        if self.ordered:\n+            values = self.codes\n+            mask = values == -1\n+            if mask.any():\n+                values = values.astype('float64')\n+                values[mask] = np.nan\n+        else:\n+            values = np.array(self)\n+        return values\n+\n     def order(self, inplace=False, ascending=True, na_position='last'):\n         \"\"\"\n         DEPRECATED: use :meth:`Categorical.sort_values`. That function"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b092e4f084767033b4d8e083edf4c761f9b45f72",
                                    "filename": "pandas/tests/series/test_analytics.py",
                                    "status": "modified",
                                    "additions": 78,
                                    "deletions": 0,
                                    "changes": 78,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Ftests%2Fseries%2Ftest_analytics.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/3fe85afef47e9e079a0fa24f826bb6faaa2341d5/pandas%2Ftests%2Fseries%2Ftest_analytics.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fseries%2Ftest_analytics.py?ref=3fe85afef47e9e079a0fa24f826bb6faaa2341d5",
                                    "patch": "@@ -1057,6 +1057,84 @@ def test_rank(self):\n         iranks = iseries.rank()\n         assert_series_equal(iranks, exp)\n \n+    def test_rank_categorical(self):\n+        # GH issue #15420 rank incorrectly orders ordered categories\n+\n+        # Test ascending/descending ranking for ordered categoricals\n+        exp = pd.Series([1., 2., 3., 4., 5., 6.])\n+        exp_desc = pd.Series([6., 5., 4., 3., 2., 1.])\n+        ordered = pd.Series(\n+            ['first', 'second', 'third', 'fourth', 'fifth', 'sixth']\n+        ).astype('category', ).cat.set_categories(\n+            ['first', 'second', 'third', 'fourth', 'fifth', 'sixth'],\n+            ordered=True\n+        )\n+        assert_series_equal(ordered.rank(), exp)\n+        assert_series_equal(ordered.rank(ascending=False), exp_desc)\n+\n+        # Unordered categoricals should be ranked as objects\n+        unordered = pd.Series(\n+            ['first', 'second', 'third', 'fourth', 'fifth', 'sixth'],\n+        ).astype('category').cat.set_categories(\n+            ['first', 'second', 'third', 'fourth', 'fifth', 'sixth'],\n+            ordered=False\n+        )\n+        exp_unordered = pd.Series([2., 4., 6., 3., 1., 5.])\n+        res = unordered.rank()\n+        assert_series_equal(res, exp_unordered)\n+\n+        # Test na_option for rank data\n+        na_ser = pd.Series(\n+            ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', np.NaN]\n+        ).astype('category', ).cat.set_categories(\n+            [\n+                'first', 'second', 'third', 'fourth',\n+                'fifth', 'sixth', 'seventh'\n+            ],\n+            ordered=True\n+        )\n+\n+        exp_top = pd.Series([2., 3., 4., 5., 6., 7., 1.])\n+        exp_bot = pd.Series([1., 2., 3., 4., 5., 6., 7.])\n+        exp_keep = pd.Series([1., 2., 3., 4., 5., 6., np.NaN])\n+\n+        assert_series_equal(na_ser.rank(na_option='top'), exp_top)\n+        assert_series_equal(na_ser.rank(na_option='bottom'), exp_bot)\n+        assert_series_equal(na_ser.rank(na_option='keep'), exp_keep)\n+\n+        # Test na_option for rank data with ascending False\n+        exp_top = pd.Series([7., 6., 5., 4., 3., 2., 1.])\n+        exp_bot = pd.Series([6., 5., 4., 3., 2., 1., 7.])\n+        exp_keep = pd.Series([6., 5., 4., 3., 2., 1., np.NaN])\n+\n+        assert_series_equal(\n+            na_ser.rank(na_option='top', ascending=False),\n+            exp_top\n+        )\n+        assert_series_equal(\n+            na_ser.rank(na_option='bottom', ascending=False),\n+            exp_bot\n+        )\n+        assert_series_equal(\n+            na_ser.rank(na_option='keep', ascending=False),\n+            exp_keep\n+        )\n+\n+        # Test with pct=True\n+        na_ser = pd.Series(\n+            ['first', 'second', 'third', 'fourth', np.NaN],\n+        ).astype('category').cat.set_categories(\n+            ['first', 'second', 'third', 'fourth'],\n+            ordered=True\n+        )\n+        exp_top = pd.Series([0.4, 0.6, 0.8, 1., 0.2])\n+        exp_bot = pd.Series([0.2, 0.4, 0.6, 0.8, 1.])\n+        exp_keep = pd.Series([0.25, 0.5, 0.75, 1., np.NaN])\n+\n+        assert_series_equal(na_ser.rank(na_option='top', pct=True), exp_top)\n+        assert_series_equal(na_ser.rank(na_option='bottom', pct=True), exp_bot)\n+        assert_series_equal(na_ser.rank(na_option='keep', pct=True), exp_keep)\n+\n     def test_rank_signature(self):\n         s = Series([0, 1])\n         s.rank(method='average')"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/series/test_analytics.py "
            },
            {
                "id": 15348,
                "created_at": "2017-02-08T16:17:43Z",
                "closed_at": "2017-02-10T19:51:05Z",
                "title": "pd.melt does not allow a tuple for value_vars",
                "labels": "Bug, Reshaping, API Design",
                "commits": [
                    {
                        "hash": "61deba5cfc43425e35c8fc61bcad1123c83a6a5a",
                        "commit_date": "2017-02-10T19:49:06Z",
                        "parents": "dcb4e47a0b6620f1efbe5e02ed493e6513fc8763",
                        "stat": {
                            "total": 4,
                            "additions": 56,
                            "deletions": 52,
                            "files": [
                                {
                                    "sha": "9f86c777c665dd882b60711d831011ffe4b0774a",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 2,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=61deba5cfc43425e35c8fc61bcad1123c83a6a5a",
                                    "patch": "@@ -578,6 +578,5 @@ Bug Fixes\n \n - Bug in ``DataFrame.boxplot`` where ``fontsize`` was not applied to the tick labels on both axes (:issue:`15108`)\n - Bug in ``Series.replace`` and ``DataFrame.replace`` which failed on empty replacement dicts (:issue:`15289`)\n-\n-\n+- Bug in ``pd.melt()`` where passing a tuple value for ``value_vars`` caused a ``TypeError`` (:issue:`15348`)\n - Bug in ``.eval()`` which caused multiline evals to fail with local variables not on the first line (:issue:`15342`)"
                                },
                                {
                                    "sha": "cebaf4e3fd89ba1a9e0eac1a1c8977c2f2f06c5f",
                                    "filename": "pandas/core/reshape.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 2,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/pandas%2Fcore%2Freshape.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/pandas%2Fcore%2Freshape.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Freshape.py?ref=61deba5cfc43425e35c8fc61bcad1123c83a6a5a",
                                    "patch": "@@ -761,16 +761,26 @@ def melt(frame, id_vars=None, value_vars=None, var_name=None,\n     \"\"\"\n     # TODO: what about the existing index?\n     if id_vars is not None:\n-        if not isinstance(id_vars, (tuple, list, np.ndarray)):\n+        if not is_list_like(id_vars):\n             id_vars = [id_vars]\n+        elif (isinstance(frame.columns, MultiIndex) and\n+              not isinstance(id_vars, list)):\n+            raise ValueError('id_vars must be a list of tuples when columns'\n+                             ' are a MultiIndex')\n         else:\n             id_vars = list(id_vars)\n     else:\n         id_vars = []\n \n     if value_vars is not None:\n-        if not isinstance(value_vars, (tuple, list, np.ndarray)):\n+        if not is_list_like(value_vars):\n             value_vars = [value_vars]\n+        elif (isinstance(frame.columns, MultiIndex) and\n+              not isinstance(value_vars, list)):\n+            raise ValueError('value_vars must be a list of tuples when'\n+                             ' columns are a MultiIndex')\n+        else:\n+            value_vars = list(value_vars)\n         frame = frame.loc[:, id_vars + value_vars]\n     else:\n         frame = frame.copy()"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "d587e4ea6a1fa4502bda75f07a668079142c353e",
                                    "filename": "pandas/tests/test_reshape.py",
                                    "status": "modified",
                                    "additions": 39,
                                    "deletions": 0,
                                    "changes": 39,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/pandas%2Ftests%2Ftest_reshape.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/61deba5cfc43425e35c8fc61bcad1123c83a6a5a/pandas%2Ftests%2Ftest_reshape.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_reshape.py?ref=61deba5cfc43425e35c8fc61bcad1123c83a6a5a",
                                    "patch": "@@ -56,6 +56,45 @@ def test_value_vars(self):\n                               columns=['id1', 'id2', 'variable', 'value'])\n         tm.assert_frame_equal(result4, expected4)\n \n+    def test_value_vars_types(self):\n+        # GH 15348\n+        expected = DataFrame({'id1': self.df['id1'].tolist() * 2,\n+                              'id2': self.df['id2'].tolist() * 2,\n+                              'variable': ['A'] * 10 + ['B'] * 10,\n+                              'value': (self.df['A'].tolist() +\n+                                        self.df['B'].tolist())},\n+                             columns=['id1', 'id2', 'variable', 'value'])\n+\n+        for type_ in (tuple, list, np.array):\n+            result = melt(self.df, id_vars=['id1', 'id2'],\n+                          value_vars=type_(('A', 'B')))\n+            tm.assert_frame_equal(result, expected)\n+\n+    def test_vars_work_with_multiindex(self):\n+        expected = DataFrame({\n+            ('A', 'a'): self.df1[('A', 'a')],\n+            'CAP': ['B'] * len(self.df1),\n+            'low': ['b'] * len(self.df1),\n+            'value': self.df1[('B', 'b')],\n+        }, columns=[('A', 'a'), 'CAP', 'low', 'value'])\n+\n+        result = melt(self.df1, id_vars=[('A', 'a')], value_vars=[('B', 'b')])\n+        tm.assert_frame_equal(result, expected)\n+\n+    def test_tuple_vars_fail_with_multiindex(self):\n+        # melt should fail with an informative error message if\n+        # the columns have a MultiIndex and a tuple is passed\n+        # for id_vars or value_vars.\n+        tuple_a = ('A', 'a')\n+        list_a = [tuple_a]\n+        tuple_b = ('B', 'b')\n+        list_b = [tuple_b]\n+\n+        for id_vars, value_vars in ((tuple_a, list_b), (list_a, tuple_b),\n+                                    (tuple_a, tuple_b)):\n+            with tm.assertRaisesRegexp(ValueError, r'MultiIndex'):\n+                melt(self.df1, id_vars=id_vars, value_vars=value_vars)\n+\n     def test_custom_var_name(self):\n         result5 = melt(self.df, var_name=self.var_name)\n         self.assertEqual(result5.columns.tolist(), ['var', 'value'])"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_reshape.py"
            },
            {
                "id": 15277,
                "created_at": "2017-02-01T06:08:04Z",
                "closed_at": "2017-02-06T15:26:42Z",
                "title": "DataFrame.fillna() downcast argument does not work when the value for fillna is a dict instead of a scalar",
                "labels": "Bug, Missing-data",
                "commits": [
                    {
                        "hash": "f742a66a9b1c5c7756ecfefb5d38c5fca14700b2",
                        "commit_date": "2017-02-06T15:25:58Z",
                        "parents": "7d6afc4b22fde9ce32161917c2440947505bf4ad",
                        "stat": {
                            "total": 3,
                            "additions": 33,
                            "deletions": 30,
                            "files": [
                                {
                                    "sha": "16caef57673f71afbc11fef33539a3c10d3c761b",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 2,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=f742a66a9b1c5c7756ecfefb5d38c5fca14700b2",
                                    "patch": "@@ -496,6 +496,7 @@ Bug Fixes\n - Bug in ``pd.read_csv()`` for the C engine where ``usecols`` were being indexed incorrectly with ``parse_dates`` (:issue:`14792`)\n - Incorrect dtyped ``Series`` was returned by comparison methods (e.g., ``lt``, ``gt``, ...) against a constant for an empty ``DataFrame`` (:issue:`15077`)\n - Bug in ``Series.dt.round`` inconsistent behaviour on NAT's with different arguments (:issue:`14940`)\n+- Bug in ``DataFrame.fillna()`` where the argument ``downcast`` was ignored when fillna value was of type ``dict`` (:issue:`15277`)\n \n \n - Bug in ``.read_json()`` for Python 2 where ``lines=True`` and contents contain non-ascii unicode characters (:issue:`15132`)\n@@ -509,7 +510,5 @@ Bug Fixes\n \n \n \n-\n-\n - Bug in ``DataFrame.boxplot`` where ``fontsize`` was not applied to the tick labels on both axes (:issue:`15108`)\n - Bug in ``Series.replace`` and ``DataFrame.replace`` which failed on empty replacement dicts (:issue:`15289`)"
                                },
                                {
                                    "sha": "bb2664a5b8d28819bc492b7c5fedc828a7ce9f58",
                                    "filename": "pandas/core/generic.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Fcore%2Fgeneric.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Fcore%2Fgeneric.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fgeneric.py?ref=f742a66a9b1c5c7756ecfefb5d38c5fca14700b2",
                                    "patch": "@@ -3347,7 +3347,7 @@ def fillna(self, value=None, method=None, axis=None, inplace=False,\n                     if k not in result:\n                         continue\n                     obj = result[k]\n-                    obj.fillna(v, limit=limit, inplace=True)\n+                    obj.fillna(v, limit=limit, inplace=True, downcast=downcast)\n                 return result\n             elif not is_list_like(value):\n                 new_data = self._data.fillna(value=value, limit=limit,"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "eabdb79295c27496a5ccfea9c64e5297a7cf71b4",
                                    "filename": "pandas/tests/frame/test_missing.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Ftests%2Fframe%2Ftest_missing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Ftests%2Fframe%2Ftest_missing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_missing.py?ref=f742a66a9b1c5c7756ecfefb5d38c5fca14700b2",
                                    "patch": "@@ -252,6 +252,20 @@ def test_fillna(self):\n         result = df.fillna(value={'Date': df['Date2']})\n         assert_frame_equal(result, expected)\n \n+    def test_fillna_downcast(self):\n+        # GH 15277\n+        # infer int64 from float64\n+        df = pd.DataFrame({'a': [1., np.nan]})\n+        result = df.fillna(0, downcast='infer')\n+        expected = pd.DataFrame({'a': [1, 0]})\n+        assert_frame_equal(result, expected)\n+\n+        # infer int64 from float64 when fillna value is a dict\n+        df = pd.DataFrame({'a': [1., np.nan]})\n+        result = df.fillna({'a': 0}, downcast='infer')\n+        expected = pd.DataFrame({'a': [1, 0]})\n+        assert_frame_equal(result, expected)\n+\n     def test_fillna_dtype_conversion(self):\n         # make sure that fillna on an empty frame works\n         df = DataFrame(index=[\"A\", \"B\", \"C\"], columns=[1, 2, 3, 4, 5])"
                                },
                                {
                                    "sha": "8c877ade6fe98ee7b4ae8dc2660e054ddb21b81a",
                                    "filename": "pandas/tests/series/test_missing.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Ftests%2Fseries%2Ftest_missing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f742a66a9b1c5c7756ecfefb5d38c5fca14700b2/pandas%2Ftests%2Fseries%2Ftest_missing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fseries%2Ftest_missing.py?ref=f742a66a9b1c5c7756ecfefb5d38c5fca14700b2",
                                    "patch": "@@ -273,6 +273,20 @@ def test_datetime64tz_fillna_round_issue(self):\n \n         assert_series_equal(filled, expected)\n \n+    def test_fillna_downcast(self):\n+        # GH 15277\n+        # infer int64 from float64\n+        s = pd.Series([1., np.nan])\n+        result = s.fillna(0, downcast='infer')\n+        expected = pd.Series([1, 0])\n+        assert_series_equal(result, expected)\n+\n+        # infer int64 from float64 when fillna value is a dict\n+        s = pd.Series([1., np.nan])\n+        result = s.fillna({1: 0}, downcast='infer')\n+        expected = pd.Series([1, 0])\n+        assert_series_equal(result, expected)\n+\n     def test_fillna_int(self):\n         s = Series(np.random.randint(-100, 100, 50))\n         s.fillna(method='ffill', inplace=True)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_missing.py\npipenv run python -m pytest pandas/tests/series/test_missing.py"
            },
            {
                "id": 15262,
                "created_at": "2017-01-30T15:29:29Z",
                "closed_at": "2017-02-24T20:18:20Z",
                "title": "BUG in MultiIndex truncated repr with integer level names",
                "labels": "Bug, Output-Formatting",
                "commits": [
                    {
                        "hash": "595580464a256fb883e8baa5b6e62f2013f0cf1a",
                        "commit_date": "2017-02-24T20:17:01Z",
                        "parents": "7e0a71b02d77a8efbadf2e8c804dbff59639061e",
                        "stat": {
                            "total": 22,
                            "additions": 68,
                            "deletions": 46,
                            "files": [
                                {
                                    "sha": "7426b5ca2a69de73faa086487847350e5a255582",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -550,6 +550,7 @@ Bug Fixes\n \n - Bug in ``Series.where()`` and ``DataFrame.where()`` where array-like conditionals were being rejected (:issue:`15414`)\n - Bug in ``Series`` construction with a datetimetz (:issue:`14928`)\n+- Bug in output formatting of a ``MultiIndex`` when names are integers (:issue:`12223`, :issue:`15262`)\n \n - Bug in compat for passing long integers to ``Timestamp.replace`` (:issue:`15030`)\n - Bug in ``.loc`` that would not return the correct dtype for scalar access for a DataFrame (:issue:`11617`)"
                                },
                                {
                                    "sha": "ce3481fc17c5b32d5ed57d75e208720042645782",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -2876,7 +2876,7 @@ def set_index(self, keys, drop=True, append=False, inplace=False,\n             names = [x for x in self.index.names]\n             if isinstance(self.index, MultiIndex):\n                 for i in range(self.index.nlevels):\n-                    arrays.append(self.index.get_level_values(i))\n+                    arrays.append(self.index._get_level_values(i))\n             else:\n                 arrays.append(self.index)\n \n@@ -2886,9 +2886,9 @@ def set_index(self, keys, drop=True, append=False, inplace=False,\n                 # append all but the last column so we don't have to modify\n                 # the end of this loop\n                 for n in range(col.nlevels - 1):\n-                    arrays.append(col.get_level_values(n))\n+                    arrays.append(col._get_level_values(n))\n \n-                level = col.get_level_values(col.nlevels - 1)\n+                level = col._get_level_values(col.nlevels - 1)\n                 names.extend(col.names)\n             elif isinstance(col, Series):\n                 level = col._values"
                                },
                                {
                                    "sha": "831ca3886773ec3cd82e06e954cf42a3c9e6597a",
                                    "filename": "pandas/core/groupby.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Fgroupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Fgroupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fgroupby.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -291,8 +291,8 @@ def _set_grouper(self, obj, sort=False):\n                 # equivalent to the axis name\n                 if isinstance(ax, MultiIndex):\n                     level = ax._get_level_number(level)\n-                    ax = Index(ax.get_level_values(\n-                        level), name=ax.names[level])\n+                    ax = Index(ax._get_level_values(level),\n+                               name=ax.names[level])\n \n                 else:\n                     if level not in (0, ax.name):\n@@ -761,7 +761,7 @@ def _index_with_as_index(self, b):\n         gp = self.grouper\n         levels = chain((gp.levels[i][gp.labels[i][b]]\n                         for i in range(len(gp.groupings))),\n-                       (original.get_level_values(i)[b]\n+                       (original._get_level_values(i)[b]\n                         for i in range(original.nlevels)))\n         new = MultiIndex.from_arrays(list(levels))\n         new.names = gp.names + original.names"
                                },
                                {
                                    "sha": "87cb088c2e91efa68d0801641a6c06f14efdaf0a",
                                    "filename": "pandas/core/reshape.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Freshape.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fcore%2Freshape.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Freshape.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -811,7 +811,8 @@ def melt(frame, id_vars=None, value_vars=None, var_name=None,\n     mdata[value_name] = frame.values.ravel('F')\n     for i, col in enumerate(var_name):\n         # asanyarray will keep the columns as an Index\n-        mdata[col] = np.asanyarray(frame.columns.get_level_values(i)).repeat(N)\n+        mdata[col] = np.asanyarray(frame.columns\n+                                   ._get_level_values(i)).repeat(N)\n \n     return DataFrame(mdata, columns=mcolumns)\n "
                                },
                                {
                                    "sha": "4c081770e01252ae6c3abcb8f2ba5103e5d1ba9f",
                                    "filename": "pandas/formats/format.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fformats%2Fformat.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fformats%2Fformat.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fformats%2Fformat.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -1566,7 +1566,7 @@ def _save_header(self):\n                     if isinstance(index_label, list) and len(index_label) > 1:\n                         col_line.extend([''] * (len(index_label) - 1))\n \n-                col_line.extend(columns.get_level_values(i))\n+                col_line.extend(columns._get_level_values(i))\n \n                 writer.writerow(col_line)\n "
                                },
                                {
                                    "sha": "5d43d2d32af6707ce385aeb6bc69dbc88e3d5ed6",
                                    "filename": "pandas/indexes/base.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 4,
                                    "changes": 10,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Findexes%2Fbase.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Findexes%2Fbase.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fbase.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -2334,9 +2334,9 @@ def set_value(self, arr, key, value):\n         self._engine.set_value(_values_from_object(arr),\n                                _values_from_object(key), value)\n \n-    def get_level_values(self, level):\n+    def _get_level_values(self, level):\n         \"\"\"\n-        Return vector of label values for requested level, equal to the length\n+        Return an Index of values for requested level, equal to the length\n         of the index\n \n         Parameters\n@@ -2345,12 +2345,14 @@ def get_level_values(self, level):\n \n         Returns\n         -------\n-        values : ndarray\n+        values : Index\n         \"\"\"\n-        # checks that level number is actually just 1\n+\n         self._validate_index_level(level)\n         return self\n \n+    get_level_values = _get_level_values\n+\n     _index_shared_docs['get_indexer'] = \"\"\"\n         Compute indexer and mask for new index given the current index. The\n         indexer should be then used as an input to ndarray.take to align the"
                                },
                                {
                                    "sha": "23a42265a149b8ad386164a2110fd10483316378",
                                    "filename": "pandas/indexes/multi.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 6,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Findexes%2Fmulti.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Findexes%2Fmulti.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fmulti.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -684,7 +684,7 @@ def is_monotonic_increasing(self):\n         \"\"\"\n \n         # reversed() because lexsort() wants the most significant key last.\n-        values = [self._get_level_values(i)\n+        values = [self._get_level_values(i).values\n                   for i in reversed(range(len(self.levels)))]\n         try:\n             sort_order = np.lexsort(values)\n@@ -866,7 +866,8 @@ def _get_level_values(self, level):\n         labels = self.labels[level]\n         filled = algos.take_1d(unique._values, labels,\n                                fill_value=unique._na_value)\n-        return filled\n+        values = unique._shallow_copy(filled)\n+        return values\n \n     def get_level_values(self, level):\n         \"\"\"\n@@ -883,7 +884,7 @@ def get_level_values(self, level):\n         \"\"\"\n         level = self._get_level_number(level)\n         values = self._get_level_values(level)\n-        return self.levels[level]._shallow_copy(values)\n+        return values\n \n     def format(self, space=2, sparsify=None, adjoin=True, names=False,\n                na_rep=None, formatter=None):\n@@ -966,7 +967,8 @@ def to_frame(self, index=True):\n         \"\"\"\n \n         from pandas import DataFrame\n-        result = DataFrame({(name or level): self.get_level_values(level)\n+        result = DataFrame({(name or level):\n+                            self._get_level_values(level)\n                             for name, level in\n                             zip(self.names, range(len(self.levels)))},\n                            copy=False)\n@@ -1301,8 +1303,8 @@ def append(self, other):\n                for o in other):\n             arrays = []\n             for i in range(self.nlevels):\n-                label = self.get_level_values(i)\n-                appended = [o.get_level_values(i) for o in other]\n+                label = self._get_level_values(i)\n+                appended = [o._get_level_values(i) for o in other]\n                 arrays.append(label.append(appended))\n             return MultiIndex.from_arrays(arrays, names=self.names)\n "
                                },
                                {
                                    "sha": "2ab642b3af0c7a54fa815cb5a12827018b2c7084",
                                    "filename": "pandas/io/sql.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fio%2Fsql.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Fio%2Fsql.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fio%2Fsql.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -749,7 +749,7 @@ def _get_column_names_and_types(self, dtype_mapper):\n         if self.index is not None:\n             for i, idx_label in enumerate(self.index):\n                 idx_type = dtype_mapper(\n-                    self.frame.index.get_level_values(i))\n+                    self.frame.index._get_level_values(i))\n                 column_names_and_types.append((text_type(idx_label),\n                                               idx_type, True))\n "
                                },
                                {
                                    "sha": "6df6444aeafab4d0bde0d19979f10af7b66c0507",
                                    "filename": "pandas/util/doctools.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Futil%2Fdoctools.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Futil%2Fdoctools.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Futil%2Fdoctools.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -113,12 +113,12 @@ def _insert_index(self, data):\n         else:\n             for i in range(idx_nlevels):\n                 data.insert(i, 'Index{0}'.format(i),\n-                            data.index.get_level_values(i))\n+                            data.index._get_level_values(i))\n \n         col_nlevels = data.columns.nlevels\n         if col_nlevels > 1:\n-            col = data.columns.get_level_values(0)\n-            values = [data.columns.get_level_values(i).values\n+            col = data.columns._get_level_values(0)\n+            values = [data.columns._get_level_values(i).values\n                       for i in range(1, col_nlevels)]\n             col_df = pd.DataFrame(values)\n             data.columns = col_df.columns"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "6f06a55ad065efc46e840845d3e91b44ba85e0c4",
                                    "filename": "pandas/tests/frame/test_combine_concat.py",
                                    "status": "modified",
                                    "additions": 18,
                                    "deletions": 0,
                                    "changes": 18,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/595580464a256fb883e8baa5b6e62f2013f0cf1a/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py?ref=595580464a256fb883e8baa5b6e62f2013f0cf1a",
                                    "patch": "@@ -422,6 +422,24 @@ def test_concat_axis_parameter(self):\n         with assertRaisesRegexp(ValueError, 'No axis named'):\n             pd.concat([series1, series2], axis='something')\n \n+    def test_concat_numerical_names(self):\n+        # #15262  # #12223\n+        df = pd.DataFrame({'col': range(9)},\n+                          dtype='int32',\n+                          index=(pd.MultiIndex\n+                                 .from_product([['A0', 'A1', 'A2'],\n+                                                ['B0', 'B1', 'B2']],\n+                                               names=[1, 2])))\n+        result = pd.concat((df.iloc[:2, :], df.iloc[-2:, :]))\n+        expected = pd.DataFrame({'col': [0, 1, 7, 8]},\n+                                dtype='int32',\n+                                index=pd.MultiIndex.from_tuples([('A0', 'B0'),\n+                                                                 ('A0', 'B1'),\n+                                                                 ('A2', 'B1'),\n+                                                                 ('A2', 'B2')],\n+                                                                names=[1, 2]))\n+        tm.assert_frame_equal(result, expected)\n+\n \n class TestDataFrameCombineFirst(tm.TestCase, TestData):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_combine_concat.py"
            },
            {
                "id": 15155,
                "created_at": "2017-01-18T18:36:35Z",
                "closed_at": "2017-01-19T20:21:53Z",
                "title": "Groupby level fails to enumerate groups",
                "labels": "Bug, Groupby, Categorical",
                "commits": [
                    {
                        "hash": "4c65d5fc79ea435bc4e47d8af2914cba324117bd",
                        "commit_date": "2017-01-19T20:19:44Z",
                        "parents": "fd5471208244ae1cb9cb426d6aa02ab408cfacba",
                        "stat": {
                            "total": 0,
                            "additions": 24,
                            "deletions": 24,
                            "files": [
                                {
                                    "sha": "0c065937c3622d99cedcab454dd090c9cc5dafb8",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/4c65d5fc79ea435bc4e47d8af2914cba324117bd/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/4c65d5fc79ea435bc4e47d8af2914cba324117bd/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=4c65d5fc79ea435bc4e47d8af2914cba324117bd",
                                    "patch": "@@ -388,6 +388,7 @@ Bug Fixes\n \n - Bug in compat for passing long integers to ``Timestamp.replace`` (:issue:`15030`)\n - Bug in ``.loc`` that would not return the correct dtype for scalar access for a DataFrame (:issue:`11617`)\n+- Bug in ``GroupBy.get_group()`` failing with a categorical grouper (:issue:`15155`)\n \n \n "
                                },
                                {
                                    "sha": "e3ffa40f5f94a124c0842ddc691ee5ce4f0a0741",
                                    "filename": "pandas/indexes/category.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/4c65d5fc79ea435bc4e47d8af2914cba324117bd/pandas%2Findexes%2Fcategory.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/4c65d5fc79ea435bc4e47d8af2914cba324117bd/pandas%2Findexes%2Fcategory.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fcategory.py?ref=4c65d5fc79ea435bc4e47d8af2914cba324117bd",
                                    "patch": "@@ -255,6 +255,9 @@ def categories(self):\n     def ordered(self):\n         return self._data.ordered\n \n+    def _reverse_indexer(self):\n+        return self._data._reverse_indexer()\n+\n     def __contains__(self, key):\n         hash(key)\n         return key in self.values"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "81aa183426be98ad2ca58524d0e6407959ecbc75",
                                    "filename": "pandas/tests/groupby/test_categorical.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/4c65d5fc79ea435bc4e47d8af2914cba324117bd/pandas%2Ftests%2Fgroupby%2Ftest_categorical.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/4c65d5fc79ea435bc4e47d8af2914cba324117bd/pandas%2Ftests%2Fgroupby%2Ftest_categorical.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fgroupby%2Ftest_categorical.py?ref=4c65d5fc79ea435bc4e47d8af2914cba324117bd",
                                    "patch": "@@ -67,6 +67,26 @@ def setUp(self):\n              'E': np.random.randn(11),\n              'F': np.random.randn(11)})\n \n+    def test_level_groupby_get_group(self):\n+        # GH15155\n+        df = DataFrame(data=np.arange(2, 22, 2),\n+                       index=MultiIndex(\n+                           levels=[pd.CategoricalIndex([\"a\", \"b\"]), range(10)],\n+                           labels=[[0] * 5 + [1] * 5, range(10)],\n+                           names=[\"Index1\", \"Index2\"]))\n+        g = df.groupby(level=[\"Index1\"])\n+\n+        # expected should equal test.loc[[\"a\"]]\n+        # GH15166\n+        expected = DataFrame(data=np.arange(2, 12, 2),\n+                             index=pd.MultiIndex(levels=[pd.CategoricalIndex(\n+                                 [\"a\", \"b\"]), range(5)],\n+            labels=[[0] * 5, range(5)],\n+            names=[\"Index1\", \"Index2\"]))\n+        result = g.get_group('a')\n+\n+        assert_frame_equal(result, expected)\n+\n     def test_apply_use_categorical_name(self):\n         from pandas import qcut\n         cats = qcut(self.df.C, 4)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/groupby/test_categorical.py "
            },
            {
                "id": 15055,
                "created_at": "2017-01-04T14:18:48Z",
                "closed_at": "2017-01-23T18:11:50Z",
                "title": ".str.replace does not accept a repl function",
                "labels": "Bug, Strings",
                "commits": [
                    {
                        "hash": "be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb",
                        "commit_date": "2017-01-23T18:06:42Z",
                        "parents": "a1b6587153ebf81b02272fc8a717d2b6c9d0c4aa",
                        "stat": {
                            "total": 8,
                            "additions": 129,
                            "deletions": 121,
                            "files": [
                                {
                                    "sha": "52e05c5d511bcf5f9162cd6b6612f612391a4087",
                                    "filename": "doc/source/text.rst",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 1,
                                    "changes": 21,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/doc%2Fsource%2Ftext.rst",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/doc%2Fsource%2Ftext.rst",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Ftext.rst?ref=be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb",
                                    "patch": "@@ -146,6 +146,25 @@ following code will cause trouble because of the regular expression meaning of\n    # We need to escape the special character (for >1 len patterns)\n    dollars.str.replace(r'-\\$', '-')\n \n+The ``replace`` method can also take a callable as replacement. It is called \n+on every ``pat`` using :func:`re.sub`. The callable should expect one \n+positional argument (a regex object) and return a string.\n+\n+.. versionadded:: 0.20.0\n+\n+.. ipython:: python\n+\n+   # Reverse every lowercase alphabetic word\n+   pat = r'[a-z]+'\n+   repl = lambda m: m.group(0)[::-1]\n+   pd.Series(['foo 123', 'bar baz', np.nan]).str.replace(pat, repl)\n+\n+   # Using regex groups\n+   pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n+   repl = lambda m: m.group('two').swapcase()\n+   pd.Series(['Foo Bar Baz', np.nan]).str.replace(pat, repl)\n+\n+\n Indexing with ``.str``\n ----------------------\n \n@@ -406,7 +425,7 @@ Method Summary\n     :meth:`~Series.str.join`;Join strings in each element of the Series with passed separator\n     :meth:`~Series.str.get_dummies`;Split strings on the delimiter returning DataFrame of dummy variables\n     :meth:`~Series.str.contains`;Return boolean array if each string contains pattern/regex\n-    :meth:`~Series.str.replace`;Replace occurrences of pattern/regex with some other string\n+    :meth:`~Series.str.replace`;Replace occurrences of pattern/regex with some other string or the return value of a callable given the occurrence\n     :meth:`~Series.str.repeat`;Duplicate values (``s.str.repeat(3)`` equivalent to ``x * 3``)\n     :meth:`~Series.str.pad`;\"Add whitespace to left, right, or both sides of strings\"\n     :meth:`~Series.str.center`;Equivalent to ``str.center``"
                                },
                                {
                                    "sha": "7e4fa44ea8dedb77a758b2b3dbdb8869c6a9676e",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb",
                                    "patch": "@@ -24,6 +24,7 @@ New features\n ~~~~~~~~~~~~\n \n - Integration with the ``feather-format``, including a new top-level ``pd.read_feather()`` and ``DataFrame.to_feather()`` method, see :ref:`here <io.feather>`.\n+- ``.str.replace`` now accepts a callable, as replacement, which is passed to ``re.sub`` (:issue:`15055`)\n \n \n "
                                },
                                {
                                    "sha": "c48defe39a01100f29a2da61e73367f7ea74fab8",
                                    "filename": "pandas/core/strings.py",
                                    "status": "modified",
                                    "additions": 63,
                                    "deletions": 7,
                                    "changes": 70,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/pandas%2Fcore%2Fstrings.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/pandas%2Fcore%2Fstrings.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fstrings.py?ref=be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb",
                                    "patch": "@@ -167,7 +167,17 @@ def _map(f, arr, na_mask=False, na_value=np.nan, dtype=object):\n         try:\n             convert = not all(mask)\n             result = lib.map_infer_mask(arr, f, mask.view(np.uint8), convert)\n-        except (TypeError, AttributeError):\n+        except (TypeError, AttributeError) as e:\n+            # Reraise the exception if callable `f` got wrong number of args.\n+            # The user may want to be warned by this, instead of getting NaN\n+            if compat.PY2:\n+                p_err = r'takes (no|(exactly|at (least|most)) ?\\d+) arguments?'\n+            else:\n+                p_err = (r'((takes)|(missing)) (?(2)from \\d+ to )?\\d+ '\n+                         r'(?(3)required )positional arguments?')\n+\n+            if len(e.args) >= 1 and re.search(p_err, e.args[0]):\n+                raise e\n \n             def g(x):\n                 try:\n@@ -303,8 +313,13 @@ def str_replace(arr, pat, repl, n=-1, case=True, flags=0):\n     ----------\n     pat : string\n         Character sequence or regular expression\n-    repl : string\n-        Replacement sequence\n+    repl : string or callable\n+        Replacement string or a callable. The callable is passed the regex\n+        match object and must return a replacement string to be used.\n+        See :func:`re.sub`.\n+\n+    .. versionadded:: 0.20.0\n+\n     n : int, default -1 (all)\n         Number of replacements to make from start\n     case : boolean, default True\n@@ -315,12 +330,53 @@ def str_replace(arr, pat, repl, n=-1, case=True, flags=0):\n     Returns\n     -------\n     replaced : Series/Index of objects\n+\n+    Examples\n+    --------\n+    When ``repl`` is a string, every ``pat`` is replaced as with\n+    :meth:`str.replace`. NaN value(s) in the Series are left as is.\n+\n+    >>> Series(['foo', 'fuz', np.nan]).str.replace('f', 'b')\n+    0    boo\n+    1    buz\n+    2    NaN\n+    dtype: object\n+\n+    When ``repl`` is a callable, it is called on every ``pat`` using\n+    :func:`re.sub`. The callable should expect one positional argument\n+    (a regex object) and return a string.\n+\n+    To get the idea:\n+\n+    >>> Series(['foo', 'fuz', np.nan]).str.replace('f', repr)\n+    0    <_sre.SRE_Match object; span=(0, 1), match='f'>oo\n+    1    <_sre.SRE_Match object; span=(0, 1), match='f'>uz\n+    2                                                  NaN\n+    dtype: object\n+\n+    Reverse every lowercase alphabetic word:\n+\n+    >>> repl = lambda m: m.group(0)[::-1]\n+    >>> Series(['foo 123', 'bar baz', np.nan]).str.replace(r'[a-z]+', repl)\n+    0    oof 123\n+    1    rab zab\n+    2        NaN\n+    dtype: object\n+\n+    Using regex groups:\n+\n+    >>> pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n+    >>> repl = lambda m: m.group('two').swapcase()\n+    >>> Series(['Foo Bar Baz', np.nan]).str.replace(pat, repl)\n+    0    bAR\n+    1    NaN\n+    dtype: object\n     \"\"\"\n \n-    # Check whether repl is valid (GH 13438)\n-    if not is_string_like(repl):\n-        raise TypeError(\"repl must be a string\")\n-    use_re = not case or len(pat) > 1 or flags\n+    # Check whether repl is valid (GH 13438, GH 15055)\n+    if not (is_string_like(repl) or callable(repl)):\n+        raise TypeError(\"repl must be a string or callable\")\n+    use_re = not case or len(pat) > 1 or flags or callable(repl)\n \n     if use_re:\n         if not case:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f59127c853ed117f42531175009bde11a5319b71",
                                    "filename": "pandas/tests/test_strings.py",
                                    "status": "modified",
                                    "additions": 37,
                                    "deletions": 0,
                                    "changes": 37,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/pandas%2Ftests%2Ftest_strings.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb/pandas%2Ftests%2Ftest_strings.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_strings.py?ref=be3f2aea4a8ad3f6bf4eb84e6ca3269abff5fcfb",
                                    "patch": "@@ -436,6 +436,43 @@ def test_replace(self):\n                     values = klass(data)\n                     self.assertRaises(TypeError, values.str.replace, 'a', repl)\n \n+    def test_replace_callable(self):\n+        # GH 15055\n+        values = Series(['fooBAD__barBAD', NA])\n+\n+        # test with callable\n+        repl = lambda m: m.group(0).swapcase()\n+        result = values.str.replace('[a-z][A-Z]{2}', repl, n=2)\n+        exp = Series(['foObaD__baRbaD', NA])\n+        tm.assert_series_equal(result, exp)\n+\n+        # test with wrong number of arguments, raising an error\n+        if compat.PY2:\n+            p_err = r'takes (no|(exactly|at (least|most)) ?\\d+) arguments?'\n+        else:\n+            p_err = (r'((takes)|(missing)) (?(2)from \\d+ to )?\\d+ '\n+                     r'(?(3)required )positional arguments?')\n+\n+        repl = lambda: None\n+        with tm.assertRaisesRegexp(TypeError, p_err):\n+            values.str.replace('a', repl)\n+\n+        repl = lambda m, x: None\n+        with tm.assertRaisesRegexp(TypeError, p_err):\n+            values.str.replace('a', repl)\n+\n+        repl = lambda m, x, y=None: None\n+        with tm.assertRaisesRegexp(TypeError, p_err):\n+            values.str.replace('a', repl)\n+\n+        # test regex named groups\n+        values = Series(['Foo Bar Baz', NA])\n+        pat = r\"(?P<first>\\w+) (?P<middle>\\w+) (?P<last>\\w+)\"\n+        repl = lambda m: m.group('middle').swapcase()\n+        result = values.str.replace(pat, repl)\n+        exp = Series(['bAR', NA])\n+        tm.assert_series_equal(result, exp)\n+\n     def test_repeat(self):\n         values = Series(['a', 'b', NA, 'c', NA, 'd'])\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_strings.py"
            },
            {
                "id": 15021,
                "created_at": "2016-12-31T10:14:32Z",
                "closed_at": "2017-02-07T13:59:45Z",
                "title": "ENH: enable 'on' keyword for groupby(..).resample()",
                "labels": "Bug, Resample",
                "commits": [
                    {
                        "hash": "6d2293f7399390800ec00b2cf78afa3b9043bef9",
                        "commit_date": "2017-02-07T13:57:51Z",
                        "parents": "34cdfa48881118a6327fe0e599fb41467ef6ffcc",
                        "stat": {
                            "total": 0,
                            "additions": 19,
                            "deletions": 19,
                            "files": [
                                {
                                    "sha": "3f6c06e20b5462015b9604bc7fcc30c52b349b5a",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6d2293f7399390800ec00b2cf78afa3b9043bef9/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6d2293f7399390800ec00b2cf78afa3b9043bef9/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=6d2293f7399390800ec00b2cf78afa3b9043bef9",
                                    "patch": "@@ -434,6 +434,7 @@ Bug Fixes\n - Bug in ``pd.read_csv()`` in which missing data was being improperly handled with ``usecols`` (:issue:`6710`)\n - Bug in ``pd.read_csv()`` in which a file containing a row with many columns followed by rows with fewer columns would cause a crash (:issue:`14125`)\n - Bug in ``pd.tools.hashing.hash_pandas_object()`` in which hashing of categoricals depended on the ordering of categories, instead of just their values. (:issue:`15143`)\n+- Bug in ``.groupby(..).resample()`` when passed the ``on=`` kwarg. (:issue:`15021`)\n \n - Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`)\n "
                                },
                                {
                                    "sha": "5692d6c5cabdee423291b746430f57b15a37b440",
                                    "filename": "pandas/tseries/resample.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6d2293f7399390800ec00b2cf78afa3b9043bef9/pandas%2Ftseries%2Fresample.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6d2293f7399390800ec00b2cf78afa3b9043bef9/pandas%2Ftseries%2Fresample.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Fresample.py?ref=6d2293f7399390800ec00b2cf78afa3b9043bef9",
                                    "patch": "@@ -975,6 +975,10 @@ def resample(obj, kind=None, **kwds):\n def get_resampler_for_grouping(groupby, rule, how=None, fill_method=None,\n                                limit=None, kind=None, **kwargs):\n     \"\"\" return our appropriate resampler when grouping as well \"\"\"\n+\n+    # .resample uses 'on' similar to how .groupby uses 'key'\n+    kwargs['key'] = kwargs.pop('on', None)\n+\n     tg = TimeGrouper(freq=rule, **kwargs)\n     resampler = tg._get_resampler(groupby.obj, kind=kind)\n     r = resampler._get_resampler_for_grouping(groupby=groupby)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c40f930fbd094b87774b22cf1064e2c32cebdafc",
                                    "filename": "pandas/tseries/tests/test_resample.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6d2293f7399390800ec00b2cf78afa3b9043bef9/pandas%2Ftseries%2Ftests%2Ftest_resample.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6d2293f7399390800ec00b2cf78afa3b9043bef9/pandas%2Ftseries%2Ftests%2Ftest_resample.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Ftests%2Ftest_resample.py?ref=6d2293f7399390800ec00b2cf78afa3b9043bef9",
                                    "patch": "@@ -217,6 +217,20 @@ def test_groupby_resample_api(self):\n             lambda x: x.resample('1D').ffill())[['val']]\n         assert_frame_equal(result, expected)\n \n+    def test_groupby_resample_on_api(self):\n+\n+        # GH 15021\n+        # .groupby(...).resample(on=...) results in an unexpected\n+        # keyword warning.\n+        df = pd.DataFrame({'key': ['A', 'B'] * 5,\n+                           'dates': pd.date_range('2016-01-01', periods=10),\n+                           'values': np.random.randn(10)})\n+\n+        expected = df.set_index('dates').groupby('key').resample('D').mean()\n+\n+        result = df.groupby('key').resample('D', on='dates').mean()\n+        assert_frame_equal(result, expected)\n+\n     def test_plot_api(self):\n         tm._skip_if_no_mpl()\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tseries/tests/test_resample.py"
            },
            {
                "id": 14973,
                "created_at": "2016-12-23T17:12:08Z",
                "closed_at": "2016-12-30T19:14:33Z",
                "title": "Using the python power operator on numerical Index objects yields unexpected results",
                "labels": "Bug, Numeric Operations",
                "commits": [
                    {
                        "hash": "298b241be32365e3e2c8d53fbad098a05de044c9",
                        "commit_date": "2016-12-30T19:13:48Z",
                        "parents": "4a4635d5825a42b8a82483ec000abf4a63967a1e",
                        "stat": {
                            "total": 1,
                            "additions": 14,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "f41dc3edaabf47580259b1d3c3894b8edd6c0550",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/298b241be32365e3e2c8d53fbad098a05de044c9/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/298b241be32365e3e2c8d53fbad098a05de044c9/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=298b241be32365e3e2c8d53fbad098a05de044c9",
                                    "patch": "@@ -283,6 +283,7 @@ Performance Improvements\n Bug Fixes\n ~~~~~~~~~\n \n+- Bug in ``Index`` power operations with reversed operands (:issue:`14973`)\n - Bug in ``TimedeltaIndex`` addition where overflow was being allowed without error (:issue:`14816`)\n - Bug in ``DataFrame`` construction in which unsigned 64-bit integer elements were being converted to objects (:issue:`14881`)\n - Bug in ``astype()`` where ``inf`` values were incorrectly converted to integers. Now raises error now with ``astype()`` for Series and DataFrames (:issue:`14265`)"
                                },
                                {
                                    "sha": "b87fb5dc8478257895d920a8014ac4dc48389945",
                                    "filename": "pandas/indexes/base.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/298b241be32365e3e2c8d53fbad098a05de044c9/pandas%2Findexes%2Fbase.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/298b241be32365e3e2c8d53fbad098a05de044c9/pandas%2Findexes%2Fbase.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fbase.py?ref=298b241be32365e3e2c8d53fbad098a05de044c9",
                                    "patch": "@@ -3543,7 +3543,9 @@ def _evaluate_numeric_binop(self, other):\n             operator.sub, '__sub__', reversed=True)\n         cls.__mul__ = cls.__rmul__ = _make_evaluate_binop(\n             operator.mul, '__mul__')\n-        cls.__pow__ = cls.__rpow__ = _make_evaluate_binop(\n+        cls.__rpow__ = _make_evaluate_binop(\n+            operator.pow, '__pow__', reversed=True)\n+        cls.__pow__ = _make_evaluate_binop(\n             operator.pow, '__pow__')\n         cls.__mod__ = _make_evaluate_binop(\n             operator.mod, '__mod__')"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f7f072d5b5d2a5fb653b79b1ff492daca47427d4",
                                    "filename": "pandas/tests/indexes/test_numeric.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/298b241be32365e3e2c8d53fbad098a05de044c9/pandas%2Ftests%2Findexes%2Ftest_numeric.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/298b241be32365e3e2c8d53fbad098a05de044c9/pandas%2Ftests%2Findexes%2Ftest_numeric.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexes%2Ftest_numeric.py?ref=298b241be32365e3e2c8d53fbad098a05de044c9",
                                    "patch": "@@ -105,6 +105,15 @@ def test_numeric_compat(self):\n         for r, e in zip(result, expected):\n             tm.assert_index_equal(r, e)\n \n+        # test power calculations both ways, GH 14973\n+        expected = pd.Float64Index(2.0**idx.values)\n+        result = 2.0**idx\n+        tm.assert_index_equal(result, expected)\n+\n+        expected = pd.Float64Index(idx.values**2.0)\n+        result = idx**2.0\n+        tm.assert_index_equal(result, expected)\n+\n     def test_explicit_conversions(self):\n \n         # GH 8608"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexes/test_numeric.py"
            },
            {
                "id": 14922,
                "created_at": "2016-12-19T15:37:14Z",
                "closed_at": "2016-12-23T21:04:14Z",
                "title": "BUG: sorting with large float and multiple columns incorrect",
                "labels": "Bug, Reshaping",
                "commits": [
                    {
                        "hash": "6bea8275e504a594ac4fee71b5c941fb520c8b1a",
                        "commit_date": "2016-12-23T21:03:47Z",
                        "parents": "97b4295add096fb207faf4c8ac413a3244f5fffd",
                        "stat": {
                            "total": 2,
                            "additions": 52,
                            "deletions": 50,
                            "files": [
                                {
                                    "sha": "4799d2711231b2cd6e0bda3e6a14d6b9dc8c2f42",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6bea8275e504a594ac4fee71b5c941fb520c8b1a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6bea8275e504a594ac4fee71b5c941fb520c8b1a/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=6bea8275e504a594ac4fee71b5c941fb520c8b1a",
                                    "patch": "@@ -283,6 +283,7 @@ Bug Fixes\n - Bug in ``astype()`` where ``inf`` values were incorrectly converted to integers. Now raises error now with ``astype()`` for Series and DataFrames (:issue:`14265`)\n - Bug in ``DataFrame(..).apply(to_numeric)`` when values are of type decimal.Decimal. (:issue:`14827`)\n - Bug in ``describe()`` when passing a numpy array which does not contain the median to the ``percentiles`` keyword argument (:issue:`14908`)\n+- Bug in ``DataFrame.sort_values()`` when sorting by multiple columns where one column is of type ``int64`` and contains ``NaT`` (:issue:`14922`)\n \n \n "
                                },
                                {
                                    "sha": "9b7bf2bf058ef989fee2a968e5fa7dd89aab34bb",
                                    "filename": "pandas/core/algorithms.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6bea8275e504a594ac4fee71b5c941fb520c8b1a/pandas%2Fcore%2Falgorithms.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6bea8275e504a594ac4fee71b5c941fb520c8b1a/pandas%2Fcore%2Falgorithms.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Falgorithms.py?ref=6bea8275e504a594ac4fee71b5c941fb520c8b1a",
                                    "patch": "@@ -349,7 +349,8 @@ def factorize(values, sort=False, order=None, na_sentinel=-1, size_hint=None):\n \n     table = hash_klass(size_hint or len(vals))\n     uniques = vec_klass()\n-    labels = table.get_labels(vals, uniques, 0, na_sentinel, True)\n+    check_nulls = not is_integer_dtype(values)\n+    labels = table.get_labels(vals, uniques, 0, na_sentinel, check_nulls)\n \n     labels = _ensure_platform_int(labels)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "579a4bf5d54d54c2511e54ff5dfe88f2f25c3591",
                                    "filename": "pandas/tests/frame/test_sorting.py",
                                    "status": "modified",
                                    "additions": 47,
                                    "deletions": 1,
                                    "changes": 48,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6bea8275e504a594ac4fee71b5c941fb520c8b1a/pandas%2Ftests%2Fframe%2Ftest_sorting.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6bea8275e504a594ac4fee71b5c941fb520c8b1a/pandas%2Ftests%2Fframe%2Ftest_sorting.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_sorting.py?ref=6bea8275e504a594ac4fee71b5c941fb520c8b1a",
                                    "patch": "@@ -6,7 +6,7 @@\n \n from pandas.compat import lrange\n from pandas import (DataFrame, Series, MultiIndex, Timestamp,\n-                    date_range)\n+                    date_range, NaT)\n \n from pandas.util.testing import (assert_series_equal,\n                                  assert_frame_equal,\n@@ -491,3 +491,49 @@ def test_frame_column_inplace_sort_exception(self):\n \n         cp = s.copy()\n         cp.sort_values()  # it works!\n+\n+    def test_sort_nat_values_in_int_column(self):\n+\n+        # GH 14922: \"sorting with large float and multiple columns incorrect\"\n+\n+        # cause was that the int64 value NaT was considered as \"na\". Which is\n+        # only correct for datetime64 columns.\n+\n+        int_values = (2, int(NaT))\n+        float_values = (2.0, -1.797693e308)\n+\n+        df = DataFrame(dict(int=int_values, float=float_values),\n+                       columns=[\"int\", \"float\"])\n+\n+        df_reversed = DataFrame(dict(int=int_values[::-1],\n+                                     float=float_values[::-1]),\n+                                columns=[\"int\", \"float\"],\n+                                index=[1, 0])\n+\n+        # NaT is not a \"na\" for int64 columns, so na_position must not\n+        # influence the result:\n+        df_sorted = df.sort_values([\"int\", \"float\"], na_position=\"last\")\n+        assert_frame_equal(df_sorted, df_reversed)\n+\n+        df_sorted = df.sort_values([\"int\", \"float\"], na_position=\"first\")\n+        assert_frame_equal(df_sorted, df_reversed)\n+\n+        # reverse sorting order\n+        df_sorted = df.sort_values([\"int\", \"float\"], ascending=False)\n+        assert_frame_equal(df_sorted, df)\n+\n+        # and now check if NaT is still considered as \"na\" for datetime64\n+        # columns:\n+        df = DataFrame(dict(datetime=[Timestamp(\"2016-01-01\"), NaT],\n+                            float=float_values), columns=[\"datetime\", \"float\"])\n+\n+        df_reversed = DataFrame(dict(datetime=[NaT, Timestamp(\"2016-01-01\")],\n+                                     float=float_values[::-1]),\n+                                columns=[\"datetime\", \"float\"],\n+                                index=[1, 0])\n+\n+        df_sorted = df.sort_values([\"datetime\", \"float\"], na_position=\"first\")\n+        assert_frame_equal(df_sorted, df_reversed)\n+\n+        df_sorted = df.sort_values([\"datetime\", \"float\"], na_position=\"last\")\n+        assert_frame_equal(df_sorted, df_reversed)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_sorting.py"
            },
            {
                "id": 14776,
                "created_at": "2016-11-30T21:56:38Z",
                "closed_at": "2016-12-04T17:34:59Z",
                "title": "BUG: group.apply with non-lexsorted levels and sort=True",
                "labels": "Bug, Groupby, MultiIndex",
                "commits": [
                    {
                        "hash": "f23010aa930e4301a6e70efce92ed1afc50dfaaa",
                        "commit_date": "2016-12-04T17:34:14Z",
                        "parents": "27fcd811f5b5df89eeede049cd048d94a65e7ff4",
                        "stat": {
                            "total": 2,
                            "additions": 33,
                            "deletions": 31,
                            "files": [
                                {
                                    "sha": "d365a3f30db25b50878c9d68d14ebc50e3798be6",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f23010aa930e4301a6e70efce92ed1afc50dfaaa/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f23010aa930e4301a6e70efce92ed1afc50dfaaa/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=f23010aa930e4301a6e70efce92ed1afc50dfaaa",
                                    "patch": "@@ -36,7 +36,7 @@ Bug Fixes\n - Bug in ``pd.read_csv`` for the Python engine in which an unhelpful error message was being raised when ``skipfooter`` was not being respected by Python's CSV library (:issue:`13879`)\n \n \n-\n+- Bug in ``.groupby(..., sort=True)`` of a non-lexsorted MultiIndex when grouping with multiple levels (:issue:`14776`)\n \n \n "
                                },
                                {
                                    "sha": "f449e166861904d9486ad3a6266acb4b44face45",
                                    "filename": "pandas/core/groupby.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 1,
                                    "changes": 12,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f23010aa930e4301a6e70efce92ed1afc50dfaaa/pandas%2Fcore%2Fgroupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f23010aa930e4301a6e70efce92ed1afc50dfaaa/pandas%2Fcore%2Fgroupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fgroupby.py?ref=f23010aa930e4301a6e70efce92ed1afc50dfaaa",
                                    "patch": "@@ -861,7 +861,17 @@ def reset_identity(values):\n             if isinstance(result, Series):\n                 result = result.reindex(ax)\n             else:\n-                result = result.reindex_axis(ax, axis=self.axis)\n+\n+                # this is a very unfortunate situation\n+                # we have a multi-index that is NOT lexsorted\n+                # and we have a result which is duplicated\n+                # we can't reindex, so we resort to this\n+                # GH 14776\n+                if isinstance(ax, MultiIndex) and not ax.is_unique:\n+                    result = result.take(result.index.get_indexer_for(\n+                        ax.values).unique(), axis=self.axis)\n+                else:\n+                    result = result.reindex_axis(ax, axis=self.axis)\n \n         elif self.group_keys:\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "37499e09d6dc6f88cd16a8764127f479fc36ff39",
                                    "filename": "pandas/tests/test_groupby.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 0,
                                    "changes": 19,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f23010aa930e4301a6e70efce92ed1afc50dfaaa/pandas%2Ftests%2Ftest_groupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f23010aa930e4301a6e70efce92ed1afc50dfaaa/pandas%2Ftests%2Ftest_groupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_groupby.py?ref=f23010aa930e4301a6e70efce92ed1afc50dfaaa",
                                    "patch": "@@ -4736,6 +4736,25 @@ def test_groupby_multiindex_not_lexsorted(self):\n             result = not_lexsorted_df.groupby('a').mean()\n         tm.assert_frame_equal(expected, result)\n \n+        # a transforming function should work regardless of sort\n+        # GH 14776\n+        df = DataFrame({'x': ['a', 'a', 'b', 'a'],\n+                        'y': [1, 1, 2, 2],\n+                        'z': [1, 2, 3, 4]}).set_index(['x', 'y'])\n+        self.assertFalse(df.index.is_lexsorted())\n+\n+        for level in [0, 1, [0, 1]]:\n+            for sort in [False, True]:\n+                result = df.groupby(level=level, sort=sort).apply(\n+                    DataFrame.drop_duplicates)\n+                expected = df\n+                tm.assert_frame_equal(expected, result)\n+\n+                result = df.sort_index().groupby(level=level, sort=sort).apply(\n+                    DataFrame.drop_duplicates)\n+                expected = df.sort_index()\n+                tm.assert_frame_equal(expected, result)\n+\n     def test_groupby_levels_and_columns(self):\n         # GH9344, GH9049\n         idx_names = ['x', 'y']"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_groupby.py"
            },
            {
                "id": 14770,
                "created_at": "2016-11-30T06:45:08Z",
                "closed_at": "2016-12-04T17:42:07Z",
                "title": "BUG: Error upon Series.Groupby.nunique with empty Series (#12553)",
                "labels": "Bug, Groupby",
                "commits": [
                    {
                        "hash": "c0e13d1bccd4a783486eba8cc769db48a7875de8",
                        "commit_date": "2016-12-04T17:37:45Z",
                        "parents": "f23010aa930e4301a6e70efce92ed1afc50dfaaa",
                        "stat": {
                            "total": 1,
                            "additions": 15,
                            "deletions": 14,
                            "files": [
                                {
                                    "sha": "fe900d0480d018cbb495dbf8d445e6c1013f113a",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c0e13d1bccd4a783486eba8cc769db48a7875de8/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c0e13d1bccd4a783486eba8cc769db48a7875de8/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=c0e13d1bccd4a783486eba8cc769db48a7875de8",
                                    "patch": "@@ -59,6 +59,8 @@ Bug Fixes\n \n \n - Bug ``HDFStore`` writing a ``MultiIndex`` when using ``data_columns=True`` (:issue:`14435`)\n+- Bug in ``Series.groupby.nunique()`` raising an ``IndexError`` for an empty ``Series`` (:issue:`12553`)\n+\n \n \n - Bug in clipboard functions on linux with python2 with unicode and separators (:issue:`13747`)"
                                },
                                {
                                    "sha": "66c9e38766989b9cd7c17549a86c8fd7627494bd",
                                    "filename": "pandas/core/groupby.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c0e13d1bccd4a783486eba8cc769db48a7875de8/pandas%2Fcore%2Fgroupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c0e13d1bccd4a783486eba8cc769db48a7875de8/pandas%2Fcore%2Fgroupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fgroupby.py?ref=c0e13d1bccd4a783486eba8cc769db48a7875de8",
                                    "patch": "@@ -2908,6 +2908,7 @@ def true_and_notnull(x, *args, **kwargs):\n     def nunique(self, dropna=True):\n         \"\"\" Returns number of unique elements in the group \"\"\"\n         ids, _, _ = self.grouper.group_info\n+\n         val = self.obj.get_values()\n \n         try:\n@@ -2938,7 +2939,10 @@ def nunique(self, dropna=True):\n             inc[idx] = 1\n \n         out = np.add.reduceat(inc, idx).astype('int64', copy=False)\n-        res = out if ids[0] != -1 else out[1:]\n+        if len(ids):\n+            res = out if ids[0] != -1 else out[1:]\n+        else:\n+            res = out[1:]\n         ri = self.grouper.result_index\n \n         # we might have duplications among the bins"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a2e1c5e9ff2e87fd3935fbc62fe7b8e70bed9cc6",
                                    "filename": "pandas/tests/test_groupby.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 0,
                                    "changes": 7,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/c0e13d1bccd4a783486eba8cc769db48a7875de8/pandas%2Ftests%2Ftest_groupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/c0e13d1bccd4a783486eba8cc769db48a7875de8/pandas%2Ftests%2Ftest_groupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_groupby.py?ref=c0e13d1bccd4a783486eba8cc769db48a7875de8",
                                    "patch": "@@ -6773,6 +6773,13 @@ def test_nunique_with_object(self):\n         expected = pd.Series([1] * 5, name='name', index=index)\n         tm.assert_series_equal(result, expected)\n \n+    def test_nunique_with_empty_series(self):\n+        # GH 12553\n+        data = pd.Series(name='name')\n+        result = data.groupby(level=0).nunique()\n+        expected = pd.Series(name='name', dtype='int64')\n+        tm.assert_series_equal(result, expected)\n+\n     def test_transform_with_non_scalar_group(self):\n         # GH 10165\n         cols = pd.MultiIndex.from_tuples(["
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_groupby.py"
            },
            {
                "id": 14730,
                "created_at": "2016-11-24T15:47:13Z",
                "closed_at": "2017-01-04T00:00:09Z",
                "title": "Indexing a mulitIndex with a series fails",
                "labels": "Bug, MultiIndex",
                "commits": [
                    {
                        "hash": "6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af",
                        "commit_date": "2017-01-03T23:59:38Z",
                        "parents": "b957f6fd54c32c962f228d1d19548509e1531968",
                        "stat": {
                            "total": 13,
                            "additions": 54,
                            "deletions": 41,
                            "files": [
                                {
                                    "sha": "2db03724e564d3badc23b2b69c97cee67605b9d5",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af",
                                    "patch": "@@ -299,7 +299,7 @@ Bug Fixes\n - Bug in ``pd.to_numeric()`` in which float and unsigned integer elements were being improperly casted (:issue:`14941`, :issue:`15005`)\n - Bug in ``pd.read_csv()`` in which the ``dialect`` parameter was not being verified before processing (:issue:`14898`)\n \n-\n+- Bug in ``DataFrame.loc`` with indexing a ``MultiIndex`` with a ``Series`` indexer (:issue:`14730`)\n \n - Bug in ``pd.read_msgpack()`` in which ``Series`` categoricals were being improperly processed (:issue:`14901`)\n - Bug in ``Series.ffill()`` with mixed dtypes containing tz-aware datetimes. (:issue:`14956`)"
                                },
                                {
                                    "sha": "9fa5b67083b2df2daec16a2d310283436c5298ab",
                                    "filename": "pandas/core/indexing.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Fcore%2Findexing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Fcore%2Findexing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Findexing.py?ref=6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af",
                                    "patch": "@@ -1461,6 +1461,9 @@ def _getitem_axis(self, key, axis=0):\n             if isinstance(labels, MultiIndex):\n                 if (not isinstance(key, tuple) and len(key) > 1 and\n                         not isinstance(key[0], tuple)):\n+                    if isinstance(key, ABCSeries):\n+                        # GH 14730\n+                        key = list(key)\n                     key = tuple([key])\n \n             # an iterable multi-selection"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7da3cb377e63d985b221bff22a293f088942dcdb",
                                    "filename": "pandas/tests/indexes/test_multi.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 12,
                                    "changes": 27,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Ftests%2Findexes%2Ftest_multi.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Ftests%2Findexes%2Ftest_multi.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexes%2Ftest_multi.py?ref=6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af",
                                    "patch": "@@ -1,26 +1,29 @@\n # -*- coding: utf-8 -*-\n \n-from datetime import timedelta\n-from itertools import product\n-import nose\n import re\n import warnings\n \n-from pandas import (DataFrame, date_range, period_range, MultiIndex, Index,\n-                    CategoricalIndex, compat)\n-from pandas.core.common import PerformanceWarning, UnsortedIndexError\n-from pandas.indexes.base import InvalidIndexError\n-from pandas.compat import range, lrange, u, PY3, long, lzip\n+from datetime import timedelta\n+from itertools import product\n+\n+import nose\n \n import numpy as np\n \n-from pandas.util.testing import (assert_almost_equal, assertRaises,\n-                                 assertRaisesRegexp, assert_copy)\n+import pandas as pd\n+\n+from pandas import (CategoricalIndex, DataFrame, Index, MultiIndex,\n+                    compat, date_range, period_range)\n+from pandas.compat import PY3, long, lrange, lzip, range, u\n+from pandas.core.common import PerformanceWarning, UnsortedIndexError\n+from pandas.indexes.base import InvalidIndexError\n+from pandas.lib import Timestamp\n \n import pandas.util.testing as tm\n \n-import pandas as pd\n-from pandas.lib import Timestamp\n+from pandas.util.testing import (assertRaises, assertRaisesRegexp,\n+                                 assert_almost_equal, assert_copy)\n+\n \n from .common import Base\n "
                                },
                                {
                                    "sha": "4e5558309bad5d9477786fe2b9467609dd7697ae",
                                    "filename": "pandas/tests/indexing/test_indexing.py",
                                    "status": "modified",
                                    "additions": 22,
                                    "deletions": 0,
                                    "changes": 22,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Ftests%2Findexing%2Ftest_indexing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af/pandas%2Ftests%2Findexing%2Ftest_indexing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexing%2Ftest_indexing.py?ref=6eb705f5350a0eeffd9e7663d7b4bf54acb4d6af",
                                    "patch": "@@ -1210,6 +1210,28 @@ def test_loc_getitem_label_array_like(self):\n         self.check_result('array like', 'loc', Series(index=[4, 8, 12]).index,\n                           'ix', [4, 8, 12], typs=['ints'], axes=2)\n \n+    def test_loc_getitem_series(self):\n+        # GH14730\n+        # passing a series as a key with a MultiIndex\n+        index = MultiIndex.from_product([[1, 2, 3], ['A', 'B', 'C']])\n+        x = Series(index=index, data=range(9), dtype=np.float64)\n+        y = Series([1, 3])\n+        expected = Series(\n+            data=[0, 1, 2, 6, 7, 8],\n+            index=MultiIndex.from_product([[1, 3], ['A', 'B', 'C']]),\n+            dtype=np.float64)\n+        result = x.loc[y]\n+        tm.assert_series_equal(result, expected)\n+\n+        result = x.loc[[1, 3]]\n+        tm.assert_series_equal(result, expected)\n+\n+        empty = Series(data=[], dtype=np.float64)\n+        expected = Series([], index=MultiIndex(\n+            levels=index.levels, labels=[[], []], dtype=np.float64))\n+        result = x.loc[empty]\n+        tm.assert_series_equal(result, expected)\n+\n     def test_loc_getitem_bool(self):\n         # boolean indexers\n         b = [True, False, True, False]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexing/test_indexing.py"
            },
            {
                "id": 14721,
                "created_at": "2016-11-23T16:03:30Z",
                "closed_at": "2016-12-20T13:46:00Z",
                "title": "Series.unique converts uint64 to int64 (with overflow)",
                "labels": "Bug, Dtype Conversions",
                "commits": [
                    {
                        "hash": "b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                        "commit_date": "2016-12-20T13:43:31Z",
                        "parents": "5faf32a62b52912bc0c4a26622bfc3d72b5121ff",
                        "stat": {
                            "total": 7,
                            "additions": 66,
                            "deletions": 59,
                            "files": [
                                {
                                    "sha": "dd790e89cbc08d0932271704adc3166fd9db2838",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -259,4 +259,6 @@ Bug Fixes\n \n \n \n+\n+- Bug in ``Series.unique()`` in which unsigned 64-bit integers were causing overflow (:issue:`14721`)\n - Require at least 0.23 version of cython to avoid problems with character encodings (:issue:`14699`)"
                                },
                                {
                                    "sha": "e51774ce4d9b4b68e76389b8855c756ec81dbccc",
                                    "filename": "pandas/core/algorithms.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fcore%2Falgorithms.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fcore%2Falgorithms.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Falgorithms.py?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -25,6 +25,7 @@\n                                  _ensure_platform_int,\n                                  _ensure_object,\n                                  _ensure_float64,\n+                                 _ensure_uint64,\n                                  _ensure_int64,\n                                  is_list_like)\n from pandas.compat.numpy import _np_version_under1p10\n@@ -129,9 +130,12 @@ def unique1d(values):\n         table = htable.Int64HashTable(len(values))\n         uniques = table.unique(_ensure_int64(values))\n         uniques = uniques.view('m8[ns]')\n-    elif np.issubdtype(values.dtype, np.integer):\n+    elif np.issubdtype(values.dtype, np.signedinteger):\n         table = htable.Int64HashTable(len(values))\n         uniques = table.unique(_ensure_int64(values))\n+    elif np.issubdtype(values.dtype, np.unsignedinteger):\n+        table = htable.UInt64HashTable(len(values))\n+        uniques = table.unique(_ensure_uint64(values))\n     else:\n \n         # its cheaper to use a String Hash Table than Object"
                                },
                                {
                                    "sha": "cd06b938310a87a08ebee58a92b6dc2dee757efa",
                                    "filename": "pandas/hashtable.pxd",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 1,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fhashtable.pxd",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fhashtable.pxd",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fhashtable.pxd?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -1,10 +1,17 @@\n-from khash cimport kh_int64_t, kh_float64_t, kh_pymap_t, kh_str_t, int64_t, float64_t\n+from khash cimport (kh_int64_t, kh_uint64_t, kh_float64_t, kh_pymap_t,\n+                    kh_str_t, uint64_t, int64_t, float64_t)\n \n # prototypes for sharing\n \n cdef class HashTable:\n     pass\n \n+cdef class UInt64HashTable(HashTable):\n+    cdef kh_uint64_t *table\n+\n+    cpdef get_item(self, uint64_t val)\n+    cpdef set_item(self, uint64_t key, Py_ssize_t val)\n+\n cdef class Int64HashTable(HashTable):\n     cdef kh_int64_t *table\n "
                                },
                                {
                                    "sha": "c1c190704b4c7fbb2e02afe230637e34cabab476",
                                    "filename": "pandas/src/algos_common_helper.pxi.in",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Falgos_common_helper.pxi.in",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Falgos_common_helper.pxi.in",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Falgos_common_helper.pxi.in?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -553,6 +553,7 @@ dtypes = [('float64', 'FLOAT64', 'float64'),\n           ('int16', 'INT16', 'int16'),\n           ('int32', 'INT32', 'int32'),\n           ('int64', 'INT64', 'int64'),\n+          ('uint64', 'UINT64', 'uint64'),\n           # ('platform_int', 'INT', 'int_'),\n           # ('object', 'OBJECT', 'object_'),\n ]"
                                },
                                {
                                    "sha": "55c840b20c78b50e11218bfe64cf0de44b0695aa",
                                    "filename": "pandas/src/hashtable_class_helper.pxi.in",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fhashtable_class_helper.pxi.in",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fhashtable_class_helper.pxi.in",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Fhashtable_class_helper.pxi.in?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -17,7 +17,8 @@ WARNING: DO NOT edit .pxi FILE directly, .pxi is generated from .pxi.in\n \n dtypes = [('Float64', 'float64', 'float64_t'),\n           ('Int64', 'int64', 'int64_t'),\n-          ('String', 'string', 'char *')]\n+          ('String', 'string', 'char *'),\n+          ('UInt64', 'uint64', 'uint64_t')]\n }}\n \n {{for name, dtype, arg in dtypes}}\n@@ -40,6 +41,7 @@ cdef inline void append_data_{{dtype}}({{name}}VectorData *data,\n \n ctypedef fused vector_data:\n     Int64VectorData\n+    UInt64VectorData\n     Float64VectorData\n     StringVectorData\n \n@@ -54,6 +56,7 @@ cdef inline bint needs_resize(vector_data *data) nogil:\n \n # name, dtype, arg, idtype\n dtypes = [('Float64', 'float64', 'float64_t', 'np.float64'),\n+          ('UInt64', 'uint64', 'uint64_t', 'np.uint64'),\n           ('Int64', 'int64', 'int64_t', 'np.int64')]\n \n }}\n@@ -201,6 +204,7 @@ cdef class HashTable:\n \n # name, dtype, null_condition, float_group\n dtypes = [('Float64', 'float64', 'val != val', True),\n+          ('UInt64', 'uint64', 'val == 0', False),\n           ('Int64', 'int64', 'val == iNaT', False)]\n \n }}"
                                },
                                {
                                    "sha": "f3e16cfd32963ea5550ec719c16b14670185c318",
                                    "filename": "pandas/src/hashtable_func_helper.pxi.in",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fhashtable_func_helper.pxi.in",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fhashtable_func_helper.pxi.in",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Fhashtable_func_helper.pxi.in?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -11,7 +11,7 @@ WARNING: DO NOT edit .pxi FILE directly, .pxi is generated from .pxi.in\n {{py:\n \n # name\n-dtypes = ['float64', 'int64']\n+dtypes = ['float64', 'int64', 'uint64']\n \n }}\n "
                                },
                                {
                                    "sha": "adb0fe285dbb8d374be4609ac3e0e15cea063997",
                                    "filename": "pandas/src/khash.pxd",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 2,
                                    "changes": 21,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fkhash.pxd",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fkhash.pxd",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Fkhash.pxd?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -1,5 +1,5 @@\n from cpython cimport PyObject\n-from numpy cimport int64_t, int32_t, uint32_t, float64_t\n+from numpy cimport int64_t, uint64_t, int32_t, uint32_t, float64_t\n \n cdef extern from \"khash_python.h\":\n     ctypedef uint32_t khint_t\n@@ -55,7 +55,6 @@ cdef extern from \"khash_python.h\":\n \n     bint kh_exist_str(kh_str_t*, khiter_t) nogil\n \n-\n     ctypedef struct kh_int64_t:\n         khint_t n_buckets, size, n_occupied, upper_bound\n         uint32_t *flags\n@@ -72,6 +71,24 @@ cdef extern from \"khash_python.h\":\n \n     bint kh_exist_int64(kh_int64_t*, khiter_t) nogil\n \n+    ctypedef uint64_t khuint64_t\n+\n+    ctypedef struct kh_uint64_t:\n+        khint_t n_buckets, size, n_occupied, upper_bound\n+        uint32_t *flags\n+        khuint64_t *keys\n+        size_t *vals\n+\n+    inline kh_uint64_t* kh_init_uint64() nogil\n+    inline void kh_destroy_uint64(kh_uint64_t*) nogil\n+    inline void kh_clear_uint64(kh_uint64_t*) nogil\n+    inline khint_t kh_get_uint64(kh_uint64_t*, int64_t) nogil\n+    inline void kh_resize_uint64(kh_uint64_t*, khint_t) nogil\n+    inline khint_t kh_put_uint64(kh_uint64_t*, int64_t, int*) nogil\n+    inline void kh_del_uint64(kh_uint64_t*, khint_t) nogil\n+\n+    bint kh_exist_uint64(kh_uint64_t*, khiter_t) nogil\n+\n     ctypedef struct kh_float64_t:\n         khint_t n_buckets, size, n_occupied, upper_bound\n         uint32_t *flags"
                                },
                                {
                                    "sha": "869607a44c00131a512ff28408487dad28604821",
                                    "filename": "pandas/src/klib/khash.h",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fklib%2Fkhash.h",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Fsrc%2Fklib%2Fkhash.h",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Fklib%2Fkhash.h?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -567,12 +567,14 @@ typedef const char *kh_cstr_t;\n \n #define kh_exist_str(h, k) (kh_exist(h, k))\n #define kh_exist_float64(h, k) (kh_exist(h, k))\n+#define kh_exist_uint64(h, k) (kh_exist(h, k))\n #define kh_exist_int64(h, k) (kh_exist(h, k))\n #define kh_exist_int32(h, k) (kh_exist(h, k))\n \n KHASH_MAP_INIT_STR(str, size_t)\n KHASH_MAP_INIT_INT(int32, size_t)\n KHASH_MAP_INIT_INT64(int64, size_t)\n+KHASH_MAP_INIT_UINT64(uint64, size_t)\n \n \n #endif /* __AC_KHASH_H */"
                                },
                                {
                                    "sha": "06c8ef6e35cd7e452ffa92396a471352dd9f640e",
                                    "filename": "pandas/types/common.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Ftypes%2Fcommon.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Ftypes%2Fcommon.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftypes%2Fcommon.py?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -32,6 +32,8 @@ def _ensure_float(arr):\n         arr = arr.astype(float)\n     return arr\n \n+\n+_ensure_uint64 = algos.ensure_uint64\n _ensure_int64 = algos.ensure_int64\n _ensure_int32 = algos.ensure_int32\n _ensure_int16 = algos.ensure_int16"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7f1745edbb816c908831867ec462482f864504fa",
                                    "filename": "pandas/tests/test_algos.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 1,
                                    "changes": 15,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Ftests%2Ftest_algos.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b35c68996d4dfcd565dfbd7e27b53b392efe14cf/pandas%2Ftests%2Ftest_algos.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_algos.py?ref=b35c68996d4dfcd565dfbd7e27b53b392efe14cf",
                                    "patch": "@@ -899,6 +899,18 @@ def test_lookup_nan(self):\n         self.assert_numpy_array_equal(m.lookup(xs),\n                                       np.arange(len(xs), dtype=np.int64))\n \n+    def test_lookup_overflow(self):\n+        xs = np.array([1, 2, 2**63], dtype=np.uint64)\n+        m = hashtable.UInt64HashTable()\n+        m.map_locations(xs)\n+        self.assert_numpy_array_equal(m.lookup(xs),\n+                                      np.arange(len(xs), dtype=np.int64))\n+\n+    def test_get_unique(self):\n+        s = pd.Series([1, 2, 2**63, 2**63], dtype=np.uint64)\n+        exp = np.array([1, 2, 2**63], dtype=np.uint64)\n+        self.assert_numpy_array_equal(s.unique(), exp)\n+\n     def test_vector_resize(self):\n         # Test for memory errors after internal vector\n         # reallocations (pull request #7157)\n@@ -915,7 +927,8 @@ def _test_vector_resize(htable, uniques, dtype, nvals):\n             (hashtable.PyObjectHashTable, hashtable.ObjectVector, 'object'),\n             (hashtable.StringHashTable, hashtable.ObjectVector, 'object'),\n             (hashtable.Float64HashTable, hashtable.Float64Vector, 'float64'),\n-            (hashtable.Int64HashTable, hashtable.Int64Vector, 'int64')]\n+            (hashtable.Int64HashTable, hashtable.Int64Vector, 'int64'),\n+            (hashtable.UInt64HashTable, hashtable.UInt64Vector, 'uint64')]\n \n         for (tbl, vect, dtype) in test_cases:\n             # resizing to empty is a special case"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_algos.py"
            },
            {
                "id": 14707,
                "created_at": "2016-11-22T04:26:16Z",
                "closed_at": "2016-12-06T11:35:03Z",
                "title": "BUG: _nsorted incorrect with duplicated values in index (#13412)",
                "labels": "Bug, Reshaping",
                "commits": [
                    {
                        "hash": "6e514dacc131f044deb74f0a562a51ef3b1201eb",
                        "commit_date": "2016-12-06T11:33:35Z",
                        "parents": "4378f82967f59097055eef17ede50aa515525551",
                        "stat": {
                            "total": 15,
                            "additions": 97,
                            "deletions": 82,
                            "files": [
                                {
                                    "sha": "df73a474b26837daa047a26e9e6444290b397302",
                                    "filename": "asv_bench/benchmarks/frame_methods.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/asv_bench%2Fbenchmarks%2Fframe_methods.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/asv_bench%2Fbenchmarks%2Fframe_methods.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/asv_bench%2Fbenchmarks%2Fframe_methods.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -1012,3 +1012,14 @@ def setup(self):\n \n     def time_frame_quantile_axis1(self):\n         self.df.quantile([0.1, 0.5], axis=1)\n+\n+\n+class frame_nlargest(object):\n+    goal_time = 0.2\n+\n+    def setup(self):\n+        self.df = DataFrame(np.random.randn(1000, 3),\n+                            columns=list('ABC'))\n+\n+    def time_frame_nlargest(self):\n+        self.df.nlargest(100, 'A')"
                                },
                                {
                                    "sha": "0567a3c3fa2bbee612255586d6dcf27e66fa61f6",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -61,6 +61,7 @@ Bug Fixes\n - Bug in ``HDFStore`` when writing a ``MultiIndex`` when using ``data_columns=True`` (:issue:`14435`)\n - Bug in ``HDFStore.append()`` when writing a ``Series`` and passing a ``min_itemsize`` argument containing a value for the ``index`` (:issue:`11412`)\n - Bug in ``Series.groupby.nunique()`` raising an ``IndexError`` for an empty ``Series`` (:issue:`12553`)\n+- Bug in ``DataFrame.nlargest`` and ``DataFrame.nsmallest`` when the index had duplicate values (:issue:`13412`)\n \n \n "
                                },
                                {
                                    "sha": "effca6398419e7115e3744c9737e0bc4fbda4f31",
                                    "filename": "pandas/core/algorithms.py",
                                    "status": "modified",
                                    "additions": 28,
                                    "deletions": 2,
                                    "changes": 30,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Falgorithms.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Falgorithms.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Falgorithms.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -684,11 +684,12 @@ def select_n_slow(dropped, n, keep, method):\n _select_methods = {'nsmallest': nsmallest, 'nlargest': nlargest}\n \n \n-def select_n(series, n, keep, method):\n-    \"\"\"Implement n largest/smallest.\n+def select_n_series(series, n, keep, method):\n+    \"\"\"Implement n largest/smallest for pandas Series\n \n     Parameters\n     ----------\n+    series : pandas.Series object\n     n : int\n     keep : {'first', 'last'}, default 'first'\n     method : str, {'nlargest', 'nsmallest'}\n@@ -717,6 +718,31 @@ def select_n(series, n, keep, method):\n     return dropped.iloc[inds]\n \n \n+def select_n_frame(frame, columns, n, method, keep):\n+    \"\"\"Implement n largest/smallest for pandas DataFrame\n+\n+    Parameters\n+    ----------\n+    frame : pandas.DataFrame object\n+    columns : list or str\n+    n : int\n+    keep : {'first', 'last'}, default 'first'\n+    method : str, {'nlargest', 'nsmallest'}\n+\n+    Returns\n+    -------\n+    nordered : DataFrame\n+    \"\"\"\n+    from pandas.core.series import Series\n+    if not is_list_like(columns):\n+        columns = [columns]\n+    columns = list(columns)\n+    ser = getattr(frame[columns[0]], method)(n, keep=keep)\n+    if isinstance(ser, Series):\n+        ser = ser.to_frame()\n+    return ser.merge(frame, on=columns[0], left_index=True)[frame.columns]\n+\n+\n def _finalize_nsmallest(arr, kth_val, n, keep, narr):\n     ns, = np.nonzero(arr <= kth_val)\n     inds = ns[arr[ns].argsort(kind='mergesort')][:n]"
                                },
                                {
                                    "sha": "0053135e1fd8577263e8dd52f31ab025d0e4cc02",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 11,
                                    "changes": 13,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -3337,15 +3337,6 @@ def sortlevel(self, level=0, axis=0, ascending=True, inplace=False,\n         return self.sort_index(level=level, axis=axis, ascending=ascending,\n                                inplace=inplace, sort_remaining=sort_remaining)\n \n-    def _nsorted(self, columns, n, method, keep):\n-        if not is_list_like(columns):\n-            columns = [columns]\n-        columns = list(columns)\n-        ser = getattr(self[columns[0]], method)(n, keep=keep)\n-        ascending = dict(nlargest=False, nsmallest=True)[method]\n-        return self.loc[ser.index].sort_values(columns, ascending=ascending,\n-                                               kind='mergesort')\n-\n     def nlargest(self, n, columns, keep='first'):\n         \"\"\"Get the rows of a DataFrame sorted by the `n` largest\n         values of `columns`.\n@@ -3378,7 +3369,7 @@ def nlargest(self, n, columns, keep='first'):\n         1  10  b   2\n         2   8  d NaN\n         \"\"\"\n-        return self._nsorted(columns, n, 'nlargest', keep)\n+        return algos.select_n_frame(self, columns, n, 'nlargest', keep)\n \n     def nsmallest(self, n, columns, keep='first'):\n         \"\"\"Get the rows of a DataFrame sorted by the `n` smallest\n@@ -3412,7 +3403,7 @@ def nsmallest(self, n, columns, keep='first'):\n         0  1  a   1\n         2  8  d NaN\n         \"\"\"\n-        return self._nsorted(columns, n, 'nsmallest', keep)\n+        return algos.select_n_frame(self, columns, n, 'nsmallest', keep)\n \n     def swaplevel(self, i=-2, j=-1, axis=0):\n         \"\"\""
                                },
                                {
                                    "sha": "958cf183578ddfc3724b18b2cf5018b0c5c1f254",
                                    "filename": "pandas/core/series.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Fseries.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Fcore%2Fseries.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fseries.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -1940,7 +1940,7 @@ def nlargest(self, n=5, keep='first'):\n         >>> s = pd.Series(np.random.randn(1e6))\n         >>> s.nlargest(10)  # only sorts up to the N requested\n         \"\"\"\n-        return algos.select_n(self, n=n, keep=keep, method='nlargest')\n+        return algos.select_n_series(self, n=n, keep=keep, method='nlargest')\n \n     @deprecate_kwarg('take_last', 'keep', mapping={True: 'last',\n                                                    False: 'first'})\n@@ -1978,7 +1978,7 @@ def nsmallest(self, n=5, keep='first'):\n         >>> s = pd.Series(np.random.randn(1e6))\n         >>> s.nsmallest(10)  # only sorts up to the N requested\n         \"\"\"\n-        return algos.select_n(self, n=n, keep=keep, method='nsmallest')\n+        return algos.select_n_series(self, n=n, keep=keep, method='nsmallest')\n \n     def sortlevel(self, level=0, ascending=True, sort_remaining=True):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f6081e14d4081c87388b40419669c0b27d750953",
                                    "filename": "pandas/tests/frame/test_analytics.py",
                                    "status": "modified",
                                    "additions": 29,
                                    "deletions": 0,
                                    "changes": 29,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Ftests%2Fframe%2Ftest_analytics.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Ftests%2Fframe%2Ftest_analytics.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_analytics.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -1323,6 +1323,35 @@ def test_nsmallest_multiple_columns(self):\n         expected = df.sort_values(['a', 'c']).head(5)\n         tm.assert_frame_equal(result, expected)\n \n+    def test_nsmallest_nlargest_duplicate_index(self):\n+        # GH 13412\n+        df = pd.DataFrame({'a': [1, 2, 3, 4],\n+                           'b': [4, 3, 2, 1],\n+                           'c': [0, 1, 2, 3]},\n+                          index=[0, 0, 1, 1])\n+        result = df.nsmallest(4, 'a')\n+        expected = df.sort_values('a').head(4)\n+        tm.assert_frame_equal(result, expected)\n+\n+        result = df.nlargest(4, 'a')\n+        expected = df.sort_values('a', ascending=False).head(4)\n+        tm.assert_frame_equal(result, expected)\n+\n+        result = df.nsmallest(4, ['a', 'c'])\n+        expected = df.sort_values(['a', 'c']).head(4)\n+        tm.assert_frame_equal(result, expected)\n+\n+        result = df.nsmallest(4, ['c', 'a'])\n+        expected = df.sort_values(['c', 'a']).head(4)\n+        tm.assert_frame_equal(result, expected)\n+\n+        result = df.nlargest(4, ['a', 'c'])\n+        expected = df.sort_values(['a', 'c'], ascending=False).head(4)\n+        tm.assert_frame_equal(result, expected)\n+\n+        result = df.nlargest(4, ['c', 'a'])\n+        expected = df.sort_values(['c', 'a'], ascending=False).head(4)\n+        tm.assert_frame_equal(result, expected)\n     # ----------------------------------------------------------------------\n     # Isin\n "
                                },
                                {
                                    "sha": "d4c209d4532e4462f317074955b0b97e99d1eb9d",
                                    "filename": "pandas/tests/series/test_analytics.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Ftests%2Fseries%2Ftest_analytics.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6e514dacc131f044deb74f0a562a51ef3b1201eb/pandas%2Ftests%2Fseries%2Ftest_analytics.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fseries%2Ftest_analytics.py?ref=6e514dacc131f044deb74f0a562a51ef3b1201eb",
                                    "patch": "@@ -1532,6 +1532,15 @@ def test_nsmallest_nlargest(self):\n         with tm.assertRaisesRegexp(ValueError, msg):\n             s.nlargest(keep='invalid')\n \n+        # GH 13412\n+        s = Series([1, 4, 3, 2], index=[0, 0, 1, 1])\n+        result = s.nlargest(3)\n+        expected = s.sort_values(ascending=False).head(3)\n+        assert_series_equal(result, expected)\n+        result = s.nsmallest(3)\n+        expected = s.sort_values().head(3)\n+        assert_series_equal(result, expected)\n+\n     def test_sortlevel(self):\n         mi = MultiIndex.from_tuples([[1, 1, 3], [1, 1, 1]], names=list('ABC'))\n         s = Series([1, 2], mi)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_analytics.py\npipenv run python -m pytest pandas/tests/series/test_analytics.py"
            },
            {
                "id": 14682,
                "created_at": "2016-11-17T22:22:17Z",
                "closed_at": "2016-11-22T11:29:39Z",
                "title": "AmbiguousTimeError on groupby when including a DST change",
                "labels": "Bug, Groupby, Timezones",
                "commits": [
                    {
                        "hash": "9f2e45378cbce5532a8edf2484d62a802369634e",
                        "commit_date": "2016-11-22T11:29:02Z",
                        "parents": "f862b52e752e7f9003ca754179dec7503fccfffa",
                        "stat": {
                            "total": 7,
                            "additions": 47,
                            "deletions": 40,
                            "files": [
                                {
                                    "sha": "5a255d1e62043b348055afc834422f913aa74d90",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9f2e45378cbce5532a8edf2484d62a802369634e/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9f2e45378cbce5532a8edf2484d62a802369634e/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=9f2e45378cbce5532a8edf2484d62a802369634e",
                                    "patch": "@@ -49,6 +49,7 @@ Bug Fixes\n - Compat with python 3.6 for some indexing exception types (:issue:`14684`, :issue:`14689`)\n - Compat with python 3.6 for deprecation warnings in the test suite (:issue:`14681`)\n - Compat with python 3.6 for Timestamp pickles (:issue:`14689`)\n+- Bug in resampling a ``DatetimeIndex`` in local TZ, covering a DST change, which would raise ``AmbiguousTimeError`` (:issue:`14682`)\n \n \n "
                                },
                                {
                                    "sha": "024306edef2d8bc16a0ada981d3d8db046dcb192",
                                    "filename": "pandas/tseries/index.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Findex.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Findex.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Findex.py?ref=9f2e45378cbce5532a8edf2484d62a802369634e",
                                    "patch": "@@ -439,7 +439,7 @@ def _generate(cls, start, end, periods, name, offset,\n                 tz = tz.localize(date.replace(tzinfo=None)).tzinfo\n \n         if tz is not None and inferred_tz is not None:\n-            if not inferred_tz == tz:\n+            if not tslib.get_timezone(inferred_tz) == tslib.get_timezone(tz):\n                 raise AssertionError(\"Inferred time zone not equal to passed \"\n                                      \"time zone\")\n "
                                },
                                {
                                    "sha": "31781eb3fc1317a39d9a0d69441e439f1493abba",
                                    "filename": "pandas/tseries/resample.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 5,
                                    "changes": 16,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Fresample.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Fresample.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Fresample.py?ref=9f2e45378cbce5532a8edf2484d62a802369634e",
                                    "patch": "@@ -1283,9 +1283,18 @@ def _adjust_dates_anchored(first, last, offset, closed='right', base=0):\n     #\n     # See https://github.com/pandas-dev/pandas/issues/8683\n \n+    # 14682 - Since we need to drop the TZ information to perform\n+    # the adjustment in the presence of a DST change,\n+    # save TZ Info and the DST state of the first and last parameters\n+    # so that we can accurately rebuild them at the end.\n     first_tzinfo = first.tzinfo\n+    last_tzinfo = last.tzinfo\n+    first_dst = bool(first.dst())\n+    last_dst = bool(last.dst())\n+\n     first = first.tz_localize(None)\n     last = last.tz_localize(None)\n+\n     start_day_nanos = first.normalize().value\n \n     base_nanos = (base % offset.n) * offset.nanos // offset.n\n@@ -1320,11 +1329,8 @@ def _adjust_dates_anchored(first, last, offset, closed='right', base=0):\n         else:\n             lresult = last.value + offset.nanos\n \n-#     return (Timestamp(fresult, tz=first.tz),\n-#             Timestamp(lresult, tz=last.tz))\n-\n-    return (Timestamp(fresult).tz_localize(first_tzinfo),\n-            Timestamp(lresult).tz_localize(first_tzinfo))\n+    return (Timestamp(fresult).tz_localize(first_tzinfo, ambiguous=first_dst),\n+            Timestamp(lresult).tz_localize(last_tzinfo, ambiguous=last_dst))\n \n \n def asfreq(obj, freq, method=None, how=None, normalize=False):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b8c060c024867baa9115e6baa7fce9cd7d0137ee",
                                    "filename": "pandas/tseries/tests/test_resample.py",
                                    "status": "modified",
                                    "additions": 27,
                                    "deletions": 1,
                                    "changes": 28,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Ftests%2Ftest_resample.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/9f2e45378cbce5532a8edf2484d62a802369634e/pandas%2Ftseries%2Ftests%2Ftest_resample.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Ftests%2Ftest_resample.py?ref=9f2e45378cbce5532a8edf2484d62a802369634e",
                                    "patch": "@@ -1912,7 +1912,33 @@ def test_resample_size(self):\n         right = Series(val, index=ix)\n         assert_series_equal(left, right)\n \n-    def test_resmaple_dst_anchor(self):\n+    def test_resample_across_dst(self):\n+        # The test resamples a DatetimeIndex with values before and after a\n+        # DST change\n+        # Issue: 14682\n+\n+        # The DatetimeIndex we will start with\n+        # (note that DST happens at 03:00+02:00 -> 02:00+01:00)\n+        # 2016-10-30 02:23:00+02:00, 2016-10-30 02:23:00+01:00\n+        df1 = DataFrame([1477786980, 1477790580], columns=['ts'])\n+        dti1 = DatetimeIndex(pd.to_datetime(df1.ts, unit='s')\n+                             .dt.tz_localize('UTC')\n+                             .dt.tz_convert('Europe/Madrid'))\n+\n+        # The expected DatetimeIndex after resampling.\n+        # 2016-10-30 02:00:00+02:00, 2016-10-30 02:00:00+01:00\n+        df2 = DataFrame([1477785600, 1477789200], columns=['ts'])\n+        dti2 = DatetimeIndex(pd.to_datetime(df2.ts, unit='s')\n+                             .dt.tz_localize('UTC')\n+                             .dt.tz_convert('Europe/Madrid'))\n+        df = DataFrame([5, 5], index=dti1)\n+\n+        result = df.resample(rule='H').sum()\n+        expected = DataFrame([5, 5], index=dti2)\n+\n+        assert_frame_equal(result, expected)\n+\n+    def test_resample_dst_anchor(self):\n         # 5172\n         dti = DatetimeIndex([datetime(2012, 11, 4, 23)], tz='US/Eastern')\n         df = DataFrame([5], index=dti)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tseries/tests/test_resample.py"
            },
            {
                "id": 14652,
                "created_at": "2016-11-14T12:21:48Z",
                "closed_at": "2016-11-17T12:49:43Z",
                "title": "BUG: pandas.cut and negative values",
                "labels": "Bug, Groupby, Reshaping",
                "commits": [
                    {
                        "hash": "2fc0c68ace1cb447f1fa6f016295575a2024db3d",
                        "commit_date": "2016-11-17T12:46:45Z",
                        "parents": "726efc7cd55744eb88636dd6f11293f18355a10a",
                        "stat": {
                            "total": 4,
                            "additions": 21,
                            "deletions": 17,
                            "files": [
                                {
                                    "sha": "8feb5355bb2957f2b9a91c42928b2eb09707f840",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 2,
                                    "changes": 5,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2fc0c68ace1cb447f1fa6f016295575a2024db3d/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2fc0c68ace1cb447f1fa6f016295575a2024db3d/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=2fc0c68ace1cb447f1fa6f016295575a2024db3d",
                                    "patch": "@@ -23,5 +23,6 @@ Performance Improvements\n Bug Fixes\n ~~~~~~~~~\n \n-- compat with ``dateutil==2.6.0`` for testing (:issue:`14621`)\n-- allow ``nanoseconds`` in ``Timestamp.replace`` kwargs (:issue:`14621`)\n+- Compat with ``dateutil==2.6.0``; segfault reported in the testing suite (:issue:`14621`)\n+- Allow ``nanoseconds`` in ``Timestamp.replace`` as a kwarg (:issue:`14621`)\n+- Bug in ``pd.cut`` with negative values and a single bin (:issue:`14652`)"
                                },
                                {
                                    "sha": "ef75f2f84779bb5d7d89e67f3a32d1b8590f4673",
                                    "filename": "pandas/tools/tile.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2fc0c68ace1cb447f1fa6f016295575a2024db3d/pandas%2Ftools%2Ftile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2fc0c68ace1cb447f1fa6f016295575a2024db3d/pandas%2Ftools%2Ftile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftile.py?ref=2fc0c68ace1cb447f1fa6f016295575a2024db3d",
                                    "patch": "@@ -98,8 +98,8 @@ def cut(x, bins, right=True, labels=None, retbins=False, precision=3,\n         mn, mx = [mi + 0.0 for mi in rng]\n \n         if mn == mx:  # adjust end points before binning\n-            mn -= .001 * mn\n-            mx += .001 * mx\n+            mn -= .001 * abs(mn)\n+            mx += .001 * abs(mx)\n             bins = np.linspace(mn, mx, bins + 1, endpoint=True)\n         else:  # adjust end points after binning\n             bins = np.linspace(mn, mx, bins + 1, endpoint=True)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e5b9c65b515d6a5824a39a3d7a784473cf70d601",
                                    "filename": "pandas/tools/tests/test_tile.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 0,
                                    "changes": 12,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/2fc0c68ace1cb447f1fa6f016295575a2024db3d/pandas%2Ftools%2Ftests%2Ftest_tile.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/2fc0c68ace1cb447f1fa6f016295575a2024db3d/pandas%2Ftools%2Ftests%2Ftest_tile.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftests%2Ftest_tile.py?ref=2fc0c68ace1cb447f1fa6f016295575a2024db3d",
                                    "patch": "@@ -271,6 +271,18 @@ def test_series_retbins(self):\n                                     np.array([0, 0, 1, 1], dtype=np.int8))\n         tm.assert_numpy_array_equal(bins, np.array([0, 1.5, 3]))\n \n+    def test_single_bin(self):\n+        # issue 14652\n+        expected = Series([0, 0])\n+\n+        s = Series([9., 9.])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n+        s = Series([-9., -9.])\n+        result = cut(s, 1, labels=False)\n+        tm.assert_series_equal(result, expected)\n+\n \n def curpath():\n     pth, _ = os.path.split(os.path.abspath(__file__))"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tools/tests/test_tile.py"
            },
            {
                "id": 14621,
                "created_at": "2016-11-09T07:07:02Z",
                "closed_at": "2016-11-12T15:59:51Z",
                "title": "dateutil 2.6 gives segfault in normalizing timestamp with datetutil timezone",
                "labels": "Bug, Timezones, Compat",
                "commits": [
                    {
                        "hash": "f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                        "commit_date": "2016-11-12T15:59:08Z",
                        "parents": "46000daf257c8e574ca943a57023deac74460edd",
                        "stat": {
                            "total": 27,
                            "additions": 215,
                            "deletions": 188,
                            "files": [
                                {
                                    "sha": "d1fc1fe24a079fe91523f2c719a9e67621c544ff",
                                    "filename": "ci/requirements-3.5_OSX.pip",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/ci%2Frequirements-3.5_OSX.pip",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/ci%2Frequirements-3.5_OSX.pip",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/ci%2Frequirements-3.5_OSX.pip?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -1 +1 @@\n-python-dateutil>=2.5.0\n+python-dateutil==2.5.3"
                                },
                                {
                                    "sha": "dc11dd17bfdd7ff8bafbae55b739a5aa3560986b",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -22,3 +22,6 @@ Performance Improvements\n \n Bug Fixes\n ~~~~~~~~~\n+\n+- compat with ``dateutil==2.6.0`` for testing (:issue:`14621`)\n+- allow ``nanoseconds`` in ``Timestamp.replace`` kwargs (:issue:`14621`)"
                                },
                                {
                                    "sha": "2e3852a7eddddc5bb5e0feb29f88dec9dfc1fa15",
                                    "filename": "pandas/tseries/offsets.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Foffsets.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Foffsets.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Foffsets.py?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -68,6 +68,7 @@ def wrapper(self, other):\n                 other = other.tz_localize(None)\n \n             result = func(self, other)\n+\n             if self._adjust_dst:\n                 result = tslib._localize_pydatetime(result, tz)\n "
                                },
                                {
                                    "sha": "685de214cef7d8c4324c4778bd608e977122a466",
                                    "filename": "pandas/tslib.pyx",
                                    "status": "modified",
                                    "additions": 80,
                                    "deletions": 15,
                                    "changes": 95,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftslib.pyx",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftslib.pyx",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftslib.pyx?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -98,6 +98,7 @@ except NameError: # py3\n cdef inline object create_timestamp_from_ts(\n         int64_t value, pandas_datetimestruct dts,\n         object tz, object freq):\n+    \"\"\" convenience routine to construct a Timestamp from its parts \"\"\"\n     cdef _Timestamp ts_base\n     ts_base = _Timestamp.__new__(Timestamp, dts.year, dts.month,\n                                  dts.day, dts.hour, dts.min,\n@@ -112,6 +113,7 @@ cdef inline object create_timestamp_from_ts(\n cdef inline object create_datetime_from_ts(\n         int64_t value, pandas_datetimestruct dts,\n         object tz, object freq):\n+    \"\"\" convenience routine to construct a datetime.datetime from its parts \"\"\"\n     return datetime(dts.year, dts.month, dts.day, dts.hour,\n                     dts.min, dts.sec, dts.us, tz)\n \n@@ -378,7 +380,6 @@ class Timestamp(_Timestamp):\n         # Mixing pydatetime positional and keyword arguments is forbidden!\n \n         cdef _TSObject ts\n-        cdef _Timestamp ts_base\n \n         if offset is not None:\n             # deprecate offset kwd in 0.19.0, GH13593\n@@ -412,17 +413,7 @@ class Timestamp(_Timestamp):\n             from pandas.tseries.frequencies import to_offset\n             freq = to_offset(freq)\n \n-        # make datetime happy\n-        ts_base = _Timestamp.__new__(cls, ts.dts.year, ts.dts.month,\n-                                     ts.dts.day, ts.dts.hour, ts.dts.min,\n-                                     ts.dts.sec, ts.dts.us, ts.tzinfo)\n-\n-        # fill out rest of data\n-        ts_base.value = ts.value\n-        ts_base.freq = freq\n-        ts_base.nanosecond = ts.dts.ps / 1000\n-\n-        return ts_base\n+        return create_timestamp_from_ts(ts.value, ts.dts, ts.tzinfo, freq)\n \n     def _round(self, freq, rounder):\n \n@@ -660,8 +651,80 @@ class Timestamp(_Timestamp):\n     astimezone = tz_convert\n \n     def replace(self, **kwds):\n-        return Timestamp(datetime.replace(self, **kwds),\n-                         freq=self.freq)\n+        \"\"\"\n+        implements datetime.replace, handles nanoseconds\n+\n+        Parameters\n+        ----------\n+        kwargs: key-value dict\n+\n+        accepted keywords are:\n+        year, month, day, hour, minute, second, microsecond, nanosecond, tzinfo\n+\n+        values must be integer, or for tzinfo, a tz-convertible\n+\n+        Returns\n+        -------\n+        Timestamp with fields replaced\n+        \"\"\"\n+\n+        cdef:\n+            pandas_datetimestruct dts\n+            int64_t value\n+            object tzinfo, result, k, v\n+            _TSObject ts\n+\n+        # set to naive if needed\n+        tzinfo = self.tzinfo\n+        value = self.value\n+        if tzinfo is not None:\n+            value = tz_convert_single(value, 'UTC', tzinfo)\n+\n+        # setup components\n+        pandas_datetime_to_datetimestruct(value, PANDAS_FR_ns, &dts)\n+        dts.ps = self.nanosecond * 1000\n+\n+        # replace\n+        def validate(k, v):\n+            \"\"\" validate integers \"\"\"\n+            if not isinstance(v, int):\n+                raise ValueError(\"value must be an integer, received {v} for {k}\".format(v=type(v), k=k))\n+            return v\n+\n+        for k, v in kwds.items():\n+            if k == 'year':\n+                dts.year = validate(k, v)\n+            elif k == 'month':\n+                dts.month = validate(k, v)\n+            elif k == 'day':\n+                dts.day = validate(k, v)\n+            elif k == 'hour':\n+                dts.hour = validate(k, v)\n+            elif k == 'minute':\n+                dts.min = validate(k, v)\n+            elif k == 'second':\n+                dts.sec = validate(k, v)\n+            elif k == 'microsecond':\n+                dts.us = validate(k, v)\n+            elif k == 'nanosecond':\n+                dts.ps = validate(k, v) * 1000\n+            elif k == 'tzinfo':\n+                tzinfo = v\n+            else:\n+                raise ValueError(\"invalid name {} passed\".format(k))\n+\n+        # reconstruct & check bounds\n+        value = pandas_datetimestruct_to_datetime(PANDAS_FR_ns, &dts)\n+        if value != NPY_NAT:\n+            _check_dts_bounds(&dts)\n+\n+        # set tz if needed\n+        if tzinfo is not None:\n+            value = tz_convert_single(value, tzinfo, 'UTC')\n+\n+        result = create_timestamp_from_ts(value, dts, tzinfo, self.freq)\n+\n+        return result\n \n     def isoformat(self, sep='T'):\n         base = super(_Timestamp, self).isoformat(sep=sep)\n@@ -5041,7 +5104,9 @@ cpdef normalize_date(object dt):\n     -------\n     normalized : datetime.datetime or Timestamp\n     \"\"\"\n-    if PyDateTime_Check(dt):\n+    if is_timestamp(dt):\n+        return dt.replace(hour=0, minute=0, second=0, microsecond=0, nanosecond=0)\n+    elif PyDateTime_Check(dt):\n         return dt.replace(hour=0, minute=0, second=0, microsecond=0)\n     elif PyDate_Check(dt):\n         return datetime(dt.year, dt.month, dt.day)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "768e9212e6c4254aa2ceea7df1ee1be2f82d9fd3",
                                    "filename": "pandas/tseries/tests/test_offsets.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 6,
                                    "changes": 20,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_offsets.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_offsets.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Ftests%2Ftest_offsets.py?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -1,4 +1,5 @@\n import os\n+from distutils.version import LooseVersion\n from datetime import date, datetime, timedelta\n from dateutil.relativedelta import relativedelta\n from pandas.compat import range, iteritems\n@@ -4851,6 +4852,7 @@ def _test_all_offsets(self, n, **kwds):\n \n     def _test_offset(self, offset_name, offset_n, tstart, expected_utc_offset):\n         offset = DateOffset(**{offset_name: offset_n})\n+\n         t = tstart + offset\n         if expected_utc_offset is not None:\n             self.assertTrue(get_utc_offset_hours(t) == expected_utc_offset)\n@@ -4890,17 +4892,23 @@ def _make_timestamp(self, string, hrs_offset, tz):\n         return Timestamp(string + offset_string).tz_convert(tz)\n \n     def test_fallback_plural(self):\n-        \"\"\"test moving from daylight savings to standard time\"\"\"\n+        # test moving from daylight savings to standard time\n+        import dateutil\n         for tz, utc_offsets in self.timezone_utc_offsets.items():\n             hrs_pre = utc_offsets['utc_offset_daylight']\n             hrs_post = utc_offsets['utc_offset_standard']\n-            self._test_all_offsets(\n-                n=3, tstart=self._make_timestamp(self.ts_pre_fallback,\n-                                                 hrs_pre, tz),\n-                expected_utc_offset=hrs_post)\n+\n+            if dateutil.__version__ != LooseVersion('2.6.0'):\n+                # buggy ambiguous behavior in 2.6.0\n+                # GH 14621\n+                # https://github.com/dateutil/dateutil/issues/321\n+                self._test_all_offsets(\n+                    n=3, tstart=self._make_timestamp(self.ts_pre_fallback,\n+                                                     hrs_pre, tz),\n+                    expected_utc_offset=hrs_post)\n \n     def test_springforward_plural(self):\n-        \"\"\"test moving from standard to daylight savings\"\"\"\n+        # test moving from standard to daylight savings\n         for tz, utc_offsets in self.timezone_utc_offsets.items():\n             hrs_pre = utc_offsets['utc_offset_standard']\n             hrs_post = utc_offsets['utc_offset_daylight']"
                                },
                                {
                                    "sha": "db8cda5c76479191ffa93461409c10e7b4a1f365",
                                    "filename": "pandas/tseries/tests/test_timezones.py",
                                    "status": "modified",
                                    "additions": 86,
                                    "deletions": 3,
                                    "changes": 89,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_timezones.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_timezones.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Ftests%2Ftest_timezones.py?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -4,7 +4,7 @@\n \n import numpy as np\n import pytz\n-\n+from distutils.version import LooseVersion\n from pandas.types.dtypes import DatetimeTZDtype\n from pandas import (Index, Series, DataFrame, isnull, Timestamp)\n \n@@ -518,8 +518,12 @@ def f():\n \n         times = date_range(\"2013-10-26 23:00\", \"2013-10-27 01:00\", freq=\"H\",\n                            tz=tz, ambiguous='infer')\n-        self.assertEqual(times[0], Timestamp('2013-10-26 23:00', tz=tz))\n-        self.assertEqual(times[-1], Timestamp('2013-10-27 01:00', tz=tz))\n+        self.assertEqual(times[0], Timestamp('2013-10-26 23:00', tz=tz,\n+                                             freq=\"H\"))\n+        if dateutil.__version__ != LooseVersion('2.6.0'):\n+            # GH 14621\n+            self.assertEqual(times[-1], Timestamp('2013-10-27 01:00', tz=tz,\n+                                                  freq=\"H\"))\n \n     def test_ambiguous_nat(self):\n         tz = self.tz('US/Eastern')\n@@ -1163,6 +1167,85 @@ class TestTimeZones(tm.TestCase):\n     def setUp(self):\n         tm._skip_if_no_pytz()\n \n+    def test_replace(self):\n+        # GH 14621\n+        # GH 7825\n+        # replacing datetime components with and w/o presence of a timezone\n+        dt = Timestamp('2016-01-01 09:00:00')\n+        result = dt.replace(hour=0)\n+        expected = Timestamp('2016-01-01 00:00:00')\n+        self.assertEqual(result, expected)\n+\n+        for tz in self.timezones:\n+            dt = Timestamp('2016-01-01 09:00:00', tz=tz)\n+            result = dt.replace(hour=0)\n+            expected = Timestamp('2016-01-01 00:00:00', tz=tz)\n+            self.assertEqual(result, expected)\n+\n+        # we preserve nanoseconds\n+        dt = Timestamp('2016-01-01 09:00:00.000000123', tz=tz)\n+        result = dt.replace(hour=0)\n+        expected = Timestamp('2016-01-01 00:00:00.000000123', tz=tz)\n+        self.assertEqual(result, expected)\n+\n+        # test all\n+        dt = Timestamp('2016-01-01 09:00:00.000000123', tz=tz)\n+        result = dt.replace(year=2015, month=2, day=2, hour=0, minute=5,\n+                            second=5, microsecond=5, nanosecond=5)\n+        expected = Timestamp('2015-02-02 00:05:05.000005005', tz=tz)\n+        self.assertEqual(result, expected)\n+\n+        # error\n+        def f():\n+            dt.replace(foo=5)\n+        self.assertRaises(ValueError, f)\n+\n+        def f():\n+            dt.replace(hour=0.1)\n+        self.assertRaises(ValueError, f)\n+\n+        # assert conversion to naive is the same as replacing tzinfo with None\n+        dt = Timestamp('2013-11-03 01:59:59.999999-0400', tz='US/Eastern')\n+        self.assertEqual(dt.tz_localize(None), dt.replace(tzinfo=None))\n+\n+    def test_ambiguous_compat(self):\n+        # validate that pytz and dateutil are compat for dst\n+        # when the transition happens\n+        tm._skip_if_no_dateutil()\n+        tm._skip_if_no_pytz()\n+\n+        pytz_zone = 'Europe/London'\n+        dateutil_zone = 'dateutil/Europe/London'\n+        result_pytz = (Timestamp('2013-10-27 01:00:00')\n+                       .tz_localize(pytz_zone, ambiguous=0))\n+        result_dateutil = (Timestamp('2013-10-27 01:00:00')\n+                           .tz_localize(dateutil_zone, ambiguous=0))\n+        self.assertEqual(result_pytz.value, result_dateutil.value)\n+        self.assertEqual(result_pytz.value, 1382835600000000000)\n+\n+        # dateutil 2.6 buggy w.r.t. ambiguous=0\n+        if dateutil.__version__ != LooseVersion('2.6.0'):\n+            # GH 14621\n+            # https://github.com/dateutil/dateutil/issues/321\n+            self.assertEqual(result_pytz.to_pydatetime().tzname(),\n+                             result_dateutil.to_pydatetime().tzname())\n+            self.assertEqual(str(result_pytz), str(result_dateutil))\n+\n+        # 1 hour difference\n+        result_pytz = (Timestamp('2013-10-27 01:00:00')\n+                       .tz_localize(pytz_zone, ambiguous=1))\n+        result_dateutil = (Timestamp('2013-10-27 01:00:00')\n+                           .tz_localize(dateutil_zone, ambiguous=1))\n+        self.assertEqual(result_pytz.value, result_dateutil.value)\n+        self.assertEqual(result_pytz.value, 1382832000000000000)\n+\n+        # dateutil < 2.6 is buggy w.r.t. ambiguous timezones\n+        if dateutil.__version__ > LooseVersion('2.5.3'):\n+            # GH 14621\n+            self.assertEqual(str(result_pytz), str(result_dateutil))\n+            self.assertEqual(result_pytz.to_pydatetime().tzname(),\n+                             result_dateutil.to_pydatetime().tzname())\n+\n     def test_index_equals_with_tz(self):\n         left = date_range('1/1/2011', periods=100, freq='H', tz='utc')\n         right = date_range('1/1/2011', periods=100, freq='H', tz='US/Eastern')"
                                },
                                {
                                    "sha": "b45f867be65dd10ce9322700d19d56fdb0861ce6",
                                    "filename": "pandas/tseries/tests/test_tslib.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 2,
                                    "changes": 5,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_tslib.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f8bd08e9c2fc6365980f41b846bbae4b40f08b83/pandas%2Ftseries%2Ftests%2Ftest_tslib.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftseries%2Ftests%2Ftest_tslib.py?ref=f8bd08e9c2fc6365980f41b846bbae4b40f08b83",
                                    "patch": "@@ -327,8 +327,9 @@ def test_repr(self):\n \n         # dateutil zone change (only matters for repr)\n         import dateutil\n-        if dateutil.__version__ >= LooseVersion(\n-                '2.3') and dateutil.__version__ <= LooseVersion('2.4.0'):\n+        if (dateutil.__version__ >= LooseVersion('2.3') and\n+            (dateutil.__version__ <= LooseVersion('2.4.0') or\n+             dateutil.__version__ >= LooseVersion('2.6.0'))):\n             timezones = ['UTC', 'Asia/Tokyo', 'US/Eastern',\n                          'dateutil/US/Pacific']\n         else:"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tseries/tests/test_offsets.py\npipenv run python -m pytest pandas/tseries/tests/test_timezones.py\npipenv run python -m pandas/tseries/tests/test_tslib.py"
            },
            {
                "id": 14617,
                "created_at": "2016-11-08T20:13:26Z",
                "closed_at": "2017-02-28T14:28:37Z",
                "title": "Columns and Index share the same numpy object underneath when pd.DataFrame.cov is used",
                "labels": "Bug, Indexing, Reshaping, Numeric Operations",
                "commits": [
                    {
                        "hash": "d0a281fd60a2099c932151280af88d5392ea9a84",
                        "commit_date": "2017-02-28T14:27:50Z",
                        "parents": "dd368eb574f7f62f8e8e8d667d68b5d06ae241de",
                        "stat": {
                            "total": 3,
                            "additions": 17,
                            "deletions": 14,
                            "files": [
                                {
                                    "sha": "54df7514a882d16880f8e3394279f4b4accd5c79",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d0a281fd60a2099c932151280af88d5392ea9a84/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d0a281fd60a2099c932151280af88d5392ea9a84/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=d0a281fd60a2099c932151280af88d5392ea9a84",
                                    "patch": "@@ -631,7 +631,7 @@ Bug Fixes\n \n \n - Bug in ``.rank()`` which incorrectly ranks ordered categories (:issue:`15420`)\n-\n+- Bug in ``.corr()`` and ``.cov()`` where the column and index were the same object (:issue:`14617`)\n \n \n - Require at least 0.23 version of cython to avoid problems with character encodings (:issue:`14699`)"
                                },
                                {
                                    "sha": "021ce59e3402b70efe4e911e55d2a0a9fb3041e3",
                                    "filename": "pandas/core/frame.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 2,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d0a281fd60a2099c932151280af88d5392ea9a84/pandas%2Fcore%2Fframe.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d0a281fd60a2099c932151280af88d5392ea9a84/pandas%2Fcore%2Fframe.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fframe.py?ref=d0a281fd60a2099c932151280af88d5392ea9a84",
                                    "patch": "@@ -4725,6 +4725,7 @@ def corr(self, method='pearson', min_periods=1):\n         \"\"\"\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n+        idx = cols.copy()\n         mat = numeric_df.values\n \n         if method == 'pearson':\n@@ -4757,7 +4758,7 @@ def corr(self, method='pearson', min_periods=1):\n                     correl[i, j] = c\n                     correl[j, i] = c\n \n-        return self._constructor(correl, index=cols, columns=cols)\n+        return self._constructor(correl, index=idx, columns=cols)\n \n     def cov(self, min_periods=None):\n         \"\"\"\n@@ -4780,6 +4781,7 @@ def cov(self, min_periods=None):\n         \"\"\"\n         numeric_df = self._get_numeric_data()\n         cols = numeric_df.columns\n+        idx = cols.copy()\n         mat = numeric_df.values\n \n         if notnull(mat).all():\n@@ -4793,7 +4795,7 @@ def cov(self, min_periods=None):\n             baseCov = _algos.nancorr(_ensure_float64(mat), cov=True,\n                                      minp=min_periods)\n \n-        return self._constructor(baseCov, index=cols, columns=cols)\n+        return self._constructor(baseCov, index=idx, columns=cols)\n \n     def corrwith(self, other, axis=0, drop=False):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "111195363beb2c56227627a7b6681c27a5a21ba3",
                                    "filename": "pandas/tests/frame/test_analytics.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/d0a281fd60a2099c932151280af88d5392ea9a84/pandas%2Ftests%2Fframe%2Ftest_analytics.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/d0a281fd60a2099c932151280af88d5392ea9a84/pandas%2Ftests%2Fframe%2Ftest_analytics.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_analytics.py?ref=d0a281fd60a2099c932151280af88d5392ea9a84",
                                    "patch": "@@ -118,6 +118,15 @@ def test_corr_int_and_boolean(self):\n         for meth in ['pearson', 'kendall', 'spearman']:\n             tm.assert_frame_equal(df.corr(meth), expected)\n \n+    def test_corr_cov_independent_index_column(self):\n+        # GH 14617\n+        df = pd.DataFrame(np.random.randn(4 * 10).reshape(10, 4),\n+                          columns=list(\"abcd\"))\n+        for method in ['cov', 'corr']:\n+            result = getattr(df, method)()\n+            assert result.index is not result.columns\n+            assert result.index.equals(result.columns)\n+\n     def test_cov(self):\n         # min_periods no NAs (corner case)\n         expected = self.frame.cov()"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_analytics.py"
            },
            {
                "id": 14580,
                "created_at": "2016-11-03T21:16:37Z",
                "closed_at": "2016-12-30T21:35:23Z",
                "title": "BUG: iloc misbehavior with pd.Series: sometimes returns pd.Categorical instead",
                "labels": "Bug, Indexing, Categorical",
                "commits": [
                    {
                        "hash": "7dd451d881964d958acbd078e8dba505906b01bf",
                        "commit_date": "2016-12-30T21:32:55Z",
                        "parents": "caab85b8520d530c8c67a9e363c8f87905a456e8",
                        "stat": {
                            "total": 17,
                            "additions": 71,
                            "deletions": 54,
                            "files": [
                                {
                                    "sha": "63c2a93368de18c37786bcbac02756a39170e750",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/7dd451d881964d958acbd078e8dba505906b01bf/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/7dd451d881964d958acbd078e8dba505906b01bf/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=7dd451d881964d958acbd078e8dba505906b01bf",
                                    "patch": "@@ -311,6 +311,7 @@ Bug Fixes\n \n \n \n+- Bug in ``Series.iloc`` where a ``Categorical`` object for list-like indexes input was returned, where a ``Series`` was expected. (:issue:`14580`)\n \n \n "
                                },
                                {
                                    "sha": "dad5bf5bc70ba882503817e50928c45d68a111f0",
                                    "filename": "pandas/core/indexing.py",
                                    "status": "modified",
                                    "additions": 32,
                                    "deletions": 17,
                                    "changes": 49,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/7dd451d881964d958acbd078e8dba505906b01bf/pandas%2Fcore%2Findexing.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/7dd451d881964d958acbd078e8dba505906b01bf/pandas%2Fcore%2Findexing.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Findexing.py?ref=7dd451d881964d958acbd078e8dba505906b01bf",
                                    "patch": "@@ -1596,6 +1596,27 @@ def _get_slice_axis(self, slice_obj, axis=0):\n         else:\n             return self.obj.take(slice_obj, axis=axis, convert=False)\n \n+    def _get_list_axis(self, key_list, axis=0):\n+        \"\"\"\n+        Return Series values by list or array of integers\n+\n+        Parameters\n+        ----------\n+        key_list : list-like positional indexer\n+        axis : int (can only be zero)\n+\n+        Returns\n+        -------\n+        Series object\n+        \"\"\"\n+\n+        # validate list bounds\n+        self._is_valid_list_like(key_list, axis)\n+\n+        # force an actual list\n+        key_list = list(key_list)\n+        return self.obj.take(key_list, axis=axis, convert=False)\n+\n     def _getitem_axis(self, key, axis=0):\n \n         if isinstance(key, slice):\n@@ -1606,26 +1627,20 @@ def _getitem_axis(self, key, axis=0):\n             self._has_valid_type(key, axis)\n             return self._getbool_axis(key, axis=axis)\n \n-        # a single integer or a list of integers\n-        else:\n-\n-            if is_list_like_indexer(key):\n-\n-                # validate list bounds\n-                self._is_valid_list_like(key, axis)\n-\n-                # force an actual list\n-                key = list(key)\n+        # a list of integers\n+        elif is_list_like_indexer(key):\n+            return self._get_list_axis(key, axis=axis)\n \n-            else:\n-                key = self._convert_scalar_indexer(key, axis)\n+        # a single integer\n+        else:\n+            key = self._convert_scalar_indexer(key, axis)\n \n-                if not is_integer(key):\n-                    raise TypeError(\"Cannot index by location index with a \"\n-                                    \"non-integer key\")\n+            if not is_integer(key):\n+                raise TypeError(\"Cannot index by location index with a \"\n+                                \"non-integer key\")\n \n-                # validate the location\n-                self._is_valid_integer(key, axis)\n+            # validate the location\n+            self._is_valid_integer(key, axis)\n \n             return self._get_loc(key, axis=axis)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "fe61f16cf7eb1ab310a8562341d70d390666fd41",
                                    "filename": "pandas/tests/test_categorical.py",
                                    "status": "modified",
                                    "additions": 21,
                                    "deletions": 0,
                                    "changes": 21,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/7dd451d881964d958acbd078e8dba505906b01bf/pandas%2Ftests%2Ftest_categorical.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/7dd451d881964d958acbd078e8dba505906b01bf/pandas%2Ftests%2Ftest_categorical.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_categorical.py?ref=7dd451d881964d958acbd078e8dba505906b01bf",
                                    "patch": "@@ -54,6 +54,27 @@ def test_getitem_listlike(self):\n         expected = c[np.array([100000]).astype(np.int64)].codes\n         self.assert_numpy_array_equal(result, expected)\n \n+    def test_getitem_category_type(self):\n+        # GH 14580\n+        # test iloc() on Series with Categorical data\n+\n+        s = pd.Series([1, 2, 3]).astype('category')\n+\n+        # get slice\n+        result = s.iloc[0:2]\n+        expected = pd.Series([1, 2]).astype('category', categories=[1, 2, 3])\n+        tm.assert_series_equal(result, expected)\n+\n+        # get list of indexes\n+        result = s.iloc[[0, 1]]\n+        expected = pd.Series([1, 2]).astype('category', categories=[1, 2, 3])\n+        tm.assert_series_equal(result, expected)\n+\n+        # get boolean array\n+        result = s.iloc[[True, False, False]]\n+        expected = pd.Series([1]).astype('category', categories=[1, 2, 3])\n+        tm.assert_series_equal(result, expected)\n+\n     def test_setitem(self):\n \n         # int/positional"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_categorical.py"
            },
            {
                "id": 14522,
                "created_at": "2016-10-27T15:45:30Z",
                "closed_at": "2016-12-30T21:43:21Z",
                "title": "Categorical.searchsorted() uses lexical order instead of the provided categorical order",
                "labels": "Bug, Indexing, Categorical",
                "commits": [
                    {
                        "hash": "0252385a71f8b8738c3223dcc44af001baa79b10",
                        "commit_date": "2016-12-30T21:42:20Z",
                        "parents": "7dd451d881964d958acbd078e8dba505906b01bf",
                        "stat": {
                            "total": 46,
                            "additions": 99,
                            "deletions": 53,
                            "files": [
                                {
                                    "sha": "02c7ac150c6af813ee2c2ca4070062536e654c97",
                                    "filename": "doc/source/whatsnew/v0.20.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/0252385a71f8b8738c3223dcc44af001baa79b10/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/0252385a71f8b8738c3223dcc44af001baa79b10/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.20.0.txt?ref=0252385a71f8b8738c3223dcc44af001baa79b10",
                                    "patch": "@@ -319,6 +319,7 @@ Bug Fixes\n \n - Bug in ``DataFrame.to_html`` with ``index=False`` and ``max_rows`` raising in ``IndexError`` (:issue:`14998`)\n \n+- Bug in ``Categorical.searchsorted()`` where alphabetical instead of the provided categorical order was used (:issue:`14522`)\n \n \n "
                                },
                                {
                                    "sha": "0562736038483dc5755e960d289c9ff68e7436ac",
                                    "filename": "pandas/core/categorical.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 3,
                                    "changes": 11,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/0252385a71f8b8738c3223dcc44af001baa79b10/pandas%2Fcore%2Fcategorical.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/0252385a71f8b8738c3223dcc44af001baa79b10/pandas%2Fcore%2Fcategorical.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fcategorical.py?ref=0252385a71f8b8738c3223dcc44af001baa79b10",
                                    "patch": "@@ -1086,10 +1086,15 @@ def searchsorted(self, value, side='left', sorter=None):\n                              \"ordered one\")\n \n         from pandas.core.series import Series\n-        values_as_codes = self.categories.values.searchsorted(\n-            Series(value).values, side=side)\n \n-        return self.codes.searchsorted(values_as_codes, sorter=sorter)\n+        values_as_codes = _get_codes_for_values(Series(value).values,\n+                                                self.categories)\n+\n+        if -1 in values_as_codes:\n+            raise ValueError(\"Value(s) to be inserted must be in categories.\")\n+\n+        return self.codes.searchsorted(values_as_codes, side=side,\n+                                       sorter=sorter)\n \n     def isnull(self):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "23280395427fd9fd05be7e4acee627c5fb1000da",
                                    "filename": "pandas/tests/test_categorical.py",
                                    "status": "modified",
                                    "additions": 44,
                                    "deletions": 43,
                                    "changes": 87,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/0252385a71f8b8738c3223dcc44af001baa79b10/pandas%2Ftests%2Ftest_categorical.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/0252385a71f8b8738c3223dcc44af001baa79b10/pandas%2Ftests%2Ftest_categorical.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_categorical.py?ref=0252385a71f8b8738c3223dcc44af001baa79b10",
                                    "patch": "@@ -1569,54 +1569,55 @@ def test_memory_usage(self):\n \n     def test_searchsorted(self):\n         # https://github.com/pandas-dev/pandas/issues/8420\n-        s1 = pd.Series(['apple', 'bread', 'bread', 'cheese', 'milk'])\n-        s2 = pd.Series(['apple', 'bread', 'bread', 'cheese', 'milk', 'donuts'])\n-        c1 = pd.Categorical(s1, ordered=True)\n-        c2 = pd.Categorical(s2, ordered=True)\n-\n-        # Single item array\n-        res = c1.searchsorted(['bread'])\n-        chk = s1.searchsorted(['bread'])\n-        exp = np.array([1], dtype=np.intp)\n-        self.assert_numpy_array_equal(res, exp)\n-        self.assert_numpy_array_equal(res, chk)\n-\n-        # Scalar version of single item array\n-        # Categorical return np.array like pd.Series, but different from\n-        # np.array.searchsorted()\n-        res = c1.searchsorted('bread')\n-        chk = s1.searchsorted('bread')\n-        exp = np.array([1], dtype=np.intp)\n-        self.assert_numpy_array_equal(res, exp)\n-        self.assert_numpy_array_equal(res, chk)\n+        # https://github.com/pandas-dev/pandas/issues/14522\n+\n+        c1 = pd.Categorical(['cheese', 'milk', 'apple', 'bread', 'bread'],\n+                 categories=['cheese', 'milk', 'apple', 'bread'],\n+                 ordered=True)\n+        s1 = pd.Series(c1)\n+        c2 = pd.Categorical(['cheese', 'milk', 'apple', 'bread', 'bread'],\n+                 categories=['cheese', 'milk', 'apple', 'bread'],\n+                 ordered=False)\n+        s2 = pd.Series(c2)\n+\n+        # Searching for single item argument, side='left' (default)\n+        res_cat = c1.searchsorted('apple')\n+        res_ser = s1.searchsorted('apple')\n+        exp = np.array([2], dtype=np.intp)\n+        self.assert_numpy_array_equal(res_cat, exp)\n+        self.assert_numpy_array_equal(res_ser, exp)\n+\n+        # Searching for single item array, side='left' (default)\n+        res_cat = c1.searchsorted(['bread'])\n+        res_ser = s1.searchsorted(['bread'])\n+        exp = np.array([3], dtype=np.intp)\n+        self.assert_numpy_array_equal(res_cat, exp)\n+        self.assert_numpy_array_equal(res_ser, exp)\n+\n+        # Searching for several items array, side='right'\n+        res_cat = c1.searchsorted(['apple', 'bread'], side='right')\n+        res_ser = s1.searchsorted(['apple', 'bread'], side='right')\n+        exp = np.array([3, 5], dtype=np.intp)\n+        self.assert_numpy_array_equal(res_cat, exp)\n+        self.assert_numpy_array_equal(res_ser, exp)\n \n-        # Searching for a value that is not present in the Categorical\n-        res = c1.searchsorted(['bread', 'eggs'])\n-        chk = s1.searchsorted(['bread', 'eggs'])\n-        exp = np.array([1, 4], dtype=np.intp)\n-        self.assert_numpy_array_equal(res, exp)\n-        self.assert_numpy_array_equal(res, chk)\n+        # Searching for a single value that is not from the Categorical\n+        self.assertRaises(ValueError, lambda: c1.searchsorted('cucumber'))\n+        self.assertRaises(ValueError, lambda: s1.searchsorted('cucumber'))\n \n-        # Searching for a value that is not present, to the right\n-        res = c1.searchsorted(['bread', 'eggs'], side='right')\n-        chk = s1.searchsorted(['bread', 'eggs'], side='right')\n-        exp = np.array([3, 4], dtype=np.intp)  # eggs before milk\n-        self.assert_numpy_array_equal(res, exp)\n-        self.assert_numpy_array_equal(res, chk)\n-\n-        # As above, but with a sorter array to reorder an unsorted array\n-        res = c2.searchsorted(['bread', 'eggs'], side='right',\n-                              sorter=[0, 1, 2, 3, 5, 4])\n-        chk = s2.searchsorted(['bread', 'eggs'], side='right',\n-                              sorter=[0, 1, 2, 3, 5, 4])\n-        # eggs after donuts, after switching milk and donuts\n-        exp = np.array([3, 5], dtype=np.intp)\n-        self.assert_numpy_array_equal(res, exp)\n-        self.assert_numpy_array_equal(res, chk)\n+        # Searching for multiple values one of each is not from the Categorical\n+        self.assertRaises(ValueError,\n+                          lambda: c1.searchsorted(['bread', 'cucumber']))\n+        self.assertRaises(ValueError,\n+                          lambda: s1.searchsorted(['bread', 'cucumber']))\n+\n+        # searchsorted call for unordered Categorical\n+        self.assertRaises(ValueError, lambda: c2.searchsorted('apple'))\n+        self.assertRaises(ValueError, lambda: s2.searchsorted('apple'))\n \n         with tm.assert_produces_warning(FutureWarning):\n             res = c1.searchsorted(v=['bread'])\n-            exp = np.array([1], dtype=np.intp)\n+            exp = np.array([3], dtype=np.intp)\n             tm.assert_numpy_array_equal(res, exp)\n \n     def test_deprecated_labels(self):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_categorical.py"
            },
            {
                "id": 14504,
                "created_at": "2016-10-26T17:42:43Z",
                "closed_at": "2016-11-17T13:10:59Z",
                "title": "BUG: to_numeric downcast = 'unsigned' would not un-sign a 0 value.",
                "labels": "Bug, Dtype Conversions",
                "commits": [
                    {
                        "hash": "b5864b0af00ef9406dca5f4988fd79ff8341bbc6",
                        "commit_date": "2016-11-17T13:10:19Z",
                        "parents": "7e6693797674611353c8e3ae6fe405114bb8140c",
                        "stat": {
                            "total": 2,
                            "additions": 41,
                            "deletions": 39,
                            "files": [
                                {
                                    "sha": "f193de7fbdbd0c35c726ac0fa9d91e525d1f9a73",
                                    "filename": "doc/source/whatsnew/v0.19.2.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.2.txt?ref=b5864b0af00ef9406dca5f4988fd79ff8341bbc6",
                                    "patch": "@@ -33,6 +33,7 @@ Bug Fixes\n \n \n - Bug in ``pd.cut`` with negative values and a single bin (:issue:`14652`)\n+- Bug in ``pd.to_numeric`` where a 0 was not unsigned on a ``downcast='unsigned'`` argument (:issue:`14401`)\n \n \n "
                                },
                                {
                                    "sha": "b50bf9dc448bccad46bce80477f220bcf3a67008",
                                    "filename": "pandas/tools/util.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/pandas%2Ftools%2Futil.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/pandas%2Ftools%2Futil.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Futil.py?ref=b5864b0af00ef9406dca5f4988fd79ff8341bbc6",
                                    "patch": "@@ -205,7 +205,7 @@ def to_numeric(arg, errors='raise', downcast=None):\n \n         if downcast in ('integer', 'signed'):\n             typecodes = np.typecodes['Integer']\n-        elif downcast == 'unsigned' and np.min(values) > 0:\n+        elif downcast == 'unsigned' and np.min(values) >= 0:\n             typecodes = np.typecodes['UnsignedInteger']\n         elif downcast == 'float':\n             typecodes = np.typecodes['Float']"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f9647721e3c5b37241e412a6a9319ddc846f3252",
                                    "filename": "pandas/tools/tests/test_util.py",
                                    "status": "modified",
                                    "additions": 37,
                                    "deletions": 1,
                                    "changes": 38,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/pandas%2Ftools%2Ftests%2Ftest_util.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b5864b0af00ef9406dca5f4988fd79ff8341bbc6/pandas%2Ftools%2Ftests%2Ftest_util.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftests%2Ftest_util.py?ref=b5864b0af00ef9406dca5f4988fd79ff8341bbc6",
                                    "patch": "@@ -4,9 +4,10 @@\n import nose\n \n import numpy as np\n+from numpy import iinfo\n \n import pandas as pd\n-from pandas import date_range, Index\n+from pandas import (date_range, Index, _np_version_under1p9)\n import pandas.util.testing as tm\n from pandas.tools.util import cartesian_product, to_numeric\n \n@@ -401,6 +402,41 @@ def test_downcast(self):\n             res = pd.to_numeric(data, downcast=downcast)\n             tm.assert_numpy_array_equal(res, expected)\n \n+    def test_downcast_limits(self):\n+        # Test the limits of each downcast. Bug: #14401.\n+        # Check to make sure numpy is new enough to run this test.\n+        if _np_version_under1p9:\n+            raise nose.SkipTest(\"Numpy version is under 1.9\")\n+\n+        i = 'integer'\n+        u = 'unsigned'\n+        dtype_downcast_min_max = [\n+            ('int8', i, [iinfo(np.int8).min, iinfo(np.int8).max]),\n+            ('int16', i, [iinfo(np.int16).min, iinfo(np.int16).max]),\n+            ('int32', i, [iinfo(np.int32).min, iinfo(np.int32).max]),\n+            ('int64', i, [iinfo(np.int64).min, iinfo(np.int64).max]),\n+            ('uint8', u, [iinfo(np.uint8).min, iinfo(np.uint8).max]),\n+            ('uint16', u, [iinfo(np.uint16).min, iinfo(np.uint16).max]),\n+            ('uint32', u, [iinfo(np.uint32).min, iinfo(np.uint32).max]),\n+            # Test will be skipped until there is more uint64 support.\n+            # ('uint64', u, [iinfo(uint64).min, iinfo(uint64).max]),\n+            ('int16', i, [iinfo(np.int8).min, iinfo(np.int8).max + 1]),\n+            ('int32', i, [iinfo(np.int16).min, iinfo(np.int16).max + 1]),\n+            ('int64', i, [iinfo(np.int32).min, iinfo(np.int32).max + 1]),\n+            ('int16', i, [iinfo(np.int8).min - 1, iinfo(np.int16).max]),\n+            ('int32', i, [iinfo(np.int16).min - 1, iinfo(np.int32).max]),\n+            ('int64', i, [iinfo(np.int32).min - 1, iinfo(np.int64).max]),\n+            ('uint16', u, [iinfo(np.uint8).min, iinfo(np.uint8).max + 1]),\n+            ('uint32', u, [iinfo(np.uint16).min, iinfo(np.uint16).max + 1]),\n+            # Test will be skipped until there is more uint64 support.\n+            # ('uint64', u, [iinfo(np.uint32).min, iinfo(np.uint32).max + 1]),\n+        ]\n+\n+        for dtype, downcast, min_max in dtype_downcast_min_max:\n+            series = pd.to_numeric(pd.Series(min_max), downcast=downcast)\n+            tm.assert_equal(series.dtype, dtype)\n+\n+\n if __name__ == '__main__':\n     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],\n                    exit=False)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tools/tests/test_util.py"
            },
            {
                "id": 14481,
                "created_at": "2016-10-24T10:54:53Z",
                "closed_at": "2016-10-24T22:20:45Z",
                "title": "BUG: fix empty intersection of RangeIndex (GH14364)",
                "labels": "Bug, Indexing",
                "commits": [
                    {
                        "hash": "fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c",
                        "commit_date": "2016-10-24T22:19:06Z",
                        "parents": "13088842a7218e8e4626ab68f0c4f204f25f0ba4",
                        "stat": {
                            "total": 3,
                            "additions": 38,
                            "deletions": 35,
                            "files": [
                                {
                                    "sha": "d5fa2af5b0ff62567a00725cab5b46fac70ae72b",
                                    "filename": "doc/source/whatsnew/v0.19.1.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt?ref=fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c",
                                    "patch": "@@ -43,7 +43,7 @@ Bug Fixes\n - Bug in string indexing against data with ``object`` ``Index`` may raise ``AttributeError`` (:issue:`14424`)\n - Corrrecly raise ``ValueError`` on empty input to ``pd.eval()`` and ``df.query()`` (:issue:`13139`)\n \n-\n+- Bug in ``RangeIndex.intersection`` when result is a empty set (:issue:`14364`).\n \n - ``pd.merge()`` will raise ``ValueError`` with non-boolean parameters in passed boolean type arguments (:issue:`14434`)\n "
                                },
                                {
                                    "sha": "7a7902b503bd698d603f0dbe2ab697e86f409d01",
                                    "filename": "pandas/indexes/range.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 2,
                                    "changes": 7,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/pandas%2Findexes%2Frange.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/pandas%2Findexes%2Frange.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Frange.py?ref=fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c",
                                    "patch": "@@ -315,14 +315,17 @@ def intersection(self, other):\n         if not isinstance(other, RangeIndex):\n             return super(RangeIndex, self).intersection(other)\n \n+        if not len(self) or not len(other):\n+            return RangeIndex._simple_new(None)\n+\n         # check whether intervals intersect\n         # deals with in- and decreasing ranges\n         int_low = max(min(self._start, self._stop + 1),\n                       min(other._start, other._stop + 1))\n         int_high = min(max(self._stop, self._start + 1),\n                        max(other._stop, other._start + 1))\n         if int_high <= int_low:\n-            return RangeIndex()\n+            return RangeIndex._simple_new(None)\n \n         # Method hint: linear Diophantine equation\n         # solve intersection problem\n@@ -332,7 +335,7 @@ def intersection(self, other):\n \n         # check whether element sets intersect\n         if (self._start - other._start) % gcd:\n-            return RangeIndex()\n+            return RangeIndex._simple_new(None)\n \n         # calculate parameters for the RangeIndex describing the\n         # intersection disregarding the lower bounds"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "26d50aa55431f1b2d2b31d64da33fe76b565f8a9",
                                    "filename": "pandas/tests/indexes/test_range.py",
                                    "status": "modified",
                                    "additions": 29,
                                    "deletions": 0,
                                    "changes": 29,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/pandas%2Ftests%2Findexes%2Ftest_range.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c/pandas%2Ftests%2Findexes%2Ftest_range.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexes%2Ftest_range.py?ref=fe2ebc15d696f02dc3137c0d0c318c7bca6abb7c",
                                    "patch": "@@ -587,6 +587,35 @@ def test_intersection(self):\n                                                 other.values)))\n         self.assert_index_equal(result, expected)\n \n+        index = RangeIndex(5)\n+\n+        # intersect of non-overlapping indices\n+        other = RangeIndex(5, 10, 1)\n+        result = index.intersection(other)\n+        expected = RangeIndex(0, 0, 1)\n+        self.assert_index_equal(result, expected)\n+\n+        other = RangeIndex(-1, -5, -1)\n+        result = index.intersection(other)\n+        expected = RangeIndex(0, 0, 1)\n+        self.assert_index_equal(result, expected)\n+\n+        # intersection of empty indices\n+        other = RangeIndex(0, 0, 1)\n+        result = index.intersection(other)\n+        expected = RangeIndex(0, 0, 1)\n+        self.assert_index_equal(result, expected)\n+\n+        result = other.intersection(index)\n+        self.assert_index_equal(result, expected)\n+\n+        # intersection of non-overlapping values based on start value and gcd\n+        index = RangeIndex(1, 10, 2)\n+        other = RangeIndex(0, 10, 4)\n+        result = index.intersection(other)\n+        expected = RangeIndex(0, 0, 1)\n+        self.assert_index_equal(result, expected)\n+\n     def test_intersect_str_dates(self):\n         dt_dates = [datetime(2012, 2, 9), datetime(2012, 2, 22)]\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexes/test_range.py"
            },
            {
                "id": 14472,
                "created_at": "2016-10-22T00:47:20Z",
                "closed_at": "2016-10-25T10:47:51Z",
                "title": "BUG: downcast = 'unsigend' on 0 would would not downcast to unsigned.",
                "labels": "Bug, Dtype Conversions",
                "commits": [
                    {
                        "hash": "6ff53c2b47b026f605e415d3cd5f3b0dda7e0774",
                        "commit_date": "2016-10-25T10:46:58Z",
                        "parents": "2e77536bdf90ef20fefd4eab751447918e07668f",
                        "stat": {
                            "total": 1,
                            "additions": 49,
                            "deletions": 48,
                            "files": [
                                {
                                    "sha": "d1bb0ed4a69a16bfc179cc9cb02b84772e98533b",
                                    "filename": "doc/source/whatsnew/v0.19.1.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt?ref=6ff53c2b47b026f605e415d3cd5f3b0dda7e0774",
                                    "patch": "@@ -57,6 +57,7 @@ Bug Fixes\n \n - Bug in ``Timestamp`` where dates very near the minimum (1677-09) could underflow on creation (:issue:`14415`)\n \n+- Bug in ``pd.to_numeric`` where 0 was not included when ``downcast='unsigned'`` is passed (:issue:`14401`)\n - Bug in ``pd.concat`` where names of the ``keys`` were not propagated to the resulting ``MultiIndex`` (:issue:`14252`)\n - Bug in ``pd.concat`` where ``axis`` cannot take string parameters ``'rows'`` or ``'columns'`` (:issue:`14369`)\n - Bug in ``pd.concat`` with dataframes heterogeneous in length and tuple ``keys`` (:issue:`14438`)"
                                },
                                {
                                    "sha": "b50bf9dc448bccad46bce80477f220bcf3a67008",
                                    "filename": "pandas/tools/util.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/pandas%2Ftools%2Futil.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/pandas%2Ftools%2Futil.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Futil.py?ref=6ff53c2b47b026f605e415d3cd5f3b0dda7e0774",
                                    "patch": "@@ -205,7 +205,7 @@ def to_numeric(arg, errors='raise', downcast=None):\n \n         if downcast in ('integer', 'signed'):\n             typecodes = np.typecodes['Integer']\n-        elif downcast == 'unsigned' and np.min(values) > 0:\n+        elif downcast == 'unsigned' and np.min(values) >= 0:\n             typecodes = np.typecodes['UnsignedInteger']\n         elif downcast == 'float':\n             typecodes = np.typecodes['Float']"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "ddd408202bcfc117bacddad55b130abf2d18012e",
                                    "filename": "pandas/tools/tests/test_util.py",
                                    "status": "modified",
                                    "additions": 46,
                                    "deletions": 0,
                                    "changes": 46,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/pandas%2Ftools%2Ftests%2Ftest_util.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/6ff53c2b47b026f605e415d3cd5f3b0dda7e0774/pandas%2Ftools%2Ftests%2Ftest_util.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftests%2Ftest_util.py?ref=6ff53c2b47b026f605e415d3cd5f3b0dda7e0774",
                                    "patch": "@@ -401,6 +401,52 @@ def test_downcast(self):\n             res = pd.to_numeric(data, downcast=downcast)\n             tm.assert_numpy_array_equal(res, expected)\n \n+    def test_downcast_limits(self):\n+        # Test the limits of each downcast. #14401\n+        # uint64 is not fully supported ATM\n+        dtype_downcast_min_max = [\n+            ('int8', 'integer',\n+             [np.iinfo(np.int8).min, np.iinfo(np.int8).max]),\n+            ('int16', 'integer',\n+             [np.iinfo(np.int16).min, np.iinfo(np.int16).max]),\n+            ('int32', 'integer',\n+             [np.iinfo(np.int32).min, np.iinfo(np.int32).max]),\n+            ('int64', 'integer',\n+             [np.iinfo(np.int64).min, np.iinfo(np.int64).max]),\n+            ('uint8', 'unsigned',\n+             [np.iinfo(np.uint8).min, np.iinfo(np.uint8).max]),\n+            ('uint16', 'unsigned',\n+             [np.iinfo(np.uint16).min, np.iinfo(np.uint16).max]),\n+            ('uint32', 'unsigned',\n+             [np.iinfo(np.uint32).min, np.iinfo(np.uint32).max]),\n+            # ('uint64', 'unsigned',\n+            # [np.iinfo(np.uint64).min, np.iinfo(np.uint64).max]),\n+\n+            ('int16', 'integer',\n+             [np.iinfo(np.int8).min, np.iinfo(np.int8).max + 1]),\n+            ('int32', 'integer',\n+             [np.iinfo(np.int16).min, np.iinfo(np.int16).max + 1]),\n+            ('int64', 'integer',\n+             [np.iinfo(np.int32).min, np.iinfo(np.int32).max + 1]),\n+            ('int16', 'integer',\n+             [np.iinfo(np.int8).min - 1, np.iinfo(np.int16).max]),\n+            ('int32', 'integer',\n+             [np.iinfo(np.int16).min - 1, np.iinfo(np.int32).max]),\n+            ('int64', 'integer',\n+             [np.iinfo(np.int32).min - 1, np.iinfo(np.int64).max]),\n+            ('uint16', 'unsigned',\n+             [np.iinfo(np.uint8).min, np.iinfo(np.uint8).max + 1]),\n+            ('uint32', 'unsigned',\n+             [np.iinfo(np.uint16).min, np.iinfo(np.uint16).max + 1]),\n+            # ('uint64', 'unsigned',\n+            # [np.iinfo(np.uint32).min, np.iinfo(np.uint32).max + 1]),\n+        ]\n+\n+        for dtype, downcast, min_max in dtype_downcast_min_max:\n+            series = pd.to_numeric(pd.Series(min_max), downcast=downcast)\n+            tm.assert_equal(series.dtype, dtype)\n+\n+\n if __name__ == '__main__':\n     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],\n                    exit=False)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tools/tests/test_util.py"
            },
            {
                "id": 14466,
                "created_at": "2016-10-21T10:53:50Z",
                "closed_at": "2016-10-25T10:55:04Z",
                "title": "BUG: incorrect broadcasting that could casuse dtype coercion in a groupby-transform",
                "labels": "Bug, Groupby",
                "commits": [
                    {
                        "hash": "f99f050aaf29e4b7e9190488904c12bb719f8210",
                        "commit_date": "2016-10-25T10:52:36Z",
                        "parents": "48520083ae27eefb9a918b430523151df9166704",
                        "stat": {
                            "total": 6,
                            "additions": 37,
                            "deletions": 31,
                            "files": [
                                {
                                    "sha": "3256e017b4df48f47407ac61e088c6ec5e6b388a",
                                    "filename": "doc/source/whatsnew/v0.19.1.txt",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f99f050aaf29e4b7e9190488904c12bb719f8210/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f99f050aaf29e4b7e9190488904c12bb719f8210/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt?ref=f99f050aaf29e4b7e9190488904c12bb719f8210",
                                    "patch": "@@ -43,9 +43,13 @@ Bug Fixes\n - Bug in string indexing against data with ``object`` ``Index`` may raise ``AttributeError`` (:issue:`14424`)\n - Corrrecly raise ``ValueError`` on empty input to ``pd.eval()`` and ``df.query()`` (:issue:`13139`)\n \n+\n - Bug in ``RangeIndex.intersection`` when result is a empty set (:issue:`14364`).\n - Bug in union of differences from a ``DatetimeIndex`; this is a regression in 0.19.0 from 0.18.1 (:issue:`14323`)\n \n+- Bug in groupby-transform broadcasting that could cause incorrect dtype coercion (:issue:`14457`)\n+\n+\n - Bug in ``Series.__setitem__`` which allowed mutating read-only arrays (:issue:`14359`).\n \n "
                                },
                                {
                                    "sha": "2a7f896e1b871c376e603836824804f64384688a",
                                    "filename": "pandas/core/groupby.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 6,
                                    "changes": 21,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f99f050aaf29e4b7e9190488904c12bb719f8210/pandas%2Fcore%2Fgroupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f99f050aaf29e4b7e9190488904c12bb719f8210/pandas%2Fcore%2Fgroupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fgroupby.py?ref=f99f050aaf29e4b7e9190488904c12bb719f8210",
                                    "patch": "@@ -3460,7 +3460,6 @@ def _transform_general(self, func, *args, **kwargs):\n         from pandas.tools.merge import concat\n \n         applied = []\n-\n         obj = self._obj_with_exclusions\n         gen = self.grouper.get_iterator(obj, axis=self.axis)\n         fast_path, slow_path = self._define_paths(func, *args, **kwargs)\n@@ -3481,14 +3480,24 @@ def _transform_general(self, func, *args, **kwargs):\n             else:\n                 res = path(group)\n \n-            # broadcasting\n             if isinstance(res, Series):\n-                if res.index.is_(obj.index):\n-                    group.T.values[:] = res\n+\n+                # we need to broadcast across the\n+                # other dimension; this will preserve dtypes\n+                # GH14457\n+                if not np.prod(group.shape):\n+                    continue\n+                elif res.index.is_(obj.index):\n+                    r = concat([res] * len(group.columns), axis=1)\n+                    r.columns = group.columns\n+                    r.index = group.index\n                 else:\n-                    group.values[:] = res\n+                    r = DataFrame(\n+                        np.concatenate([res.values] * len(group.index)\n+                                       ).reshape(group.shape),\n+                        columns=group.columns, index=group.index)\n \n-                applied.append(group)\n+                applied.append(r)\n             else:\n                 applied.append(res)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "dc326aeaa88ac7ca329f703bf108e72e897fbad1",
                                    "filename": "pandas/tests/test_groupby.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 0,
                                    "changes": 12,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/f99f050aaf29e4b7e9190488904c12bb719f8210/pandas%2Ftests%2Ftest_groupby.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/f99f050aaf29e4b7e9190488904c12bb719f8210/pandas%2Ftests%2Ftest_groupby.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_groupby.py?ref=f99f050aaf29e4b7e9190488904c12bb719f8210",
                                    "patch": "@@ -1366,6 +1366,18 @@ def nsum(x):\n         for result in results:\n             assert_series_equal(result, expected, check_names=False)\n \n+    def test_transform_coercion(self):\n+\n+        # 14457\n+        # when we are transforming be sure to not coerce\n+        # via assignment\n+        df = pd.DataFrame(dict(A=['a', 'a'], B=[0, 1]))\n+        g = df.groupby('A')\n+\n+        expected = g.transform(np.mean)\n+        result = g.transform(lambda x: np.mean(x))\n+        assert_frame_equal(result, expected)\n+\n     def test_with_na(self):\n         index = Index(np.arange(10))\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_groupby.py"
            },
            {
                "id": 14416,
                "created_at": "2016-10-13T14:37:04Z",
                "closed_at": "2016-10-15T19:59:53Z",
                "title": "Concat with axis rows",
                "labels": "Bug, Reshaping",
                "commits": [
                    {
                        "hash": "fd3be00bc46b416437e8cfafcf5661ec57385e2f",
                        "commit_date": "2016-10-15T19:59:16Z",
                        "parents": "750b6aebf5fb4eef0be7aca67d7ceb97ef503e32",
                        "stat": {
                            "total": 1,
                            "additions": 68,
                            "deletions": 67,
                            "files": [
                                {
                                    "sha": "fd69103c28ef52b5aab9ca60517e8f77ce2bb548",
                                    "filename": "doc/source/whatsnew/v0.19.1.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fd3be00bc46b416437e8cfafcf5661ec57385e2f/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fd3be00bc46b416437e8cfafcf5661ec57385e2f/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt?ref=fd3be00bc46b416437e8cfafcf5661ec57385e2f",
                                    "patch": "@@ -44,6 +44,7 @@ Bug Fixes\n \n \n - Bug in ``pd.concat`` where names of the ``keys`` were not propagated to the resulting ``MultiIndex`` (:issue:`14252`)\n+- Bug in ``pd.concat`` where ``axis`` cannot take string parameters ``'rows'`` or ``'columns'`` (:issue:`14369`)\n - Bug in ``MultiIndex.set_levels`` where illegal level values were still set after raising an error (:issue:`13754`)\n - Bug in ``DataFrame.to_json`` where ``lines=True`` and a value contained a ``}`` character (:issue:`14391`)\n - Bug in ``df.groupby`` causing an ``AttributeError`` when grouping a single index frame by a column and the index level (:issue`14327`)"
                                },
                                {
                                    "sha": "ce7f8908d75064f755bd919a5ebdfb2b9edef404",
                                    "filename": "pandas/tools/merge.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 1,
                                    "changes": 8,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fd3be00bc46b416437e8cfafcf5661ec57385e2f/pandas%2Ftools%2Fmerge.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fd3be00bc46b416437e8cfafcf5661ec57385e2f/pandas%2Ftools%2Fmerge.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Fmerge.py?ref=fd3be00bc46b416437e8cfafcf5661ec57385e2f",
                                    "patch": "@@ -1283,7 +1283,7 @@ def concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n         argument, unless it is passed, in which case the values will be\n         selected (see below). Any None objects will be dropped silently unless\n         they are all None in which case a ValueError will be raised\n-    axis : {0, 1, ...}, default 0\n+    axis : {0/'index', 1/'columns'}, default 0\n         The axis to concatenate along\n     join : {'inner', 'outer'}, default 'outer'\n         How to handle indexes on other axis(es)\n@@ -1411,6 +1411,12 @@ def __init__(self, objs, axis=0, join='outer', join_axes=None,\n             sample = objs[0]\n         self.objs = objs\n \n+        # Standardize axis parameter to int\n+        if isinstance(sample, Series):\n+            axis = DataFrame()._get_axis_number(axis)\n+        else:\n+            axis = sample._get_axis_number(axis)\n+\n         # Need to flip BlockManager axis in the DataFrame special case\n         self._is_frame = isinstance(sample, DataFrame)\n         if self._is_frame:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "81aa694577fb5dad01c92e61dfe1dabd45e79fcb",
                                    "filename": "pandas/tests/frame/test_combine_concat.py",
                                    "status": "modified",
                                    "additions": 59,
                                    "deletions": 0,
                                    "changes": 59,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/fd3be00bc46b416437e8cfafcf5661ec57385e2f/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/fd3be00bc46b416437e8cfafcf5661ec57385e2f/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Fframe%2Ftest_combine_concat.py?ref=fd3be00bc46b416437e8cfafcf5661ec57385e2f",
                                    "patch": "@@ -347,6 +347,65 @@ def test_concat_named_keys(self):\n                                              names=[None, None]))\n         assert_frame_equal(concatted_unnamed, expected_unnamed)\n \n+    def test_concat_axis_parameter(self):\n+        # GH 14369\n+        df1 = pd.DataFrame({'A': [0.1, 0.2]}, index=range(2))\n+        df2 = pd.DataFrame({'A': [0.3, 0.4]}, index=range(2))\n+\n+        # Index/row/0 DataFrame\n+        expected_index = pd.DataFrame(\n+            {'A': [0.1, 0.2, 0.3, 0.4]}, index=[0, 1, 0, 1])\n+\n+        concatted_index = pd.concat([df1, df2], axis='index')\n+        assert_frame_equal(concatted_index, expected_index)\n+\n+        concatted_row = pd.concat([df1, df2], axis='rows')\n+        assert_frame_equal(concatted_row, expected_index)\n+\n+        concatted_0 = pd.concat([df1, df2], axis=0)\n+        assert_frame_equal(concatted_0, expected_index)\n+\n+        # Columns/1 DataFrame\n+        expected_columns = pd.DataFrame(\n+            [[0.1, 0.3], [0.2, 0.4]], index=[0, 1], columns=['A', 'A'])\n+\n+        concatted_columns = pd.concat([df1, df2], axis='columns')\n+        assert_frame_equal(concatted_columns, expected_columns)\n+\n+        concatted_1 = pd.concat([df1, df2], axis=1)\n+        assert_frame_equal(concatted_1, expected_columns)\n+\n+        series1 = pd.Series([0.1, 0.2])\n+        series2 = pd.Series([0.3, 0.4])\n+\n+        # Index/row/0 Series\n+        expected_index_series = pd.Series(\n+            [0.1, 0.2, 0.3, 0.4], index=[0, 1, 0, 1])\n+\n+        concatted_index_series = pd.concat([series1, series2], axis='index')\n+        assert_series_equal(concatted_index_series, expected_index_series)\n+\n+        concatted_row_series = pd.concat([series1, series2], axis='rows')\n+        assert_series_equal(concatted_row_series, expected_index_series)\n+\n+        concatted_0_series = pd.concat([series1, series2], axis=0)\n+        assert_series_equal(concatted_0_series, expected_index_series)\n+\n+        # Columns/1 Series\n+        expected_columns_series = pd.DataFrame(\n+            [[0.1, 0.3], [0.2, 0.4]], index=[0, 1], columns=[0, 1])\n+\n+        concatted_columns_series = pd.concat(\n+            [series1, series2], axis='columns')\n+        assert_frame_equal(concatted_columns_series, expected_columns_series)\n+\n+        concatted_1_series = pd.concat([series1, series2], axis=1)\n+        assert_frame_equal(concatted_1_series, expected_columns_series)\n+\n+        # Testing ValueError\n+        with assertRaisesRegexp(ValueError, 'No axis named'):\n+            pd.concat([series1, series2], axis='something')\n+\n \n class TestDataFrameCombineFirst(tm.TestCase, TestData):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/frame/test_combine_concat.py"
            },
            {
                "id": 14390,
                "created_at": "2016-10-10T22:34:52Z",
                "closed_at": "2016-10-15T13:38:33Z",
                "title": "BUG: Line delimited json is breaks if string includes `}`",
                "labels": "Bug, IO JSON",
                "commits": [
                    {
                        "hash": "286b9b94d97fb2256d228a8970e92eee2ac3078a",
                        "commit_date": "2016-10-15T13:37:20Z",
                        "parents": "7d40f18718ba80e8b8d706eaed2fc4c098686e71",
                        "stat": {
                            "total": 4,
                            "additions": 45,
                            "deletions": 41,
                            "files": [
                                {
                                    "sha": "5419571c75b43a577f1614f3f84a0eaf19a47f9c",
                                    "filename": "asv_bench/benchmarks/packers.py",
                                    "status": "modified",
                                    "additions": 25,
                                    "deletions": 0,
                                    "changes": 25,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/286b9b94d97fb2256d228a8970e92eee2ac3078a/asv_bench%2Fbenchmarks%2Fpackers.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/286b9b94d97fb2256d228a8970e92eee2ac3078a/asv_bench%2Fbenchmarks%2Fpackers.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/asv_bench%2Fbenchmarks%2Fpackers.py?ref=286b9b94d97fb2256d228a8970e92eee2ac3078a",
                                    "patch": "@@ -547,6 +547,31 @@ def remove(self, f):\n             pass\n \n \n+class packers_write_json_lines(object):\n+    goal_time = 0.2\n+\n+    def setup(self):\n+        self.f = '__test__.msg'\n+        self.N = 100000\n+        self.C = 5\n+        self.index = date_range('20000101', periods=self.N, freq='H')\n+        self.df = DataFrame(dict([('float{0}'.format(i), randn(self.N)) for i in range(self.C)]), index=self.index)\n+        self.remove(self.f)\n+        self.df.index = np.arange(self.N)\n+\n+    def time_packers_write_json_lines(self):\n+        self.df.to_json(self.f, orient=\"records\", lines=True)\n+\n+    def teardown(self):\n+        self.remove(self.f)\n+\n+    def remove(self, f):\n+        try:\n+            os.remove(self.f)\n+        except:\n+            pass\n+\n+\n class packers_write_json_T(object):\n     goal_time = 0.2\n "
                                },
                                {
                                    "sha": "5c03408cbf20ff1a1f98f1bee1effdde6a5ffa44",
                                    "filename": "doc/source/whatsnew/v0.19.1.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/286b9b94d97fb2256d228a8970e92eee2ac3078a/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/286b9b94d97fb2256d228a8970e92eee2ac3078a/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.1.txt?ref=286b9b94d97fb2256d228a8970e92eee2ac3078a",
                                    "patch": "@@ -45,3 +45,4 @@ Bug Fixes\n \n - Bug in ``pd.concat`` where names of the ``keys`` were not propagated to the resulting ``MultiIndex`` (:issue:`14252`)\n - Bug in ``MultiIndex.set_levels`` where illegal level values were still set after raising an error (:issue:`13754`)\n+- Bug in ``DataFrame.to_json`` where ``lines=True`` and a value contained a ``}`` character (:issue:`14391`)"
                                },
                                {
                                    "sha": "66a8e76c09a6f3f8c7cfcffe702f489d8bda9084",
                                    "filename": "pandas/io/json.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 4,
                                    "changes": 13,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/286b9b94d97fb2256d228a8970e92eee2ac3078a/pandas%2Fio%2Fjson.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/286b9b94d97fb2256d228a8970e92eee2ac3078a/pandas%2Fio%2Fjson.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fio%2Fjson.py?ref=286b9b94d97fb2256d228a8970e92eee2ac3078a",
                                    "patch": "@@ -607,14 +607,19 @@ def _convert_to_line_delimits(s):\n     s = s[1:-1]\n     num_open_brackets_seen = 0\n     commas_to_replace = []\n+    in_quotes = False\n     for idx, char in enumerate(s):              # iter through to find all\n-        if char == ',':                         # commas that should be \\n\n-            if num_open_brackets_seen == 0:\n+        if char == '\"' and idx > 0 and s[idx - 1] != '\\\\':\n+            in_quotes = ~in_quotes\n+        elif char == ',':                         # commas that should be \\n\n+            if num_open_brackets_seen == 0 and not in_quotes:\n                 commas_to_replace.append(idx)\n         elif char == '{':\n-            num_open_brackets_seen += 1\n+            if not in_quotes:\n+                num_open_brackets_seen += 1\n         elif char == '}':\n-            num_open_brackets_seen -= 1\n+            if not in_quotes:\n+                num_open_brackets_seen -= 1\n     s_arr = np.array(list(s))                  # Turn to an array to set\n     s_arr[commas_to_replace] = '\\n'            # all commas at once.\n     s = ''.join(s_arr)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "117ac2324d0e0daa4376dd83425829af2719a73a",
                                    "filename": "pandas/io/tests/json/test_pandas.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/286b9b94d97fb2256d228a8970e92eee2ac3078a/pandas%2Fio%2Ftests%2Fjson%2Ftest_pandas.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/286b9b94d97fb2256d228a8970e92eee2ac3078a/pandas%2Fio%2Ftests%2Fjson%2Ftest_pandas.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fio%2Ftests%2Fjson%2Ftest_pandas.py?ref=286b9b94d97fb2256d228a8970e92eee2ac3078a",
                                    "patch": "@@ -962,6 +962,12 @@ def test_to_jsonl(self):\n         expected = '{\"a\":1,\"b\":2}\\n{\"a\":1,\"b\":2}'\n         self.assertEqual(result, expected)\n \n+        df = DataFrame([[\"foo}\", \"bar\"], ['foo\"', \"bar\"]], columns=['a', 'b'])\n+        result = df.to_json(orient=\"records\", lines=True)\n+        expected = '{\"a\":\"foo}\",\"b\":\"bar\"}\\n{\"a\":\"foo\\\\\"\",\"b\":\"bar\"}'\n+        self.assertEqual(result, expected)\n+        assert_frame_equal(pd.read_json(result, lines=True), df)\n+\n     def test_latin_encoding(self):\n         if compat.PY2:\n             self.assertRaisesRegexp("
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/io/tests/json/test_pandas.py"
            },
            {
                "id": 13695,
                "created_at": "2016-07-18T18:38:35Z",
                "closed_at": "2016-07-19T01:16:41Z",
                "title": "pd.merge_asof() matches out of tolerance when allow_exact_matches=False",
                "labels": "Bug, Reshaping",
                "commits": [
                    {
                        "hash": "b05453631270d4b78f79dc272222d5f3fe499ad7",
                        "commit_date": "2016-07-19T01:15:05Z",
                        "parents": "9f635cd74316d26110809bf1bb2a5525ac4d23fe",
                        "stat": {
                            "total": 9,
                            "additions": 53,
                            "deletions": 44,
                            "files": [
                                {
                                    "sha": "e728cb7910134f3b35052598e995fd0a70b62f1d",
                                    "filename": "doc/source/whatsnew/v0.19.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b05453631270d4b78f79dc272222d5f3fe499ad7/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b05453631270d4b78f79dc272222d5f3fe499ad7/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt?ref=b05453631270d4b78f79dc272222d5f3fe499ad7",
                                    "patch": "@@ -46,7 +46,7 @@ The following are now part of this API:\n ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n \n A long-time requested feature has been added through the :func:`merge_asof` function, to\n-support asof style joining of time-series. (:issue:`1870`). Full documentation is\n+support asof style joining of time-series. (:issue:`1870`, :issue:`13695`). Full documentation is\n :ref:`here <merging.merge_asof>`\n \n The :func:`merge_asof` performs an asof merge, which is similar to a left-join"
                                },
                                {
                                    "sha": "ad3b1d4e4a90ef775782c65ba8b3b6708e1c98e5",
                                    "filename": "pandas/src/join.pyx",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 8,
                                    "changes": 18,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b05453631270d4b78f79dc272222d5f3fe499ad7/pandas%2Fsrc%2Fjoin.pyx",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b05453631270d4b78f79dc272222d5f3fe499ad7/pandas%2Fsrc%2Fjoin.pyx",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fsrc%2Fjoin.pyx?ref=b05453631270d4b78f79dc272222d5f3fe499ad7",
                                    "patch": "@@ -193,11 +193,12 @@ def left_outer_asof_join(ndarray[int64_t] left, ndarray[int64_t] right,\n                         diff = left_val - right_val\n \n                         # do we allow exact matches\n-                        if allow_exact_matches and diff > tol:\n-                            right_indexer[indexer] = -1\n-                            continue\n+                        if allow_exact_matches:\n+                            if diff > tol:\n+                                right_indexer[indexer] = -1\n+                                continue\n                         elif not allow_exact_matches:\n-                            if diff >= tol:\n+                            if diff >= tol or lc == rc:\n                                 right_indexer[indexer] = -1\n                                 continue\n \n@@ -220,13 +221,14 @@ def left_outer_asof_join(ndarray[int64_t] left, ndarray[int64_t] right,\n                         diff = left_val - right_val\n \n                         # do we allow exact matches\n-                        if allow_exact_matches and diff > tol:\n-                            right_indexer[indexer] = -1\n-                            continue\n+                        if allow_exact_matches:\n+                            if diff > tol:\n+                                right_indexer[indexer] = -1\n+                                continue\n \n                         # we don't allow exact matches\n                         elif not allow_exact_matches:\n-                            if diff >= tol or not right_pos:\n+                            if diff >= tol or lc == rc:\n                                 right_indexer[indexer] = -1\n                             else:\n                                 right_indexer[indexer] = right_pos - 1"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "bcbb0f0fadb4931d3bf729f473f004d0aa90aeb9",
                                    "filename": "pandas/tools/tests/test_merge_asof.py",
                                    "status": "modified",
                                    "additions": 33,
                                    "deletions": 0,
                                    "changes": 33,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/b05453631270d4b78f79dc272222d5f3fe499ad7/pandas%2Ftools%2Ftests%2Ftest_merge_asof.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/b05453631270d4b78f79dc272222d5f3fe499ad7/pandas%2Ftools%2Ftests%2Ftest_merge_asof.py",
                                    "patch": "@@ -347,6 +347,39 @@ def test_allow_exact_matches_and_tolerance(self):\n         expected = self.allow_exact_matches_and_tolerance\n         assert_frame_equal(result, expected)\n \n+    def test_allow_exact_matches_and_tolerance2(self):\n+        # GH 13695\n+        df1 = pd.DataFrame({\n+            'time': pd.to_datetime(['2016-07-15 13:30:00.030']),\n+            'username': ['bob']})\n+        df2 = pd.DataFrame({\n+            'time': pd.to_datetime(['2016-07-15 13:30:00.000',\n+                                    '2016-07-15 13:30:00.030']),\n+            'version': [1, 2]})\n+\n+        result = pd.merge_asof(df1, df2, on='time')\n+        expected = pd.DataFrame({\n+            'time': pd.to_datetime(['2016-07-15 13:30:00.030']),\n+            'username': ['bob'],\n+            'version': [2]})\n+        assert_frame_equal(result, expected)\n+\n+        result = pd.merge_asof(df1, df2, on='time', allow_exact_matches=False)\n+        expected = pd.DataFrame({\n+            'time': pd.to_datetime(['2016-07-15 13:30:00.030']),\n+            'username': ['bob'],\n+            'version': [1]})\n+        assert_frame_equal(result, expected)\n+\n+        result = pd.merge_asof(df1, df2, on='time', allow_exact_matches=False,\n+                               tolerance=pd.Timedelta('10ms'))\n+        expected = pd.DataFrame({\n+            'time': pd.to_datetime(['2016-07-15 13:30:00.030']),\n+            'username': ['bob'],\n+            'version': [np.nan]})\n+        assert_frame_equal(result, expected)\n+\n+\n if __name__ == '__main__':\n     nose.runmodule(argv=[__file__, '-vvs', '-x', '--pdb', '--pdb-failure'],\n                    exit=False)",
                                    "true": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftools%2Ftests%2Ftest_merge_asof.py?ref=b05453631270d4b78f79dc272222d5f3fe499ad7"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tools/tests/test_merge_asof.py"
            },
            {
                "id": 13598,
                "created_at": "2016-07-09T15:22:08Z",
                "closed_at": "2016-07-19T01:11:18Z",
                "title": "Series.str.zfill() doesn't check type",
                "labels": "Bug, Error Reporting, Strings",
                "commits": [
                    {
                        "hash": "5a521713f3892539b648bc2735d3cc502feb2b48",
                        "commit_date": "2016-07-19T01:10:19Z",
                        "parents": "694fe61f931e1c0f034f93f3e0f1084a8974a1f3",
                        "stat": {
                            "total": 2,
                            "additions": 18,
                            "deletions": 16,
                            "files": [
                                {
                                    "sha": "99396f6cfbc89623665ee049a7d7e5117adbf476",
                                    "filename": "doc/source/whatsnew/v0.19.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/5a521713f3892539b648bc2735d3cc502feb2b48/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/5a521713f3892539b648bc2735d3cc502feb2b48/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt?ref=5a521713f3892539b648bc2735d3cc502feb2b48",
                                    "patch": "@@ -583,7 +583,7 @@ Bug Fixes\n - Bug in ``pd.read_csv()`` with ``engine=='c'`` in which null ``quotechar`` was not accepted even though ``quoting`` was specified as ``None`` (:issue:`13411`)\n - Bug in ``pd.read_csv()`` with ``engine=='c'`` in which fields were not properly cast to float when quoting was specified as non-numeric (:issue:`13411`)\n - Bug in ``pd.pivot_table()`` where ``margins_name`` is ignored when ``aggfunc`` is a list (:issue:`13354`)\n-\n+- Bug in ``pd.Series.str.zfill``, ``center``, ``ljust``, ``rjust``, and ``pad`` when passing non-integers, did not raise ``TypeError`` (:issue:`13598`)\n \n \n - Bug in ``Series`` arithmetic raises ``TypeError`` if it contains datetime-like as ``object`` dtype (:issue:`13043`)"
                                },
                                {
                                    "sha": "3150fc5d0143a74617da80f2f155e9911f589c17",
                                    "filename": "pandas/core/strings.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 1,
                                    "changes": 7,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/5a521713f3892539b648bc2735d3cc502feb2b48/pandas%2Fcore%2Fstrings.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/5a521713f3892539b648bc2735d3cc502feb2b48/pandas%2Fcore%2Fstrings.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Fcore%2Fstrings.py?ref=5a521713f3892539b648bc2735d3cc502feb2b48",
                                    "patch": "@@ -8,7 +8,8 @@\n                                  is_object_dtype,\n                                  is_string_like,\n                                  is_list_like,\n-                                 is_scalar)\n+                                 is_scalar,\n+                                 is_integer)\n from pandas.core.common import _values_from_object\n \n from pandas.core.algorithms import take_1d\n@@ -914,6 +915,10 @@ def str_pad(arr, width, side='left', fillchar=' '):\n     if len(fillchar) != 1:\n         raise TypeError('fillchar must be a character, not str')\n \n+    if not is_integer(width):\n+        msg = 'width must be of integer type, not {0}'\n+        raise TypeError(msg.format(type(width).__name__))\n+\n     if side == 'left':\n         f = lambda x: x.rjust(width, fillchar)\n     elif side == 'right':"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "fcdbec8fbc5c432e5ff48c84c988c4c47dd9dab5",
                                    "filename": "pandas/tests/test_strings.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/5a521713f3892539b648bc2735d3cc502feb2b48/pandas%2Ftests%2Ftest_strings.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/5a521713f3892539b648bc2735d3cc502feb2b48/pandas%2Ftests%2Ftest_strings.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Ftest_strings.py?ref=5a521713f3892539b648bc2735d3cc502feb2b48",
                                    "patch": "@@ -1603,6 +1603,15 @@ def test_pad_fillchar(self):\n                                    \"fillchar must be a character, not int\"):\n             result = values.str.pad(5, fillchar=5)\n \n+    def test_pad_width(self):\n+        # GH 13598\n+        s = Series(['1', '22', 'a', 'bb'])\n+\n+        for f in ['center', 'ljust', 'rjust', 'zfill', 'pad']:\n+            with tm.assertRaisesRegexp(TypeError,\n+                                       \"width must be of integer type, not*\"):\n+                getattr(s.str, f)('f')\n+\n     def test_translate(self):\n \n         def _check(result, expected):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/test_strings.py"
            },
            {
                "id": 13509,
                "created_at": "2016-06-24T22:02:53Z",
                "closed_at": "2016-08-26T20:21:02Z",
                "title": "BUG: Float64Index getitem raises error with tuple-like values",
                "labels": "Bug, Indexing",
                "commits": [
                    {
                        "hash": "e31f981d23c932c1679e7a893b01d7f6d936af5a",
                        "commit_date": "2016-08-26T20:16:41Z",
                        "parents": "042b6f00ad691345812e61bb7e86e52476805602",
                        "stat": {
                            "total": 9,
                            "additions": 22,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "0ff63117ed1219216a12e78fc8fb409f6cb72354",
                                    "filename": "doc/source/whatsnew/v0.19.0.txt",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/e31f981d23c932c1679e7a893b01d7f6d936af5a/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/e31f981d23c932c1679e7a893b01d7f6d936af5a/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/doc%2Fsource%2Fwhatsnew%2Fv0.19.0.txt?ref=e31f981d23c932c1679e7a893b01d7f6d936af5a",
                                    "patch": "@@ -1150,6 +1150,7 @@ Bug Fixes\n - Bug in ``DatetimeTZDtype`` dtype with ``dateutil.tz.tzlocal`` cannot be regarded as valid dtype (:issue:`13583`)\n - Bug in ``pd.read_hdf()`` where attempting to load an HDF file with a single dataset, that had one or more categorical columns, failed unless the key argument was set to the name of the dataset. (:issue:`13231`)\n - Bug in ``.rolling()`` that allowed a negative integer window in contruction of the ``Rolling()`` object, but would later fail on aggregation (:issue:`13383`)\n+- Bug in ``Series`` indexing with tuple-valued data and a numeric index (:issue:`13509`)\n \n - Bug in printing ``pd.DataFrame`` where unusual elements with the ``object`` dtype were causing segfaults (:issue:`13717`)\n - Bug in ranking ``Series`` which could result in segfaults (:issue:`13445`)"
                                },
                                {
                                    "sha": "e1ac0939812f69e82bb4e125dadd7ba827ca8020",
                                    "filename": "pandas/indexes/numeric.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 9,
                                    "changes": 10,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/e31f981d23c932c1679e7a893b01d7f6d936af5a/pandas%2Findexes%2Fnumeric.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/e31f981d23c932c1679e7a893b01d7f6d936af5a/pandas%2Findexes%2Fnumeric.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Findexes%2Fnumeric.py?ref=e31f981d23c932c1679e7a893b01d7f6d936af5a",
                                    "patch": "@@ -293,19 +293,11 @@ def get_value(self, series, key):\n         if not is_scalar(key):\n             raise InvalidIndexError\n \n-        from pandas.core.indexing import maybe_droplevels\n-        from pandas.core.series import Series\n-\n         k = _values_from_object(key)\n         loc = self.get_loc(k)\n         new_values = _values_from_object(series)[loc]\n \n-        if is_scalar(new_values) or new_values is None:\n-            return new_values\n-\n-        new_index = self[loc]\n-        new_index = maybe_droplevels(new_index, k)\n-        return Series(new_values, index=new_index, name=series.name)\n+        return new_values\n \n     def equals(self, other):\n         \"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "920aefa24b576ff3de2afc740abf5465623894f2",
                                    "filename": "pandas/tests/indexing/test_floats.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/pandas-dev/pandas/blob/e31f981d23c932c1679e7a893b01d7f6d936af5a/pandas%2Ftests%2Findexing%2Ftest_floats.py",
                                    "raw_url": "https://github.com/pandas-dev/pandas/raw/e31f981d23c932c1679e7a893b01d7f6d936af5a/pandas%2Ftests%2Findexing%2Ftest_floats.py",
                                    "contents_url": "https://api.github.com/repos/pandas-dev/pandas/contents/pandas%2Ftests%2Findexing%2Ftest_floats.py?ref=e31f981d23c932c1679e7a893b01d7f6d936af5a",
                                    "patch": "@@ -676,3 +676,14 @@ def test_floating_misc(self):\n         assert_series_equal(result1, result2)\n         assert_series_equal(result1, result3)\n         assert_series_equal(result1, Series([1], index=[2.5]))\n+\n+    def test_floating_tuples(self):\n+        # GH13509\n+        s = Series([(1, 1), (2, 2), (3, 3)], index=[0.0, 0.1, 0.2], name='foo')\n+        result = s[0.0]\n+        self.assertEqual(result, (1, 1))\n+\n+        s = Series([(1, 1), (2, 2), (3, 3)], index=[0.0, 0.0, 0.2], name='foo')\n+        result = s[0.0]\n+        expected = Series([(1, 1), (2, 2)], index=[0.0, 0.0], name='foo')\n+        assert_series_equal(result, expected)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest pandas/tests/indexing/test_floats.py"
            }
        ],
        "installSteps": "pipenv --python 3.8\npipenv install setuptools==39.1.0\npipenv install numpy==1.16.0\npipenv install cython==0.29.36\npipenv install pytz python-dateutil nose py pytest\npipenv run make clean\npipenv run make develop"
    },
    {
        "_id": "648981e2d97326f515359962",
        "username": "scrapy",
        "repository": "scrapy",
        "issues": [
            {
                "id": 2552,
                "created_at": "2017-02-09T11:24:35Z",
                "closed_at": "2019-11-19T08:50:12Z",
                "title": "scrapy.Request no init error on invalid url",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a",
                        "commit_date": "2019-10-22T13:48:02Z",
                        "parents": "1d5c270ce8caf954ce83c8db262e2a35707e0c5e",
                        "stat": {
                            "total": 1,
                            "additions": 4,
                            "deletions": 3,
                            "files": [
                                {
                                    "sha": "76a428199ad4ad919b8b1eb6caeec6880e446af6",
                                    "filename": "scrapy/http/request/__init__.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a/scrapy%2Fhttp%2Frequest%2F__init__.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a/scrapy%2Fhttp%2Frequest%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/scrapy%2Fhttp%2Frequest%2F__init__.py?ref=f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a",
                                    "patch": "@@ -66,7 +66,7 @@ def _set_url(self, url):\n         s = safe_url_string(url, self.encoding)\n         self._url = escape_ajax(s)\n \n-        if ':' not in self._url:\n+        if ('://' not in self._url) and (not self._url.startswith('data:')):\n             raise ValueError('Missing scheme in request url: %s' % self._url)\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "64f1184c356eed21946b01c17bd4ff6006b13d3c",
                                    "filename": "tests/test_http_request.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a/tests%2Ftest_http_request.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a/tests%2Ftest_http_request.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/tests%2Ftest_http_request.py?ref=f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a",
                                    "patch": "@@ -52,6 +52,8 @@ def test_init(self):\n \n     def test_url_no_scheme(self):\n         self.assertRaises(ValueError, self.request_class, 'foo')\n+        self.assertRaises(ValueError, self.request_class, '/foo/')\n+        self.assertRaises(ValueError, self.request_class, '/foo:bar')\n \n     def test_headers(self):\n         # Different ways of setting headers attribute"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest tests/test_http_request.py "
            },
            {
                "id": 1265,
                "created_at": "2015-05-29T14:14:45Z",
                "closed_at": "2015-06-01T23:31:53Z",
                "title": "Backward incompatibility for relocated paths in settings",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d",
                        "commit_date": "2015-06-01T23:31:46Z",
                        "parents": "bd2fe996aabe5467a89d9eff6831e6b0bf731be4",
                        "stat": {
                            "total": 9,
                            "additions": 116,
                            "deletions": 107,
                            "files": [
                                {
                                    "sha": "423ca43e25a7cd41757b0c8af56a01e117f18906",
                                    "filename": "scrapy/utils/conf.py",
                                    "status": "modified",
                                    "additions": 18,
                                    "deletions": 4,
                                    "changes": 22,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/scrapy%2Futils%2Fconf.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/scrapy%2Futils%2Fconf.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/scrapy%2Futils%2Fconf.py?ref=d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d",
                                    "patch": "@@ -5,16 +5,30 @@\n import six\n from six.moves.configparser import SafeConfigParser\n \n+from scrapy.utils.deprecate import update_classpath\n \n-def build_component_list(base, custom):\n+\n+def build_component_list(base, custom, convert=update_classpath):\n     \"\"\"Compose a component list based on a custom and base dict of components\n     (typically middlewares or extensions), unless custom is already a list, in\n     which case it's returned.\n     \"\"\"\n+\n+    def _check_components(complist):\n+        if len({convert(c) for c in complist}) != len(complist):\n+            raise ValueError('Some paths in {!r} convert to the same object, '\n+                             'please update your settings'.format(complist))\n+\n     if isinstance(custom, (list, tuple)):\n-        return custom\n-    compdict = base.copy()\n-    compdict.update(custom)\n+        _check_components(custom)\n+        return type(custom)(convert(c) for c in custom)\n+\n+    def _map_keys(compdict):\n+        _check_components(compdict)\n+        return {convert(k): v for k, v in six.iteritems(compdict)}\n+\n+    compdict = _map_keys(base)\n+    compdict.update(_map_keys(custom))\n     items = (x for x in six.iteritems(compdict) if x[1] is not None)\n     return [x[0] for x in sorted(items, key=itemgetter(1))]\n "
                                },
                                {
                                    "sha": "37e94fae946b59e7d6959989cc4a81cbde8ba426",
                                    "filename": "scrapy/utils/deprecate.py",
                                    "status": "modified",
                                    "additions": 34,
                                    "deletions": 0,
                                    "changes": 34,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/scrapy%2Futils%2Fdeprecate.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/scrapy%2Futils%2Fdeprecate.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/scrapy%2Futils%2Fdeprecate.py?ref=d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d",
                                    "patch": "@@ -121,3 +121,37 @@ def _clspath(cls, forced=None):\n     if forced is not None:\n         return forced\n     return '{}.{}'.format(cls.__module__, cls.__name__)\n+\n+\n+DEPRECATION_RULES = [\n+    ('scrapy.contrib_exp.downloadermiddleware.decompression.', 'scrapy.downloadermiddlewares.decompression.'),\n+    ('scrapy.contrib_exp.iterators.', 'scrapy.utils.iterators.'),\n+    ('scrapy.contrib.downloadermiddleware.', 'scrapy.downloadermiddlewares.'),\n+    ('scrapy.contrib.exporter.', 'scrapy.exporters.'),\n+    ('scrapy.contrib.linkextractors.', 'scrapy.linkextractors.'),\n+    ('scrapy.contrib.loader.processor.', 'scrapy.loader.processors.'),\n+    ('scrapy.contrib.loader.', 'scrapy.loader.'),\n+    ('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n+    ('scrapy.contrib.spidermiddleware.', 'scrapy.spidermiddlewares.'),\n+    ('scrapy.contrib.spiders.', 'scrapy.spiders.'),\n+    ('scrapy.contrib.', 'scrapy.extensions.'),\n+    ('scrapy.command.', 'scrapy.commands.'),\n+    ('scrapy.dupefilter.', 'scrapy.dupefilters.'),\n+    ('scrapy.linkextractor.', 'scrapy.linkextractors.'),\n+    ('scrapy.spider.', 'scrapy.spiders.'),\n+    ('scrapy.squeue.', 'scrapy.squeues.'),\n+    ('scrapy.statscol.', 'scrapy.statscollectors.'),\n+    ('scrapy.utils.decorator.', 'scrapy.utils.decorators.'),\n+    ('scrapy.spidermanager.SpiderManager', 'scrapy.spiderloader.SpiderLoader'),\n+]\n+\n+\n+def update_classpath(path):\n+    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n+    for prefix, replacement in DEPRECATION_RULES:\n+        if path.startswith(prefix):\n+            new_path = path.replace(prefix, replacement, 1)\n+            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n+                          ScrapyDeprecationWarning)\n+            return new_path\n+    return path"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "177d024fb574f3583ef9ff3541407ecb99e4c5a9",
                                    "filename": "tests/test_utils_conf.py",
                                    "status": "modified",
                                    "additions": 29,
                                    "deletions": 4,
                                    "changes": 33,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/tests%2Ftest_utils_conf.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/tests%2Ftest_utils_conf.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/tests%2Ftest_utils_conf.py?ref=d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d",
                                    "patch": "@@ -2,16 +2,41 @@\n \n from scrapy.utils.conf import build_component_list, arglist_to_dict\n \n-class UtilsConfTestCase(unittest.TestCase):\n \n-    def test_build_component_list(self):\n+class BuildComponentListTest(unittest.TestCase):\n+\n+    def test_build_dict(self):\n         base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n         custom = {'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(base, custom),\n+        self.assertEqual(build_component_list(base, custom, lambda x: x),\n                          ['one', 'four', 'five', 'three'])\n \n+    def test_return_list(self):\n+        custom = ['a', 'b', 'c']\n+        self.assertEqual(build_component_list(None, custom, lambda x: x),\n+                         custom)\n+\n+    def test_map_dict(self):\n+        custom = {'one': 1, 'two': 2, 'three': 3}\n+        self.assertEqual(build_component_list({}, custom, lambda x: x.upper()),\n+                         ['ONE', 'TWO', 'THREE'])\n+\n+    def test_map_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(base, custom), custom)\n+        self.assertEqual(build_component_list(None, custom, lambda x: x.upper()),\n+                         ['A', 'B', 'C'])\n+\n+    def test_duplicate_components_in_dict(self):\n+        duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}\n+        self.assertRaises(ValueError,\n+                          build_component_list, {}, duplicate_dict, lambda x: x.lower())\n+\n+    def test_duplicate_components_in_list(self):\n+        duplicate_list = ['a', 'b', 'a']\n+        self.assertRaises(ValueError,\n+                          build_component_list, None, duplicate_list, lambda x: x)\n+\n+class UtilsConfTestCase(unittest.TestCase):\n \n     def test_arglist_to_dict(self):\n         self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),"
                                },
                                {
                                    "sha": "41b8100d76ec7c019e7662a61cdb02f7a069bd29",
                                    "filename": "tests/test_utils_deprecate.py",
                                    "status": "modified",
                                    "additions": 26,
                                    "deletions": 1,
                                    "changes": 27,
                                    "blob_url": "https://github.com/scrapy/scrapy/blob/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/tests%2Ftest_utils_deprecate.py",
                                    "raw_url": "https://github.com/scrapy/scrapy/raw/d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d/tests%2Ftest_utils_deprecate.py",
                                    "contents_url": "https://api.github.com/repos/scrapy/scrapy/contents/tests%2Ftest_utils_deprecate.py?ref=d52cf8bb036ce2dc6cfbe63f78cfd172cc7c888d",
                                    "patch": "@@ -3,7 +3,7 @@\n import inspect\n import unittest\n import warnings\n-from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n from tests import mock\n \n@@ -248,3 +248,28 @@ class SubClass(DeprecatedName):\n                     pass\n \n         self.assertIn(\"Error detecting parent module\", str(w[0].message))\n+\n+\n+@mock.patch('scrapy.utils.deprecate.DEPRECATION_RULES',\n+            [('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n+             ('scrapy.contrib.', 'scrapy.extensions.')])\n+class UpdateClassPathTest(unittest.TestCase):\n+\n+    def test_old_path_gets_fixed(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            output = update_classpath('scrapy.contrib.debug.Debug')\n+        self.assertEqual(output, 'scrapy.extensions.debug.Debug')\n+        self.assertEqual(len(w), 1)\n+        self.assertIn(\"scrapy.contrib.debug.Debug\", str(w[0].message))\n+        self.assertIn(\"scrapy.extensions.debug.Debug\", str(w[0].message))\n+\n+    def test_sorted_replacement(self):\n+        with warnings.catch_warnings(record=True):\n+            output = update_classpath('scrapy.contrib.pipeline.Pipeline')\n+        self.assertEqual(output, 'scrapy.pipelines.Pipeline')\n+\n+    def test_unmatched_path_stays_the_same(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            output = update_classpath('scrapy.unmatched.Path')\n+        self.assertEqual(output, 'scrapy.unmatched.Path')\n+        self.assertEqual(len(w), 0)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\npipenv install pytest\npipenv install twisted[tls]\npipenv install -r requirements.txt\npipenv install .",
                "testSteps": "pipenv run pytest tests/test_utils_conf.py\npipenv run pytest tests/test_utils_deprecate.py"
            }
        ],
        "installSteps": "pipenv --python 3.8\npipenv install pytest\npipenv install twisted[tls]\npipenv install -r requirements-py3.txt\npipenv install ."
    },
    {
        "_id": "64898b0cd97326f515359963",
        "username": "explosion",
        "repository": "spaCy",
        "issues": [
            {
                "id": 8019,
                "created_at": "2021-05-06T15:46:48Z",
                "closed_at": "2021-05-07T08:26:46Z",
                "title": "Incomplete coverage of ordinals as numbers",
                "labels": "bug, lang / en",
                "commits": [
                    {
                        "hash": "bdeaf3a18b1c62b41425ac8d2be0dd99e418e805",
                        "commit_date": "2021-05-07T08:26:42Z",
                        "parents": "71c2a3ab4747e466e77a0a590c9fed4d3b791f29",
                        "stat": {
                            "total": 2,
                            "additions": 6,
                            "deletions": 4,
                            "files": [
                                {
                                    "sha": "b630a317dc0c4ffefdc5d5bd621e50d9ec4cd094",
                                    "filename": "spacy/lang/en/lex_attrs.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bdeaf3a18b1c62b41425ac8d2be0dd99e418e805/spacy%2Flang%2Fen%2Flex_attrs.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bdeaf3a18b1c62b41425ac8d2be0dd99e418e805/spacy%2Flang%2Fen%2Flex_attrs.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Flang%2Fen%2Flex_attrs.py?ref=bdeaf3a18b1c62b41425ac8d2be0dd99e418e805",
                                    "patch": "@@ -35,7 +35,7 @@ def like_num(text: str) -> bool:\n     # Check ordinal number\n     if text_lower in _ordinal_words:\n         return True\n-    if text_lower.endswith(\"th\"):\n+    if text_lower.endswith((\"st\", \"nd\", \"rd\", \"th\")):\n         if text_lower[:-2].isdigit():\n             return True\n     return False"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "358f4c0f9a4c3982a4156e09c56915ed396075f9",
                                    "filename": "spacy/tests/lang/en/test_text.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bdeaf3a18b1c62b41425ac8d2be0dd99e418e805/spacy%2Ftests%2Flang%2Fen%2Ftest_text.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bdeaf3a18b1c62b41425ac8d2be0dd99e418e805/spacy%2Ftests%2Flang%2Fen%2Ftest_text.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Flang%2Fen%2Ftest_text.py?ref=bdeaf3a18b1c62b41425ac8d2be0dd99e418e805",
                                    "patch": "@@ -56,7 +56,9 @@ def test_lex_attrs_like_number(en_tokenizer, text, match):\n     assert tokens[0].like_num == match\n \n \n-@pytest.mark.parametrize(\"word\", [\"third\", \"Millionth\", \"100th\", \"Hundredth\"])\n+@pytest.mark.parametrize(\n+    \"word\", [\"third\", \"Millionth\", \"100th\", \"Hundredth\", \"23rd\", \"52nd\"]\n+)\n def test_en_lex_attrs_like_number_for_ordinal(word):\n     assert like_num(word)\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/lang/en/test_text.py",
                "testStepsFull": "pipenv run python -m pytest --pyargs spacy"
            },
            {
                "id": 3830,
                "created_at": "2019-06-07T10:17:44Z",
                "closed_at": "2019-08-23T15:54:04Z",
                "title": "German model leaks internal symbol and can't seem to handle quotes as before",
                "labels": "bug, lang / de, feat / parser, perf / accuracy",
                "commits": [
                    {
                        "hash": "bb911e5f4e1c57c6374367e9b438f854d1667b84",
                        "commit_date": "2019-08-23T15:54:00Z",
                        "parents": "c417c380e388987561bb734a668e4ccca0a38c29",
                        "stat": {
                            "total": 9,
                            "additions": 60,
                            "deletions": 51,
                            "files": [
                                {
                                    "sha": "c4355f1a1198a5c13a98e9b83a6e99a9ae4e9769",
                                    "filename": "spacy/cli/train.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 5,
                                    "changes": 15,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fcli%2Ftrain.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fcli%2Ftrain.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fcli%2Ftrain.py?ref=bb911e5f4e1c57c6374367e9b438f854d1667b84",
                                    "patch": "@@ -172,16 +172,21 @@ def train(\n         nlp.disable_pipes(*other_pipes)\n         for pipe in pipeline:\n             if pipe not in nlp.pipe_names:\n-                nlp.add_pipe(nlp.create_pipe(pipe))\n+                if pipe == \"parser\":\n+                    pipe_cfg = {\"learn_tokens\": learn_tokens}\n+                else:\n+                    pipe_cfg = {}\n+                nlp.add_pipe(nlp.create_pipe(pipe, config=pipe_cfg))\n     else:\n         msg.text(\"Starting with blank model '{}'\".format(lang))\n         lang_cls = util.get_lang_class(lang)\n         nlp = lang_cls()\n         for pipe in pipeline:\n-            nlp.add_pipe(nlp.create_pipe(pipe))\n-\n-    if learn_tokens:\n-        nlp.add_pipe(nlp.create_pipe(\"merge_subtokens\"))\n+            if pipe == \"parser\":\n+                pipe_cfg = {\"learn_tokens\": learn_tokens}\n+            else:\n+                pipe_cfg = {}\n+            nlp.add_pipe(nlp.create_pipe(pipe, config=pipe_cfg))\n \n     if vectors:\n         msg.text(\"Loading vector from model '{}'\".format(vectors))"
                                },
                                {
                                    "sha": "90ccc2fbf9127e5cf6a9e081e51bfde17c3ab04d",
                                    "filename": "spacy/pipeline/pipes.pyx",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 1,
                                    "changes": 5,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fpipeline%2Fpipes.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fpipeline%2Fpipes.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fpipeline%2Fpipes.pyx?ref=bb911e5f4e1c57c6374367e9b438f854d1667b84",
                                    "patch": "@@ -1038,7 +1038,10 @@ cdef class DependencyParser(Parser):\n \n     @property\n     def postprocesses(self):\n-        return [nonproj.deprojectivize]\n+        output = [nonproj.deprojectivize]\n+        if self.cfg.get(\"learn_tokens\") is True:\n+            output.append(merge_subtokens)\n+        return tuple(output)\n \n     def add_multitask_objective(self, target):\n         if target == \"cloze\":"
                                },
                                {
                                    "sha": "eb39124cebc71a1ca22f99b4ccce401cdb3c3911",
                                    "filename": "spacy/syntax/arc_eager.pyx",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 2,
                                    "changes": 17,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fsyntax%2Farc_eager.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fsyntax%2Farc_eager.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fsyntax%2Farc_eager.pyx?ref=bb911e5f4e1c57c6374367e9b438f854d1667b84",
                                    "patch": "@@ -362,8 +362,9 @@ cdef class ArcEager(TransitionSystem):\n                         label_freqs.pop(label)\n         # Ensure these actions are present\n         actions[BREAK].setdefault('ROOT', 0)\n-        actions[RIGHT].setdefault('subtok', 0)\n-        actions[LEFT].setdefault('subtok', 0)\n+        if kwargs.get(\"learn_tokens\") is True:\n+            actions[RIGHT].setdefault('subtok', 0)\n+            actions[LEFT].setdefault('subtok', 0)\n         # Used for backoff\n         actions[RIGHT].setdefault('dep', 0)\n         actions[LEFT].setdefault('dep', 0)\n@@ -410,11 +411,23 @@ cdef class ArcEager(TransitionSystem):\n     def preprocess_gold(self, GoldParse gold):\n         if not self.has_gold(gold):\n             return None\n+        # Figure out whether we're using subtok\n+        use_subtok = False\n+        for action, labels in self.labels.items():\n+            if SUBTOK_LABEL in labels:\n+                use_subtok = True\n+                break\n         for i, (head, dep) in enumerate(zip(gold.heads, gold.labels)):\n             # Missing values\n             if head is None or dep is None:\n                 gold.c.heads[i] = i\n                 gold.c.has_dep[i] = False\n+            elif dep == SUBTOK_LABEL and not use_subtok:\n+                # If we're not doing the joint tokenization and parsing,\n+                # regard these subtok labels as missing\n+                gold.c.heads[i] = i\n+                gold.c.labels[i] = 0\n+                gold.c.has_dep[i] = False\n             else:\n                 if head > i:\n                     action = LEFT"
                                },
                                {
                                    "sha": "c4edef137fd3e2333d0bd5a3fd648296721ef385",
                                    "filename": "spacy/syntax/nn_parser.pyx",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fsyntax%2Fnn_parser.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Fsyntax%2Fnn_parser.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fsyntax%2Fnn_parser.pyx?ref=bb911e5f4e1c57c6374367e9b438f854d1667b84",
                                    "patch": "@@ -573,7 +573,8 @@ cdef class Parser:\n             get_gold_tuples = lambda: gold_tuples\n         cfg.setdefault('min_action_freq', 30)\n         actions = self.moves.get_actions(gold_parses=get_gold_tuples(),\n-                                         min_freq=cfg.get('min_action_freq', 30))\n+                                         min_freq=cfg.get('min_action_freq', 30),\n+                                         learn_tokens=self.cfg.get(\"learn_tokens\", False))\n         for action, labels in self.moves.labels.items():\n             actions.setdefault(action, {})\n             for label, freq in labels.items():"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "54ce1092488c4b6fc4f0f0cbdab9649eed8db0f3",
                                    "filename": "spacy/tests/regression/test_issue3830.py",
                                    "status": "added",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Ftests%2Fregression%2Ftest_issue3830.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/bb911e5f4e1c57c6374367e9b438f854d1667b84/spacy%2Ftests%2Fregression%2Ftest_issue3830.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue3830.py?ref=bb911e5f4e1c57c6374367e9b438f854d1667b84",
                                    "patch": "@@ -0,0 +1,20 @@\n+from spacy.pipeline.pipes import DependencyParser\n+from spacy.vocab import Vocab\n+\n+\n+def test_issue3830_no_subtok():\n+    \"\"\"Test that the parser doesn't have subtok label if not learn_tokens\"\"\"\n+    parser = DependencyParser(Vocab())\n+    parser.add_label(\"nsubj\")\n+    assert \"subtok\" not in parser.labels\n+    parser.begin_training(lambda: [])\n+    assert \"subtok\" not in parser.labels\n+\n+\n+def test_issue3830_with_subtok():\n+    \"\"\"Test that the parser does have subtok label if learn_tokens=True.\"\"\"\n+    parser = DependencyParser(Vocab(), learn_tokens=True)\n+    parser.add_label(\"nsubj\")\n+    assert \"subtok\" not in parser.labels\n+    parser.begin_training(lambda: [])\n+    assert \"subtok\" in parser.labels"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue3830.py ",
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install --no-build-isolation --editable ."
            },
            {
                "id": 3549,
                "created_at": "2019-04-07T09:11:48Z",
                "closed_at": "2019-04-09T10:50:51Z",
                "title": "Matcher with `validate = True` raises on valid patterns ",
                "labels": "bug, feat / matcher",
                "commits": [
                    {
                        "hash": "4d198a7e92f813fb9df2ade72fbeaf847284a7a0",
                        "commit_date": "2019-04-09T10:50:43Z",
                        "parents": "3ddb799f27578d3eed39ded12fc812609106b26c",
                        "stat": {
                            "total": 1,
                            "additions": 17,
                            "deletions": 16,
                            "files": [
                                {
                                    "sha": "b58c0e072a7d64152b7255c5fa00a5fd433e87a0",
                                    "filename": "spacy/matcher/matcher.pyx",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/4d198a7e92f813fb9df2ade72fbeaf847284a7a0/spacy%2Fmatcher%2Fmatcher.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/4d198a7e92f813fb9df2ade72fbeaf847284a7a0/spacy%2Fmatcher%2Fmatcher.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fmatcher%2Fmatcher.pyx?ref=4d198a7e92f813fb9df2ade72fbeaf847284a7a0",
                                    "patch": "@@ -105,7 +105,7 @@ cdef class Matcher:\n                 raise ValueError(Errors.E012.format(key=key))\n             if self.validator:\n                 errors[i] = validate_json(pattern, self.validator)\n-        if errors:\n+        if any(err for err in errors.values()):\n             raise MatchPatternError(key, errors)\n         key = self._normalize_key(key)\n         for pattern in patterns:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "3932bf19c39b3ba038a10e6239f40611eaa5fbb3",
                                    "filename": "spacy/tests/regression/test_issue3549.py",
                                    "status": "added",
                                    "additions": 15,
                                    "deletions": 0,
                                    "changes": 15,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/4d198a7e92f813fb9df2ade72fbeaf847284a7a0/spacy%2Ftests%2Fregression%2Ftest_issue3549.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/4d198a7e92f813fb9df2ade72fbeaf847284a7a0/spacy%2Ftests%2Fregression%2Ftest_issue3549.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue3549.py?ref=4d198a7e92f813fb9df2ade72fbeaf847284a7a0",
                                    "patch": "@@ -0,0 +1,15 @@\n+# coding: utf8\n+from __future__ import unicode_literals\n+\n+import pytest\n+from spacy.matcher import Matcher\n+from spacy.errors import MatchPatternError\n+\n+\n+def test_issue3549(en_vocab):\n+    \"\"\"Test that match pattern validation doesn't raise on empty errors.\"\"\"\n+    matcher = Matcher(en_vocab, validate=True)\n+    pattern = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n+    matcher.add(\"GOOD\", None, pattern)\n+    with pytest.raises(MatchPatternError):\n+        matcher.add(\"BAD\", None, [{\"X\": \"Y\"}])"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue3549.py",
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install --no-build-isolation --editable ."
            },
            {
                "id": 3531,
                "created_at": "2019-04-02T13:21:24Z",
                "closed_at": "2019-04-03T08:13:24Z",
                "title": "Displacy producing KeyError: 'settings'",
                "labels": "bug, docs, feat / visualizers",
                "commits": [
                    {
                        "hash": "6a4575a56c5e02b62be55fd39fd24b371821aea9",
                        "commit_date": "2019-04-03T08:13:16Z",
                        "parents": "2f0f439c54fa057287cfc59784c4e25f4bcf7792",
                        "stat": {
                            "total": 5,
                            "additions": 45,
                            "deletions": 40,
                            "files": [
                                {
                                    "sha": "86b933eef32ae3966ef1329ecacdc462196b4120",
                                    "filename": "spacy/displacy/render.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 5,
                                    "changes": 12,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/6a4575a56c5e02b62be55fd39fd24b371821aea9/spacy%2Fdisplacy%2Frender.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/6a4575a56c5e02b62be55fd39fd24b371821aea9/spacy%2Fdisplacy%2Frender.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fdisplacy%2Frender.py?ref=6a4575a56c5e02b62be55fd39fd24b371821aea9",
                                    "patch": "@@ -50,8 +50,9 @@ def render(self, parsed, page=False, minify=False):\n         rendered = []\n         for i, p in enumerate(parsed):\n             if i == 0:\n-                self.direction = p[\"settings\"].get(\"direction\", DEFAULT_DIR)\n-                self.lang = p[\"settings\"].get(\"lang\", DEFAULT_LANG)\n+                settings = p.get(\"settings\", {})\n+                self.direction = settings.get(\"direction\", DEFAULT_DIR)\n+                self.lang = settings.get(\"lang\", DEFAULT_LANG)\n             render_id = \"{}-{}\".format(id_prefix, i)\n             svg = self.render_svg(render_id, p[\"words\"], p[\"arcs\"])\n             rendered.append(svg)\n@@ -254,9 +255,10 @@ def render(self, parsed, page=False, minify=False):\n         rendered = []\n         for i, p in enumerate(parsed):\n             if i == 0:\n-                self.direction = p[\"settings\"].get(\"direction\", DEFAULT_DIR)\n-                self.lang = p[\"settings\"].get(\"lang\", DEFAULT_LANG)\n-            rendered.append(self.render_ents(p[\"text\"], p[\"ents\"], p[\"title\"]))\n+                settings = p.get(\"settings\", {})\n+                self.direction = settings.get(\"direction\", DEFAULT_DIR)\n+                self.lang = settings.get(\"lang\", DEFAULT_LANG)\n+            rendered.append(self.render_ents(p[\"text\"], p[\"ents\"], p.get(\"title\")))\n         if page:\n             docs = \"\".join([TPL_FIGURE.format(content=doc) for doc in rendered])\n             markup = TPL_PAGE.format(content=docs, lang=self.lang, dir=self.direction)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7b9d0bd2afb0c0490352554495f9a3ee59e9196d",
                                    "filename": "spacy/tests/regression/test_issue3531.py",
                                    "status": "added",
                                    "additions": 33,
                                    "deletions": 0,
                                    "changes": 33,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/6a4575a56c5e02b62be55fd39fd24b371821aea9/spacy%2Ftests%2Fregression%2Ftest_issue3531.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/6a4575a56c5e02b62be55fd39fd24b371821aea9/spacy%2Ftests%2Fregression%2Ftest_issue3531.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue3531.py?ref=6a4575a56c5e02b62be55fd39fd24b371821aea9",
                                    "patch": "@@ -0,0 +1,33 @@\n+# coding: utf8\n+from __future__ import unicode_literals\n+\n+from spacy import displacy\n+\n+\n+def test_issue3531():\n+    \"\"\"Test that displaCy renderer doesn't require \"settings\" key.\"\"\"\n+    example_dep = {\n+        \"words\": [\n+            {\"text\": \"But\", \"tag\": \"CCONJ\"},\n+            {\"text\": \"Google\", \"tag\": \"PROPN\"},\n+            {\"text\": \"is\", \"tag\": \"VERB\"},\n+            {\"text\": \"starting\", \"tag\": \"VERB\"},\n+            {\"text\": \"from\", \"tag\": \"ADP\"},\n+            {\"text\": \"behind.\", \"tag\": \"ADV\"},\n+        ],\n+        \"arcs\": [\n+            {\"start\": 0, \"end\": 3, \"label\": \"cc\", \"dir\": \"left\"},\n+            {\"start\": 1, \"end\": 3, \"label\": \"nsubj\", \"dir\": \"left\"},\n+            {\"start\": 2, \"end\": 3, \"label\": \"aux\", \"dir\": \"left\"},\n+            {\"start\": 3, \"end\": 4, \"label\": \"prep\", \"dir\": \"right\"},\n+            {\"start\": 4, \"end\": 5, \"label\": \"pcomp\", \"dir\": \"right\"},\n+        ],\n+    }\n+    example_ent = {\n+        \"text\": \"But Google is starting from behind.\",\n+        \"ents\": [{\"start\": 4, \"end\": 10, \"label\": \"ORG\"}],\n+    }\n+    dep_html = displacy.render(example_dep, style=\"dep\", manual=True)\n+    assert dep_html\n+    ent_html = displacy.render(example_ent, style=\"ent\", manual=True)\n+    assert ent_html"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue3531.py"
            },
            {
                "id": 2361,
                "created_at": "2018-05-24T16:00:48Z",
                "closed_at": "2018-05-28T16:36:45Z",
                "title": "displacy.render produces invalid svg by not escaping \"<\"",
                "labels": "bug, help wanted, help wanted (easy), feat / visualizers",
                "commits": [
                    {
                        "hash": "973298895119ed703b38630d3d7db01a0b08f1ea",
                        "commit_date": "2018-05-28T16:36:41Z",
                        "parents": "d85494bfae583b47a5a88f09aa6933f95891497a",
                        "stat": {
                            "total": 2,
                            "additions": 140,
                            "deletions": 138,
                            "files": [
                                {
                                    "sha": "a45dea3f2764ea12814db5bc34d5fcf032370ba5",
                                    "filename": ".github/contributors/ansgar-t.md",
                                    "status": "added",
                                    "additions": 106,
                                    "deletions": 0,
                                    "changes": 106,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/973298895119ed703b38630d3d7db01a0b08f1ea/.github%2Fcontributors%2Fansgar-t.md",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/973298895119ed703b38630d3d7db01a0b08f1ea/.github%2Fcontributors%2Fansgar-t.md",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/.github%2Fcontributors%2Fansgar-t.md?ref=973298895119ed703b38630d3d7db01a0b08f1ea",
                                    "patch": "@@ -0,0 +1,106 @@\n+# spaCy contributor agreement\n+\n+This spaCy Contributor Agreement (**\"SCA\"**) is based on the\n+[Oracle Contributor Agreement](http://www.oracle.com/technetwork/oca-405177.pdf).\n+The SCA applies to any contribution that you make to any product or project\n+managed by us (the **\"project\"**), and sets out the intellectual property rights\n+you grant to us in the contributed materials. The term **\"us\"** shall mean\n+[ExplosionAI UG (haftungsbeschr\u00e4nkt)](https://explosion.ai/legal). The term\n+**\"you\"** shall mean the person or entity identified below.\n+\n+If you agree to be bound by these terms, fill in the information requested\n+below and include the filled-in version with your first pull request, under the\n+folder [`.github/contributors/`](/.github/contributors/). The name of the file\n+should be your GitHub username, with the extension `.md`. For example, the user\n+example_user would create the file `.github/contributors/example_user.md`.\n+\n+Read this agreement carefully before signing. These terms and conditions\n+constitute a binding legal agreement.\n+\n+## Contributor Agreement\n+\n+1. The term \"contribution\" or \"contributed materials\" means any source code,\n+object code, patch, tool, sample, graphic, specification, manual,\n+documentation, or any other material posted or submitted by you to the project.\n+\n+2. With respect to any worldwide copyrights, or copyright applications and\n+registrations, in your contribution:\n+\n+    * you hereby assign to us joint ownership, and to the extent that such\n+    assignment is or becomes invalid, ineffective or unenforceable, you hereby\n+    grant to us a perpetual, irrevocable, non-exclusive, worldwide, no-charge,\n+    royalty-free, unrestricted license to exercise all rights under those\n+    copyrights. This includes, at our option, the right to sublicense these same\n+    rights to third parties through multiple levels of sublicensees or other\n+    licensing arrangements;\n+\n+    * you agree that each of us can do all things in relation to your\n+    contribution as if each of us were the sole owners, and if one of us makes\n+    a derivative work of your contribution, the one who makes the derivative\n+    work (or has it made will be the sole owner of that derivative work;\n+\n+    * you agree that you will not assert any moral rights in your contribution\n+    against us, our licensees or transferees;\n+\n+    * you agree that we may register a copyright in your contribution and\n+    exercise all ownership rights associated with it; and\n+\n+    * you agree that neither of us has any duty to consult with, obtain the\n+    consent of, pay or render an accounting to the other for any use or\n+    distribution of your contribution.\n+\n+3. With respect to any patents you own, or that you can license without payment\n+to any third party, you hereby grant to us a perpetual, irrevocable,\n+non-exclusive, worldwide, no-charge, royalty-free license to:\n+\n+    * make, have made, use, sell, offer to sell, import, and otherwise transfer\n+    your contribution in whole or in part, alone or in combination with or\n+    included in any product, work or materials arising out of the project to\n+    which your contribution was submitted, and\n+\n+    * at our option, to sublicense these same rights to third parties through\n+    multiple levels of sublicensees or other licensing arrangements.\n+\n+4. Except as set out above, you keep all right, title, and interest in your\n+contribution. The rights that you grant to us under these terms are effective\n+on the date you first submitted a contribution to us, even if your submission\n+took place before the date you sign these terms.\n+\n+5. You covenant, represent, warrant and agree that:\n+\n+    * Each contribution that you submit is and shall be an original work of\n+    authorship and you can legally grant the rights set out in this SCA;\n+\n+    * to the best of your knowledge, each contribution will not violate any\n+    third party's copyrights, trademarks, patents, or other intellectual\n+    property rights; and\n+\n+    * each contribution shall be in compliance with U.S. export control laws and\n+    other applicable export and import laws. You agree to notify us if you\n+    become aware of any circumstance which would make any of the foregoing\n+    representations inaccurate in any respect. We may publicly disclose your\n+    participation in the project, including the fact that you have signed the SCA.\n+\n+6. This SCA is governed by the laws of the State of California and applicable\n+U.S. Federal law. Any choice of law rules will not apply.\n+\n+7. Please place an \u201cx\u201d on one of the applicable statement below. Please do NOT\n+mark both statements:\n+\n+    * [x] I am signing on behalf of myself as an individual and no other person\n+    or entity, including my employer, has or will have rights with respect to my\n+    contributions.\n+\n+    * [ ] I am signing on behalf of my employer or a legal entity and I have the\n+    actual authority to contractually bind that entity.\n+\n+## Contributor Details\n+\n+| Field                          | Entry                |\n+|------------------------------- | -------------------- |\n+| Name                           | Ansgar T\u00fcmmers       |\n+| Company name (if applicable)   |                      |\n+| Title or role (if applicable)  |                      |\n+| Date                           | 2018-05-26           |\n+| GitHub username                | ansgar-t             |\n+| Website (optional)             |                      |"
                                },
                                {
                                    "sha": "fa84bf87d447f32e94cd980b476cf39b34d5fb66",
                                    "filename": "spacy/displacy/render.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 2,
                                    "changes": 6,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Fdisplacy%2Frender.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Fdisplacy%2Frender.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fdisplacy%2Frender.py?ref=973298895119ed703b38630d3d7db01a0b08f1ea",
                                    "patch": "@@ -3,7 +3,7 @@\n \n from .templates import TPL_DEP_SVG, TPL_DEP_WORDS, TPL_DEP_ARCS\n from .templates import TPL_ENT, TPL_ENTS, TPL_FIGURE, TPL_TITLE, TPL_PAGE\n-from ..util import minify_html\n+from ..util import minify_html, escape_html\n \n \n class DependencyRenderer(object):\n@@ -84,7 +84,9 @@ def render_word(self, text, tag, i):\n         \"\"\"\n         y = self.offset_y+self.word_spacing\n         x = self.offset_x+i*self.distance\n-        return TPL_DEP_WORDS.format(text=text, tag=tag, x=x, y=y)\n+        html_text = escape_html(text)\n+        return TPL_DEP_WORDS.format(text=html_text, tag=tag, x=x, y=y)\n+\n \n     def render_arrow(self, label, start, end, direction, i):\n         \"\"\"Render indivicual arrow."
                                },
                                {
                                    "sha": "c101f4962c9128e3b354cd954372564448da035e",
                                    "filename": "spacy/util.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Futil.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Futil.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Futil.py?ref=973298895119ed703b38630d3d7db01a0b08f1ea",
                                    "patch": "@@ -590,6 +590,20 @@ def minify_html(html):\n     return html.strip().replace('    ', '').replace('\\n', '')\n \n \n+def escape_html(text):\n+    \"\"\"Replace <, >, &, \" with their HTML encoded representation. Intended to\n+    prevent HTML errors in rendered displaCy markup.\n+\n+    text (unicode): The original text.\n+    RETURNS (unicode): Equivalent text to be safely used within HTML.\n+    \"\"\"\n+    text = text.replace('&', '&amp;')\n+    text = text.replace('<', '&lt;')\n+    text = text.replace('>', '&gt;')\n+    text = text.replace('\"', '&quot;')\n+    return text\n+\n+\n def use_gpu(gpu_id):\n     try:\n         import cupy.cuda.device"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a2ed3807783a8113f6b6707a327794e03bc6ebf0",
                                    "filename": "spacy/tests/regression/test_issue2361.py",
                                    "status": "added",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Ftests%2Fregression%2Ftest_issue2361.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/973298895119ed703b38630d3d7db01a0b08f1ea/spacy%2Ftests%2Fregression%2Ftest_issue2361.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue2361.py?ref=973298895119ed703b38630d3d7db01a0b08f1ea",
                                    "patch": "@@ -0,0 +1,14 @@\n+from __future__ import unicode_literals\n+import pytest\n+\n+from ...displacy import render\n+from ..util import get_doc\n+\n+def test_issue2361(de_tokenizer):\n+    tokens = de_tokenizer('< > & \" ')\n+    html = render(get_doc(tokens.vocab, [t.text for t in tokens]))\n+\n+    assert '&lt;' in html\n+    assert '&gt;' in html\n+    assert '&amp;' in html\n+    assert '&quot;' in html"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install msgpack==0.5.6 msgpack-numpy==0.4.3\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue2361.py",
                "testStepsFull": "pipenv run python -m pytest"
            },
            {
                "id": 1967,
                "created_at": "2018-02-11T04:03:10Z",
                "closed_at": "2018-03-28T18:49:54Z",
                "title": "Entity types with dashes cause errors in ner.pyx",
                "labels": "bug, feat / ner",
                "commits": [
                    {
                        "hash": "3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204",
                        "commit_date": "2018-03-28T18:51:26Z",
                        "parents": "9615ed5ed7d5a883b8f1092cbb1eb0c5a3306659",
                        "stat": {
                            "total": 3,
                            "additions": 19,
                            "deletions": 16,
                            "files": [
                                {
                                    "sha": "488f16af9df3f8203093e3f200d2de69a5119fa0",
                                    "filename": "spacy/syntax/ner.pyx",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 3,
                                    "changes": 4,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204/spacy%2Fsyntax%2Fner.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204/spacy%2Fsyntax%2Fner.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fsyntax%2Fner.pyx?ref=3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204",
                                    "patch": "@@ -84,9 +84,7 @@ cdef class BiluoPushDown(TransitionSystem):\n             for (ids, words, tags, heads, labels, biluo), _ in sents:\n                 for i, ner_tag in enumerate(biluo):\n                     if ner_tag != 'O' and ner_tag != '-':\n-                        if ner_tag.count('-') != 1:\n-                            raise ValueError(ner_tag)\n-                        _, label = ner_tag.split('-')\n+                        _, label = ner_tag.split('-', 1)\n                         if label not in seen_entities:\n                             seen_entities.add(label)\n                             for move_str in ('B', 'I', 'L', 'U'):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a6cd489ccf1f30446a4dbd0d264edf921a7adf9e",
                                    "filename": "spacy/tests/regression/test_issue1967.py",
                                    "status": "added",
                                    "additions": 15,
                                    "deletions": 0,
                                    "changes": 15,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204/spacy%2Ftests%2Fregression%2Ftest_issue1967.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204/spacy%2Ftests%2Fregression%2Ftest_issue1967.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue1967.py?ref=3eb67bbe4ba8897c76cbd43ac1a022f1a39d8204",
                                    "patch": "@@ -0,0 +1,15 @@\n+# coding: utf8\n+from __future__ import unicode_literals\n+\n+import pytest\n+\n+from ...pipeline import EntityRecognizer\n+from ...vocab import Vocab\n+\n+\n+@pytest.mark.parametrize('label', ['U-JOB-NAME'])\n+def test_issue1967(label):\n+    ner = EntityRecognizer(Vocab())\n+    entry = ([0], ['word'], ['tag'], [0], ['dep'], [label])\n+    gold_parses = [(None, [(entry, None)])]\n+    ner.moves.get_actions(gold_parses=gold_parses)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install msgpack==0.5.6 msgpack-numpy==0.4.3\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": " pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue1967.py",
                "testStepsFull": "pipenv run python -m pytest",
                "requireRebuild": true
            },
            {
                "id": 1807,
                "created_at": "2018-01-07T18:44:58Z",
                "closed_at": "2018-01-14T12:58:18Z",
                "title": "nlp.vocab.set_vector fails to add new vocab in vectors_fast_text.py",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "0153220304aa89080acf3b3fa7ccace8028c4868",
                        "commit_date": "2018-01-14T12:57:57Z",
                        "parents": "55754f0ceed36e41b5287084d295d7ededffeea1",
                        "stat": {
                            "total": 0,
                            "additions": 15,
                            "deletions": 15,
                            "files": [
                                {
                                    "sha": "a103bcd0e3dadc69dcb980a70903bf3119a86f52",
                                    "filename": "spacy/vocab.pyx",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/0153220304aa89080acf3b3fa7ccace8028c4868/spacy%2Fvocab.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/0153220304aa89080acf3b3fa7ccace8028c4868/spacy%2Fvocab.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fvocab.pyx?ref=0153220304aa89080acf3b3fa7ccace8028c4868",
                                    "patch": "@@ -335,6 +335,7 @@ cdef class Vocab:\n             else:\n                 width = self.vectors.shape[1]\n             self.vectors.resize((new_rows, width))\n+            lex = self[orth] # Adds worse to vocab\n             self.vectors.add(orth, vector=vector)\n         self.vectors.add(orth, vector=vector)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c73f008eb546ead32815b4f5890e7e6583178a77",
                                    "filename": "spacy/tests/regression/test_issue1807.py",
                                    "status": "added",
                                    "additions": 14,
                                    "deletions": 0,
                                    "changes": 14,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/0153220304aa89080acf3b3fa7ccace8028c4868/spacy%2Ftests%2Fregression%2Ftest_issue1807.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/0153220304aa89080acf3b3fa7ccace8028c4868/spacy%2Ftests%2Fregression%2Ftest_issue1807.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue1807.py?ref=0153220304aa89080acf3b3fa7ccace8028c4868",
                                    "patch": "@@ -0,0 +1,14 @@\n+'''Test vocab.set_vector also adds the word to the vocab.'''\n+from __future__ import unicode_literals\n+from ...vocab import Vocab\n+\n+import numpy \n+\n+\n+def test_issue1807():\n+    vocab = Vocab()\n+    arr = numpy.ones((50,), dtype='f')\n+    assert 'hello' not in vocab\n+    vocab.set_vector('hello', arr)\n+    assert 'hello' in vocab\n+"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install msgpack==0.5.6 msgpack-numpy==0.4.3\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue1807.py",
                "testStepsFull": "pipenv run python -m pytest",
                "requireRebuild": true
            },
            {
                "id": 1654,
                "created_at": "2017-11-27T19:07:33Z",
                "closed_at": "2017-11-28T19:38:04Z",
                "title": "Adding custom items to nlp pipeline does not insert them in the correct order",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "a31506e06060c559abfeda043503935691af2e98",
                        "commit_date": "2017-11-28T19:37:55Z",
                        "parents": "343fd2969d5c37799428f0b1fbc840667a2eb8a3",
                        "stat": {
                            "total": 2,
                            "additions": 10,
                            "deletions": 8,
                            "files": [
                                {
                                    "sha": "ec0c5d68fc2e21dd7c7b0b71624469bd92904534",
                                    "filename": "spacy/language.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/a31506e06060c559abfeda043503935691af2e98/spacy%2Flanguage.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/a31506e06060c559abfeda043503935691af2e98/spacy%2Flanguage.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Flanguage.py?ref=a31506e06060c559abfeda043503935691af2e98",
                                    "patch": "@@ -260,7 +260,7 @@ def add_pipe(self, component, name=None, before=None, after=None,\n         elif before and before in self.pipe_names:\n             self.pipeline.insert(self.pipe_names.index(before), pipe)\n         elif after and after in self.pipe_names:\n-            self.pipeline.insert(self.pipe_names.index(after), pipe)\n+            self.pipeline.insert(self.pipe_names.index(after) + 1, pipe)\n         else:\n             msg = \"Can't find '{}' in pipeline. Available names: {}\"\n             unfound = before or after"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "531c00757dcbc5577b6ab6d220dc1c8a9e5a61ee",
                                    "filename": "spacy/tests/regression/test_issue1654.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 1,
                                    "changes": 8,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/a31506e06060c559abfeda043503935691af2e98/spacy%2Ftests%2Fregression%2Ftest_issue1654.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/a31506e06060c559abfeda043503935691af2e98/spacy%2Ftests%2Fregression%2Ftest_issue1654.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue1654.py?ref=a31506e06060c559abfeda043503935691af2e98",
                                    "patch": "@@ -7,11 +7,17 @@\n from ...vocab import Vocab\n \n \n-@pytest.mark.xfail\n def test_issue1654():\n     nlp = Language(Vocab())\n     assert not nlp.pipeline\n     nlp.add_pipe(lambda doc: doc, name='1')\n     nlp.add_pipe(lambda doc: doc, name='2', after='1')\n     nlp.add_pipe(lambda doc: doc, name='3', after='2')\n     assert nlp.pipe_names == ['1', '2', '3']\n+\n+    nlp2 = Language(Vocab())\n+    assert not nlp2.pipeline\n+    nlp2.add_pipe(lambda doc: doc, name='3')\n+    nlp2.add_pipe(lambda doc: doc, name='2', before='3')\n+    nlp2.add_pipe(lambda doc: doc, name='1', before='2')\n+    assert nlp2.pipe_names == ['1', '2', '3']"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install \"Cython==0.29.22\"\npipenv run pip install \"attrs==19.1.0\"\npipenv run pip install \"pathlib\"\npipenv run pip install \"numpy>=1.7\"\npipenv run pip install \"cymem>=1.30,<1.32\"\npipenv run pip install \"preshed>=1.0.0,<2.0.0\"\npipenv run pip install \"thinc>=6.10.1,<6.11.0\"\npipenv run pip install \"murmurhash>=0.28,<0.29\"\npipenv run pip install \"plac<1.0.0,>=0.9.6\"\npipenv run pip install \"six\"\npipenv run pip install \"ujson>=1.35\"\npipenv run pip install \"dill>=0.2,<0.3\"\npipenv run pip install \"requests>=2.13.0,<3.0.0\"\npipenv run pip install \"regex==2017.4.5\"\npipenv run pip install \"ftfy>=4.4.2,<5.0.0\"\npipenv run pip install \"pytest>=3.0.6,<4.0.0\"\npipenv run pip install \"mock>=2.0.0,<3.0.0\"\npipenv run pip install \"msgpack-python\"\npipenv run pip install \"msgpack-numpy==0.4.3\"\npipenv run pip install \"html5lib==1.0b8\"\npipenv run pip install \"msgpack==0.5.6\"\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue1654.py",
                "testStepsFull": "pipenv run python -m pytest"
            },
            {
                "id": 1612,
                "created_at": "2017-11-19T17:21:44Z",
                "closed_at": "2017-11-20T20:53:18Z",
                "title": "span.orth_ != span.text",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "ab2342a10e6a94e42682ebe2f1f9c607131c6569",
                        "commit_date": "2017-11-20T20:53:15Z",
                        "parents": "ec0899600047859c8acc040837a88b0381a13d7d",
                        "stat": {
                            "total": 1,
                            "additions": 120,
                            "deletions": 119,
                            "files": [
                                {
                                    "sha": "f65a625cb5c8a7181a84b21cd060b1328d4d67cc",
                                    "filename": ".github/contributors/bdewilde.md",
                                    "status": "added",
                                    "additions": 106,
                                    "deletions": 0,
                                    "changes": 106,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/ab2342a10e6a94e42682ebe2f1f9c607131c6569/.github%2Fcontributors%2Fbdewilde.md",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/ab2342a10e6a94e42682ebe2f1f9c607131c6569/.github%2Fcontributors%2Fbdewilde.md",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/.github%2Fcontributors%2Fbdewilde.md?ref=ab2342a10e6a94e42682ebe2f1f9c607131c6569",
                                    "patch": "@@ -0,0 +1,106 @@\n+# spaCy contributor agreement\n+\n+This spaCy Contributor Agreement (**\"SCA\"**) is based on the\n+[Oracle Contributor Agreement](http://www.oracle.com/technetwork/oca-405177.pdf).\n+The SCA applies to any contribution that you make to any product or project\n+managed by us (the **\"project\"**), and sets out the intellectual property rights\n+you grant to us in the contributed materials. The term **\"us\"** shall mean\n+[ExplosionAI UG (haftungsbeschr\u00e4nkt)](https://explosion.ai/legal). The term\n+**\"you\"** shall mean the person or entity identified below.\n+\n+If you agree to be bound by these terms, fill in the information requested\n+below and include the filled-in version with your first pull request, under the\n+folder [`.github/contributors/`](/.github/contributors/). The name of the file\n+should be your GitHub username, with the extension `.md`. For example, the user\n+example_user would create the file `.github/contributors/example_user.md`.\n+\n+Read this agreement carefully before signing. These terms and conditions\n+constitute a binding legal agreement.\n+\n+## Contributor Agreement\n+\n+1. The term \"contribution\" or \"contributed materials\" means any source code,\n+object code, patch, tool, sample, graphic, specification, manual,\n+documentation, or any other material posted or submitted by you to the project.\n+\n+2. With respect to any worldwide copyrights, or copyright applications and\n+registrations, in your contribution:\n+\n+    * you hereby assign to us joint ownership, and to the extent that such\n+    assignment is or becomes invalid, ineffective or unenforceable, you hereby\n+    grant to us a perpetual, irrevocable, non-exclusive, worldwide, no-charge,\n+    royalty-free, unrestricted license to exercise all rights under those\n+    copyrights. This includes, at our option, the right to sublicense these same\n+    rights to third parties through multiple levels of sublicensees or other\n+    licensing arrangements;\n+\n+    * you agree that each of us can do all things in relation to your\n+    contribution as if each of us were the sole owners, and if one of us makes\n+    a derivative work of your contribution, the one who makes the derivative\n+    work (or has it made will be the sole owner of that derivative work;\n+\n+    * you agree that you will not assert any moral rights in your contribution\n+    against us, our licensees or transferees;\n+\n+    * you agree that we may register a copyright in your contribution and\n+    exercise all ownership rights associated with it; and\n+\n+    * you agree that neither of us has any duty to consult with, obtain the\n+    consent of, pay or render an accounting to the other for any use or\n+    distribution of your contribution.\n+\n+3. With respect to any patents you own, or that you can license without payment\n+to any third party, you hereby grant to us a perpetual, irrevocable,\n+non-exclusive, worldwide, no-charge, royalty-free license to:\n+\n+    * make, have made, use, sell, offer to sell, import, and otherwise transfer\n+    your contribution in whole or in part, alone or in combination with or\n+    included in any product, work or materials arising out of the project to\n+    which your contribution was submitted, and\n+\n+    * at our option, to sublicense these same rights to third parties through\n+    multiple levels of sublicensees or other licensing arrangements.\n+\n+4. Except as set out above, you keep all right, title, and interest in your\n+contribution. The rights that you grant to us under these terms are effective\n+on the date you first submitted a contribution to us, even if your submission\n+took place before the date you sign these terms.\n+\n+5. You covenant, represent, warrant and agree that:\n+\n+    * Each contribution that you submit is and shall be an original work of\n+    authorship and you can legally grant the rights set out in this SCA;\n+\n+    * to the best of your knowledge, each contribution will not violate any\n+    third party's copyrights, trademarks, patents, or other intellectual\n+    property rights; and\n+\n+    * each contribution shall be in compliance with U.S. export control laws and\n+    other applicable export and import laws. You agree to notify us if you\n+    become aware of any circumstance which would make any of the foregoing\n+    representations inaccurate in any respect. We may publicly disclose your\n+    participation in the project, including the fact that you have signed the SCA.\n+\n+6. This SCA is governed by the laws of the State of California and applicable\n+U.S. Federal law. Any choice of law rules will not apply.\n+\n+7. Please place an \u201cx\u201d on one of the applicable statement below. Please do NOT\n+mark both statements:\n+\n+    * [x] I am signing on behalf of myself as an individual and no other person\n+    or entity, including my employer, has or will have rights with respect to my\n+    contributions.\n+\n+    * [ ] I am signing on behalf of my employer or a legal entity and I have the\n+    actual authority to contractually bind that entity.\n+\n+## Contributor Details\n+\n+| Field                          | Entry                        |\n+|------------------------------- | ---------------------------- |\n+| Name                           | Burton DeWilde               |\n+| Company name (if applicable)   | -                            |\n+| Title or role (if applicable)  | data scientist               |\n+| Date                           | 20 November 2017             |\n+| GitHub username                | bdewilde                     |\n+| Website (optional)             | https://bdewilde.github.io/  |"
                                },
                                {
                                    "sha": "f09dfd1344d1b5e63e1b5aa0cabe46464081baff",
                                    "filename": "spacy/tokens/span.pyx",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/ab2342a10e6a94e42682ebe2f1f9c607131c6569/spacy%2Ftokens%2Fspan.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/ab2342a10e6a94e42682ebe2f1f9c607131c6569/spacy%2Ftokens%2Fspan.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftokens%2Fspan.pyx?ref=ab2342a10e6a94e42682ebe2f1f9c607131c6569",
                                    "patch": "@@ -527,7 +527,7 @@ cdef class Span:\n \n         RETURNS (unicode): The span's text.\"\"\"\n         def __get__(self):\n-            return ''.join([t.orth_ for t in self]).strip()\n+            return self.text\n \n     property lemma_:\n         \"\"\"RETURNS (unicode): The span's lemma.\"\"\""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "4587a785fe4debe54c248ac2cb2bb8ccc70c376a",
                                    "filename": "spacy/tests/regression/test_issue1612.py",
                                    "status": "added",
                                    "additions": 12,
                                    "deletions": 0,
                                    "changes": 12,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/ab2342a10e6a94e42682ebe2f1f9c607131c6569/spacy%2Ftests%2Fregression%2Ftest_issue1612.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/ab2342a10e6a94e42682ebe2f1f9c607131c6569/spacy%2Ftests%2Fregression%2Ftest_issue1612.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue1612.py?ref=ab2342a10e6a94e42682ebe2f1f9c607131c6569",
                                    "patch": "@@ -0,0 +1,12 @@\n+from __future__ import unicode_literals\n+\n+import pytest\n+\n+from ...lang.en import English\n+\n+\n+def test_issue1612():\n+    nlp = English()\n+    doc = nlp('The black cat purrs.')\n+    span = doc[1: 3]\n+    assert span.orth_ == span.text"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install \"Cython==0.29.22\"\npipenv run pip install \"attrs==19.1.0\"\npipenv run pip install \"pathlib\"\npipenv run pip install \"numpy>=1.7\"\npipenv run pip install \"cymem>=1.30,<1.32\"\npipenv run pip install \"preshed>=1.0.0,<2.0.0\"\npipenv run pip install \"thinc>=6.10.1,<6.11.0\"\npipenv run pip install \"murmurhash>=0.28,<0.29\"\npipenv run pip install \"plac<1.0.0,>=0.9.6\"\npipenv run pip install \"six\"\npipenv run pip install \"ujson>=1.35\"\npipenv run pip install \"dill>=0.2,<0.3\"\npipenv run pip install \"requests>=2.13.0,<3.0.0\"\npipenv run pip install \"regex==2017.4.5\"\npipenv run pip install \"ftfy>=4.4.2,<5.0.0\"\npipenv run pip install \"pytest>=3.0.6,<4.0.0\"\npipenv run pip install \"mock>=2.0.0,<3.0.0\"\npipenv run pip install \"msgpack-python\"\npipenv run pip install \"msgpack-numpy==0.4.3\"\npipenv run pip install \"html5lib==1.0b8\"\npipenv run pip install \"msgpack==0.5.6\"\npipenv run pip install --no-build-isolation --editable .",
                "requireRebuild": true,
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue1612.py",
                "testStepsFull": "pipenv run python -m pytest"
            },
            {
                "id": 1554,
                "created_at": "2017-11-11T21:54:18Z",
                "closed_at": "2018-03-28T21:10:02Z",
                "title": "Assertion Error using xx_ent_wiki_sm in default pipeline in certain cases",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "0de599b16b0d8b14416bdbb374b3089e4ce0c6ac",
                        "commit_date": "2018-03-28T21:10:00Z",
                        "parents": "98e9cda677e4ca8f852c11288c7f7f50b0e00189",
                        "stat": {
                            "total": 7,
                            "additions": 40,
                            "deletions": 33,
                            "files": [
                                {
                                    "sha": "1885dc8729bdbd8c76c50f775724490246aced9b",
                                    "filename": "spacy/tokens/doc.pyx",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 5,
                                    "changes": 19,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftokens%2Fdoc.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftokens%2Fdoc.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftokens%2Fdoc.pyx?ref=0de599b16b0d8b14416bdbb374b3089e4ce0c6ac",
                                    "patch": "@@ -421,7 +421,12 @@ cdef class Doc:\n             for i in range(self.length):\n                 token = &self.c[i]\n                 if token.ent_iob == 1:\n-                    assert start != -1\n+                    if start == -1:\n+                        seq = ['%s|%s' % (t.text, t.ent_iob_) for t in self[i-5:i+5]]\n+                        raise ValueError(\n+                            \"token.ent_iob values make invalid sequence: \"\n+                            \"I without B\\n\"\n+                            \"{seq}\".format(seq=' '.join(seq)))\n                 elif token.ent_iob == 2 or token.ent_iob == 0:\n                     if start != -1:\n                         output.append(Span(self, start, i, label=label))\n@@ -446,10 +451,7 @@ cdef class Doc:\n             cdef int i\n             for i in range(self.length):\n                 self.c[i].ent_type = 0\n-                # At this point we don't know whether the NER has run over the\n-                # Doc. If the ent_iob is missing, leave it missing.\n-                if self.c[i].ent_iob != 0:\n-                    self.c[i].ent_iob = 2  # Means O. Non-O are set from ents.\n+                self.c[i].ent_iob = 0  # Means missing.\n             cdef attr_t ent_type\n             cdef int start, end\n             for ent_info in ents:\n@@ -947,6 +949,13 @@ cdef class Doc:\n                 self.vocab.morphology.assign_tag(token, attr_value)\n             else:\n                 Token.set_struct_attr(token, attr_name, attr_value)\n+        # Make sure ent_iob remains consistent\n+        if self.c[end].ent_iob == 1 and token.ent_iob in (0, 2):\n+            if token.ent_type == self.c[end].ent_type:\n+                token.ent_iob = 3\n+            else:\n+                # If they're not the same entity type, let them be two entities\n+                self.c[end].ent_iob = 3\n         # Begin by setting all the head indices to absolute token positions\n         # This is easier to work with for now than the offsets\n         # Before thinking of something simpler, beware the case where a"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "31d2b8420613b13f0c9615146e37eaff1ebd2a09",
                                    "filename": "spacy/tests/doc/test_add_entities.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftests%2Fdoc%2Ftest_add_entities.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftests%2Fdoc%2Ftest_add_entities.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fdoc%2Ftest_add_entities.py?ref=0de599b16b0d8b14416bdbb374b3089e4ce0c6ac",
                                    "patch": "@@ -18,7 +18,7 @@ def test_doc_add_entities_set_ents_iob(en_vocab):\n     assert [w.ent_iob_ for w in doc] == (['O'] * len(doc))\n \n     doc.ents = [(doc.vocab.strings['ANIMAL'], 3, 4)]\n-    assert [w.ent_iob_ for w in doc] == ['O', 'O', 'O', 'B']\n+    assert [w.ent_iob_ for w in doc] == ['', '', '', 'B']\n \n     doc.ents = [(doc.vocab.strings['WORD'], 0, 2)]\n-    assert [w.ent_iob_ for w in doc] == ['B', 'I', 'O', 'O']\n+    assert [w.ent_iob_ for w in doc] == ['B', 'I', '', '']"
                                },
                                {
                                    "sha": "ae1f4f4a18a246f794d78e929a2b89eead7dc836",
                                    "filename": "spacy/tests/doc/test_span_merge.py",
                                    "status": "modified",
                                    "additions": 17,
                                    "deletions": 0,
                                    "changes": 17,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftests%2Fdoc%2Ftest_span_merge.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/0de599b16b0d8b14416bdbb374b3089e4ce0c6ac/spacy%2Ftests%2Fdoc%2Ftest_span_merge.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fdoc%2Ftest_span_merge.py?ref=0de599b16b0d8b14416bdbb374b3089e4ce0c6ac",
                                    "patch": "@@ -2,6 +2,8 @@\n from __future__ import unicode_literals\n \n from ..util import get_doc\n+from ...vocab import Vocab\n+from ...tokens import Doc\n \n import pytest\n \n@@ -95,6 +97,21 @@ def test_spans_entity_merge(en_tokenizer):\n     assert len(doc) == 15\n \n \n+def test_spans_entity_merge_iob():\n+    # Test entity IOB stays consistent after merging\n+    words = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n+    doc = Doc(Vocab(), words=words)\n+    doc.ents = [(doc.vocab.strings.add('ent-abc'), 0, 3),\n+                (doc.vocab.strings.add('ent-d'), 3, 4)]\n+    assert doc[0].ent_iob_ == \"B\"\n+    assert doc[1].ent_iob_ == \"I\"\n+    assert doc[2].ent_iob_ == \"I\"\n+    assert doc[3].ent_iob_ == \"B\"\n+    doc[0:1].merge()\n+    assert doc[0].ent_iob_ == \"B\"\n+    assert doc[1].ent_iob_ == \"I\"\n+\n+\n def test_spans_sentence_update_after_merge(en_tokenizer):\n     text = \"Stewart Lee is a stand up comedian. He lives in England and loves Joe Pasquale.\"\n     heads = [1, 1, 0, 1, 2, -1, -4, -5, 1, 0, -1, -1, -3, -4, 1, -2, -7]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install \"Cython==0.29.22\"\npipenv run pip install \"pathlib\"\npipenv run pip install \"numpy>=1.7\"\npipenv run pip install \"cymem>=1.30,<1.32\"\npipenv run pip install \"preshed>=1.0.0,<2.0.0\"\npipenv run pip install \"thinc>=6.10.1,<6.11.0\"\npipenv run pip install \"murmurhash>=0.28,<0.29\"\npipenv run pip install \"plac<1.0.0,>=0.9.6\"\npipenv run pip install \"ujson>=1.35\"\npipenv run pip install \"dill>=0.2,<0.3\"\npipenv run pip install \"requests>=2.13.0,<3.0.0\"\npipenv run pip install \"regex==2017.4.5\"\npipenv run pip install \"pytest>=3.0.6,<4.0.0\"\npipenv run pip install \"mock>=2.0.0,<3.0.0\"\npipenv run pip install \"msgpack-python==0.5.4\"\npipenv run pip install \"msgpack-numpy==0.4.1\"\npipenv run pip install --no-build-isolation --editable .",
                "requireRebuild": true,
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/doc/test_add_entities.py",
                "testStepsFull": "pipenv run python -m pytest"
            },
            {
                "id": 1518,
                "created_at": "2017-11-08T20:20:17Z",
                "closed_at": "2017-11-08T21:18:50Z",
                "title": "nlp.vocab.vectors.resize gives 'numpy.uint64' object is not iterable",
                "labels": "bug",
                "commits": [
                    {
                        "hash": "a5ea0fdf5a9656525dd8d78b7d6979538727a776",
                        "commit_date": "2017-11-08T21:18:37Z",
                        "parents": "dec79ca3322c55adc58afed3d686e35aba060436",
                        "stat": {
                            "total": 1,
                            "additions": 9,
                            "deletions": 8,
                            "files": [
                                {
                                    "sha": "6c5018d13293726dd846542ad2fa73a8c3a5d73e",
                                    "filename": "spacy/vectors.pyx",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/a5ea0fdf5a9656525dd8d78b7d6979538727a776/spacy%2Fvectors.pyx",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/a5ea0fdf5a9656525dd8d78b7d6979538727a776/spacy%2Fvectors.pyx",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Fvectors.pyx?ref=a5ea0fdf5a9656525dd8d78b7d6979538727a776",
                                    "patch": "@@ -150,7 +150,7 @@ cdef class Vectors:\n         filled = {row for row in self.key2row.values()}\n         self._unset = {row for row in range(shape[0]) if row not in filled}\n         removed_items = []\n-        for key, row in dict(self.key2row.items()):\n+        for key, row in self.key2row.items():\n             if row >= shape[0]:\n                 self.key2row.pop(key)\n                 removed_items.append((key, row))"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "d5c6eb914c2f945d0f7b0054613379a34e1c4952",
                                    "filename": "spacy/tests/regression/test_issue1518.py",
                                    "status": "added",
                                    "additions": 7,
                                    "deletions": 0,
                                    "changes": 7,
                                    "blob_url": "https://github.com/explosion/spaCy/blob/a5ea0fdf5a9656525dd8d78b7d6979538727a776/spacy%2Ftests%2Fregression%2Ftest_issue1518.py",
                                    "raw_url": "https://github.com/explosion/spaCy/raw/a5ea0fdf5a9656525dd8d78b7d6979538727a776/spacy%2Ftests%2Fregression%2Ftest_issue1518.py",
                                    "contents_url": "https://api.github.com/repos/explosion/spaCy/contents/spacy%2Ftests%2Fregression%2Ftest_issue1518.py?ref=a5ea0fdf5a9656525dd8d78b7d6979538727a776",
                                    "patch": "@@ -0,0 +1,7 @@\n+from ...vectors import Vectors\n+\n+def test_issue1518():\n+    '''Test vectors.resize() works.'''\n+    vectors = Vectors(shape=(10, 10))\n+    vectors.add(u'hello', row=2)\n+    vectors.resize((5, 9))"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.7\npipenv run pip install \"Cython==0.29.22\"\npipenv run pip install \"pathlib\"\npipenv run pip install \"numpy>=1.7\"\npipenv run pip install \"cymem>=1.30,<1.32\"\npipenv run pip install \"preshed>=1.0.0,<2.0.0\"\npipenv run pip install \"thinc>=6.10.0,<6.11.0\"\npipenv run pip install \"murmurhash>=0.28,<0.29\"\npipenv run pip install \"plac<1.0.0,>=0.9.6\"\npipenv run pip install \"ujson>=1.35\"\npipenv run pip install \"six\"\npipenv run pip install \"dill>=0.2,<0.3\"\npipenv run pip install \"requests>=2.13.0,<3.0.0\"\npipenv run pip install \"regex==2017.4.5\"\npipenv run pip install \"pytest>=3.0.6,<4.0.0\"\npipenv run pip install \"ftfy>=4.4.2,<5.0.0\"\npipenv run pip install \"mock>=2.0.0,<3.0.0\"\npipenv run pip install \"msgpack-python==0.5.4\"\npipenv run pip install \"msgpack-numpy==0.4.1\"\npipenv run pip install \"html5lib==1.0b8\"\npipenv run pip install --no-build-isolation --editable .",
                "testSteps": "pipenv run python -m pytest --pyargs spacy/tests/regression/test_issue1518.py",
                "testStepsFull": "pipenv run python -m pytest",
                "requireRebuild": "true"
            }
        ],
        "installSteps": "pipenv --python 3.7\npipenv run pip install -U pip setuptools wheel\npipenv run pip install -r requirements.txt --default-timeout=100\npipenv run pip install Cython==0.29.22\npipenv run pip install attrs==19.1.0\npipenv run pip install --no-build-isolation --editable ."
    },
    {
        "_id": "64995d9ccc5ced31ccde39da",
        "username": "saltstack",
        "repository": "salt",
        "issues": [
            {
                "id": 64477,
                "created_at": "2023-06-13T14:23:06Z",
                "closed_at": "2023-06-20T02:34:34Z",
                "title": "[BUG] file.symlink will not replace/update existing symlink",
                "labels": "Bug,Regression",
                "text_based": false,
                "commits": [
                    {
                        "hash": "77482013d68d2a2019c930ab7638fd6aa009e5e7",
                        "commit_date": "2023-06-20T02:34:21Z",
                        "parents": "ae14412da3abd215f10b8212001ab47807dded39",
                        "stat": {
                            "total": 1,
                            "additions": 27,
                            "deletions": 26,
                            "files": [
                                {
                                    "sha": "d43f01714d98a58f31c893900e33ac13967d3f0c",
                                    "filename": "changelog/64477.fixed.md",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/77482013d68d2a2019c930ab7638fd6aa009e5e7/changelog%2F64477.fixed.md",
                                    "raw_url": "https://github.com/saltstack/salt/raw/77482013d68d2a2019c930ab7638fd6aa009e5e7/changelog%2F64477.fixed.md",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F64477.fixed.md?ref=77482013d68d2a2019c930ab7638fd6aa009e5e7",
                                    "patch": "@@ -0,0 +1 @@\n+Fix file.symlink will not replace/update existing symlink"
                                },
                                {
                                    "sha": "ed9b65be1d682a56841dee15210124a6155ddb3e",
                                    "filename": "salt/states/file.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/saltstack/salt/blob/77482013d68d2a2019c930ab7638fd6aa009e5e7/salt%2Fstates%2Ffile.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/77482013d68d2a2019c930ab7638fd6aa009e5e7/salt%2Fstates%2Ffile.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Fstates%2Ffile.py?ref=77482013d68d2a2019c930ab7638fd6aa009e5e7",
                                    "patch": "@@ -1791,9 +1791,11 @@ def symlink(\n \n     if __salt__[\"file.is_link\"](name):\n         # The link exists, verify that it matches the target\n-        if os.path.normpath(__salt__[\"file.readlink\"](name)) == os.path.normpath(\n+        if os.path.normpath(__salt__[\"file.readlink\"](name)) != os.path.normpath(\n             target\n         ):\n+            __salt__[\"file.remove\"](name)\n+        else:\n             if _check_symlink_ownership(name, user, group, win_owner):\n                 # The link looks good!\n                 if salt.utils.platform.is_windows():"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a28ab64a35f67b92644068bf534bb3a2b3d9d06f",
                                    "filename": "tests/pytests/functional/states/test_file.py",
                                    "status": "modified",
                                    "additions": 22,
                                    "deletions": 0,
                                    "changes": 22,
                                    "blob_url": "https://github.com/saltstack/salt/blob/77482013d68d2a2019c930ab7638fd6aa009e5e7/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_file.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/77482013d68d2a2019c930ab7638fd6aa009e5e7/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_file.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_file.py?ref=77482013d68d2a2019c930ab7638fd6aa009e5e7",
                                    "patch": "@@ -201,3 +201,25 @@ def test_file_managed_web_source_etag_operation(\n \n     # The modified time of the cached file now changes\n     assert cached_file_mtime != os.path.getmtime(cached_file)\n+\n+\n+def test_file_symlink_replace_existing_link(states, tmp_path):\n+    # symlink name and target for state\n+    name = tmp_path / \"foo\"\n+    target = tmp_path / \"baz\"\n+\n+    # create existing symlink to replace\n+    old_target = tmp_path / \"bar\"\n+    name.symlink_to(old_target)\n+\n+    ret = states.file.symlink(\n+        name=str(name),\n+        target=str(target),\n+    )\n+\n+    assert ret.filtered == {\n+        \"name\": str(name),\n+        \"changes\": {\"new\": str(name)},\n+        \"comment\": f\"Created new symlink {str(name)} -> {str(target)}\",\n+        \"result\": True,\n+    }"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"test-3(coverage=False)\" -- tests/pytests/functional/states/test_file.py",
                "testStepsFull": "pipenv run python -m nox -e \"test-3(coverage=False)\""
            },
            {
                "id": 64430,
                "created_at": "2023-06-06T20:17:06Z",
                "closed_at": "2023-06-12T12:55:57Z",
                "title": "[BUG] regression for user.present on handling groups with dupe GIDs",
                "labels": "Bug,Regression",
                "text_based": false,
                "commits": [
                    {
                        "hash": "2e4c57edaa096cc5798c1174c93fa263287171b8",
                        "commit_date": "2023-06-09T13:07:01Z",
                        "parents": "0ffbb22b44a951c50b170db091f8a25faa415f3d",
                        "stat": {
                            "total": 23,
                            "additions": 133,
                            "deletions": 110,
                            "files": [
                                {
                                    "sha": "d57b1ff9b07dd37525b42df30f2639a3591029fb",
                                    "filename": "changelog/64430.fixed.md",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/2e4c57edaa096cc5798c1174c93fa263287171b8/changelog%2F64430.fixed.md",
                                    "raw_url": "https://github.com/saltstack/salt/raw/2e4c57edaa096cc5798c1174c93fa263287171b8/changelog%2F64430.fixed.md",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F64430.fixed.md?ref=2e4c57edaa096cc5798c1174c93fa263287171b8",
                                    "patch": "@@ -0,0 +1 @@\n+Fix regression for user.present on handling groups with dupe GIDs"
                                },
                                {
                                    "sha": "e0f620cac891edc91d8f80a3a6aa6ee9c132a71e",
                                    "filename": "salt/states/user.py",
                                    "status": "modified",
                                    "additions": 61,
                                    "deletions": 14,
                                    "changes": 75,
                                    "blob_url": "https://github.com/saltstack/salt/blob/2e4c57edaa096cc5798c1174c93fa263287171b8/salt%2Fstates%2Fuser.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/2e4c57edaa096cc5798c1174c93fa263287171b8/salt%2Fstates%2Fuser.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Fstates%2Fuser.py?ref=2e4c57edaa096cc5798c1174c93fa263287171b8",
                                    "patch": "@@ -40,11 +40,29 @@ def _group_changes(cur, wanted, remove=False):\n     \"\"\"\n     Determine if the groups need to be changed\n     \"\"\"\n-    old = set(cur)\n-    new = set(wanted)\n-    if (remove and old != new) or (not remove and not new.issubset(old)):\n-        return True\n-    return False\n+    cur = set(cur)\n+    wanted = set(wanted)\n+\n+    if cur == wanted or (not remove and wanted.issubset(cur)):\n+        return False\n+\n+    all_grps = {name: __salt__[\"group.info\"](name) for name in cur.union(wanted)}\n+\n+    if remove:\n+        diff = wanted.symmetric_difference(cur)\n+    else:\n+        diff = wanted.difference(cur)\n+\n+    remain = list(diff)\n+    for diff_grp in diff:\n+        for grp, info in all_grps.items():\n+            if grp == diff_grp:\n+                continue\n+            if all_grps[diff_grp][\"gid\"] == info[\"gid\"]:\n+                # dupe detected\n+                remain.remove(diff_grp)\n+\n+    return bool(remain)\n \n \n def _changes(\n@@ -100,6 +118,15 @@ def _changes(\n \n     change = {}\n     wanted_groups = sorted(set((groups or []) + (optional_groups or [])))\n+    lusr_groups_gids = [\n+        __salt__[\"file.group_to_gid\"](gname) for gname in lusr[\"groups\"]\n+    ]\n+    dupe_groups = {}\n+    for idx, _gid in enumerate(lusr_groups_gids):\n+        if lusr_groups_gids.count(_gid) > 1:\n+            if _gid not in dupe_groups:\n+                dupe_groups[_gid] = []\n+            dupe_groups[_gid].append(lusr[\"groups\"][idx])\n     if not remove_groups:\n         wanted_groups = sorted(set(wanted_groups + lusr[\"groups\"]))\n     if uid and lusr[\"uid\"] != uid:\n@@ -109,24 +136,44 @@ def _changes(\n     default_grp = __salt__[\"file.gid_to_group\"](gid if gid is not None else lusr[\"gid\"])\n     old_default_grp = __salt__[\"file.gid_to_group\"](lusr[\"gid\"])\n     # Remove the default group from the list for comparison purposes.\n-    if default_grp in lusr[\"groups\"]:\n-        lusr[\"groups\"].remove(default_grp)\n+    # Remove default group from wanted_groups, as this requirement is\n+    # already met\n+    if default_grp in lusr[\"groups\"] or default_grp in wanted_groups:\n+        if default_grp in salt.utils.data.flatten(dupe_groups.values()):\n+            dupe_gid = __salt__[\"file.group_to_gid\"](default_grp)\n+            for gname in dupe_groups[dupe_gid]:\n+                if gname in lusr[\"groups\"]:\n+                    lusr[\"groups\"].remove(gname)\n+                if gname in wanted_groups:\n+                    wanted_groups.remove(gname)\n+        else:\n+            if default_grp in lusr[\"groups\"]:\n+                lusr[\"groups\"].remove(default_grp)\n+            if default_grp in wanted_groups:\n+                wanted_groups.remove(default_grp)\n     # If the group is being changed, make sure that the old primary group is\n     # also removed from the list. Otherwise, if a user's gid is being changed\n     # and their old primary group is reassigned as an additional group, Salt\n     # will not properly detect the need for the change.\n     if old_default_grp != default_grp and old_default_grp in lusr[\"groups\"]:\n-        lusr[\"groups\"].remove(old_default_grp)\n+        if old_default_grp in salt.utils.data.flatten(dupe_groups.values()):\n+            dupe_gid = __salt__[\"file.group_to_gid\"](old_default_grp)\n+            for gname in dupe_groups[dupe_gid]:\n+                lusr[\"groups\"].remove(gname)\n+        else:\n+            lusr[\"groups\"].remove(old_default_grp)\n     # If there's a group by the same name as the user, remove it from the list\n     # for comparison purposes.\n     if name in lusr[\"groups\"] and name not in wanted_groups:\n-        lusr[\"groups\"].remove(name)\n-    # Remove default group from wanted_groups, as this requirement is\n-    # already met\n-    if default_grp in wanted_groups:\n-        wanted_groups.remove(default_grp)\n+        if name in salt.utils.data.flatten(dupe_groups.values()):\n+            dupe_gid = __salt__[\"file.group_to_gid\"](name)\n+            for gname in dupe_groups[dupe_gid]:\n+                lusr[\"groups\"].remove(gname)\n+        else:\n+            lusr[\"groups\"].remove(name)\n     if _group_changes(lusr[\"groups\"], wanted_groups, remove_groups):\n-        change[\"groups\"] = wanted_groups\n+        if wanted_groups or remove_groups:\n+            change[\"groups\"] = wanted_groups\n     if home and lusr[\"home\"] != home:\n         change[\"home\"] = home\n     if createhome:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "0476cee40f8abd97e7f61a921c10b6b944e33c0e",
                                    "filename": "tests/pytests/unit/states/test_user.py",
                                    "status": "modified",
                                    "additions": 48,
                                    "deletions": 9,
                                    "changes": 57,
                                    "blob_url": "https://github.com/saltstack/salt/blob/2e4c57edaa096cc5798c1174c93fa263287171b8/tests%2Fpytests%2Funit%2Fstates%2Ftest_user.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/2e4c57edaa096cc5798c1174c93fa263287171b8/tests%2Fpytests%2Funit%2Fstates%2Ftest_user.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Funit%2Fstates%2Ftest_user.py?ref=2e4c57edaa096cc5798c1174c93fa263287171b8",
                                    "patch": "@@ -123,8 +123,8 @@ def test_present_invalid_gid_change():\n     )\n     dunder_salt = {\n         \"user.info\": mock_info,\n-        \"file.group_to_gid\": MagicMock(side_effect=[\"foo\"]),\n-        \"file.gid_to_group\": MagicMock(side_effect=[5000, 5000]),\n+        \"file.group_to_gid\": MagicMock(return_value=\"foo\"),\n+        \"file.gid_to_group\": MagicMock(return_value=5000),\n     }\n     with patch.dict(user.__grains__, {\"kernel\": \"Linux\"}), patch.dict(\n         user.__salt__, dunder_salt\n@@ -148,8 +148,8 @@ def test_present_invalid_uid_gid_change():\n     )\n     dunder_salt = {\n         \"user.info\": mock_info,\n-        \"file.group_to_gid\": MagicMock(side_effect=[\"foo\"]),\n-        \"file.gid_to_group\": MagicMock(side_effect=[5000, 5000]),\n+        \"file.group_to_gid\": MagicMock(return_value=\"foo\"),\n+        \"file.gid_to_group\": MagicMock(return_value=5000),\n     }\n     with patch.dict(user.__grains__, {\"kernel\": \"Linux\"}), patch.dict(\n         user.__salt__, dunder_salt\n@@ -179,7 +179,7 @@ def test_present_uid_gid_change():\n     # get the before/after for the changes dict, and one last time to\n     # confirm that no changes still need to be made.\n     mock_info = MagicMock(side_effect=[before, before, after, after])\n-    mock_group_to_gid = MagicMock(side_effect=[5000, 5001])\n+    mock_group_to_gid = MagicMock(side_effect=[5000, 5000, 5001, 5001])\n     mock_gid_to_group = MagicMock(\n         side_effect=[\"othergroup\", \"foo\", \"othergroup\", \"othergroup\"]\n     )\n@@ -254,12 +254,11 @@ def test_changes():\n         \"file.gid_to_group\": MagicMock(side_effect=[5000, 5000]),\n     }\n \n-    def mock_exists(*args):\n-        return True\n-\n     with patch.dict(user.__grains__, {\"kernel\": \"Linux\"}), patch.dict(\n         user.__salt__, dunder_salt\n-    ), patch.dict(user.__opts__, {\"test\": False}), patch(\"os.path.isdir\", mock_exists):\n+    ), patch.dict(user.__opts__, {\"test\": False}), patch(\n+        \"os.path.isdir\", MagicMock(return_value=True)\n+    ):\n         ret = user._changes(\"foo\", maxdays=999999, inactdays=0, warndays=7)\n         assert ret == {\n             \"maxdays\": 999999,\n@@ -459,3 +458,43 @@ def test_present_password_unlock():\n         else:\n             unlock_password.assert_called_once()\n             unlock_account.assert_not_called()\n+\n+\n+@pytest.mark.parametrize(\n+    \"current,wanted,remove,return_value,expected\",\n+    [\n+        ([\"grp1\"], [\"grp1\"], False, MagicMock(return_value={\"gid\": 100}), False),\n+        (\n+            [\"grp1\"],\n+            [\"grp1\", \"grp2\"],\n+            False,\n+            MagicMock(side_effect=[{\"gid\": 100}, {\"gid\": 200}]),\n+            True,\n+        ),\n+        (\n+            [\"grp1\"],\n+            [\"grp1\", \"grp2\"],\n+            False,\n+            MagicMock(side_effect=[{\"gid\": 100}, {\"gid\": 100}]),\n+            False,\n+        ),\n+        (\n+            [\"grp1\", \"grp2\"],\n+            [\"grp1\"],\n+            True,\n+            MagicMock(side_effect=[{\"gid\": 100}, {\"gid\": 200}]),\n+            True,\n+        ),\n+        (\n+            [\"grp1\", \"grp2\"],\n+            [\"grp1\"],\n+            True,\n+            MagicMock(side_effect=[{\"gid\": 100}, {\"gid\": 100}]),\n+            False,\n+        ),\n+    ],\n+)\n+def test__group_changes(current, wanted, remove, return_value, expected):\n+    with patch.dict(user.__salt__, {\"group.info\": return_value}):\n+        ret = user._group_changes(current, wanted, remove)\n+    assert ret == expected"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"test-3(coverage=False)\"  -- tests/pytests/unit/states/test_user.py\n",
                "testStepsFull": "pipenv run python -m nox -e \"test-3(coverage=False)\""
            },
            {
                "id": 64280,
                "created_at": "2023-05-15T15:00:52Z",
                "closed_at": "2023-06-12T12:55:56Z",
                "title": "[BUG] Mako error on pillars render after upgrade to 3006.1",
                "labels": "Bug,Regression,Confirmed",
                "text_based": false,
                "commits": [
                    {
                        "hash": "560ab52ccf94c7974d5a418dfbba7409e0493066",
                        "commit_date": "2023-05-31T14:39:59Z",
                        "parents": "3ae4e2aba52e7387901e78c028e4d4d33388b191",
                        "stat": {
                            "total": 2,
                            "additions": 35,
                            "deletions": 33,
                            "files": [
                                {
                                    "sha": "5a9b905dd0a0027cacb33e406893de08595927cb",
                                    "filename": "changelog/64280.fixed.md",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/560ab52ccf94c7974d5a418dfbba7409e0493066/changelog%2F64280.fixed.md",
                                    "raw_url": "https://github.com/saltstack/salt/raw/560ab52ccf94c7974d5a418dfbba7409e0493066/changelog%2F64280.fixed.md",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F64280.fixed.md?ref=560ab52ccf94c7974d5a418dfbba7409e0493066",
                                    "patch": "@@ -0,0 +1 @@\n+Fixed file client private attribute reference on `SaltMakoTemplateLookup`"
                                },
                                {
                                    "sha": "4397ae8cc7d917cea853bee2a9d43d36e3b9cfd2",
                                    "filename": "salt/utils/mako.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 2,
                                    "changes": 6,
                                    "blob_url": "https://github.com/saltstack/salt/blob/560ab52ccf94c7974d5a418dfbba7409e0493066/salt%2Futils%2Fmako.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/560ab52ccf94c7974d5a418dfbba7409e0493066/salt%2Futils%2Fmako.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Futils%2Fmako.py?ref=560ab52ccf94c7974d5a418dfbba7409e0493066",
                                    "patch": "@@ -99,8 +99,10 @@ def cache_file(self, fpath):\n                 )\n \n         def destroy(self):\n-            if self.client:\n+            if self._file_client:\n+                file_client = self._file_client\n+                self._file_client = None\n                 try:\n-                    self.client.destroy()\n+                    file_client.destroy()\n                 except AttributeError:\n                     pass"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "952cf44652ee351895467b98b33010d80e1e394c",
                                    "filename": "tests/pytests/unit/utils/test_mako.py",
                                    "status": "added",
                                    "additions": 28,
                                    "deletions": 0,
                                    "changes": 28,
                                    "blob_url": "https://github.com/saltstack/salt/blob/560ab52ccf94c7974d5a418dfbba7409e0493066/tests%2Fpytests%2Funit%2Futils%2Ftest_mako.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/560ab52ccf94c7974d5a418dfbba7409e0493066/tests%2Fpytests%2Funit%2Futils%2Ftest_mako.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Funit%2Futils%2Ftest_mako.py?ref=560ab52ccf94c7974d5a418dfbba7409e0493066",
                                    "patch": "@@ -0,0 +1,28 @@\n+import pytest\n+\n+from tests.support.mock import Mock, call, patch\n+\n+pytest.importorskip(\"mako\")\n+\n+# This import needs to be after the above importorskip so that no ImportError\n+# is raised if Mako is not installed\n+from salt.utils.mako import SaltMakoTemplateLookup\n+\n+\n+def test_mako_template_lookup(minion_opts):\n+    \"\"\"\n+    The shudown method can be called without raising an exception when the\n+    file_client does not have a destroy method\n+    \"\"\"\n+    # Test SaltCacheLoader creating and destroying the file client created\n+    file_client = Mock()\n+    with patch(\"salt.fileclient.get_file_client\", return_value=file_client):\n+        loader = SaltMakoTemplateLookup(minion_opts)\n+        assert loader._file_client is None\n+        assert loader.file_client() is file_client\n+        assert loader._file_client is file_client\n+        try:\n+            loader.destroy()\n+        except AttributeError:\n+            pytest.fail(\"Regression when calling SaltMakoTemplateLookup.destroy()\")\n+        assert file_client.mock_calls == [call.destroy()]"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"test-3(coverage=False)\"  -- tests/pytests/unit/utils/test_mako.py",
                "testStepsFull": "pipenv run python -m nox -e \"test-3(coverage=False)\""
            },
            {
                "id": 62988,
                "created_at": "2022-11-01T12:45:09Z",
                "closed_at": "2022-11-07T19:58:46Z",
                "title": "[BUG] module.run new style defintion is identified as legacy",
                "labels": "State-Module,Bug,State-Compiler,needs-triage",
                "text_based": false,
                "commits": [
                    {
                        "hash": "29812c3dcdfc7cb6b54480e796187ec57a27c18d",
                        "commit_date": "2022-11-07T19:58:42Z",
                        "parents": "0f6b9efbea38c0bde30d0197ea24206cea1caed7",
                        "stat": {
                            "total": 3,
                            "additions": 68,
                            "deletions": 65,
                            "files": [
                                {
                                    "sha": "b525e2f38ac8276ac63da818d0e772bcd41cf638",
                                    "filename": "changelog/62988.fixed",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/29812c3dcdfc7cb6b54480e796187ec57a27c18d/changelog%2F62988.fixed",
                                    "raw_url": "https://github.com/saltstack/salt/raw/29812c3dcdfc7cb6b54480e796187ec57a27c18d/changelog%2F62988.fixed",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F62988.fixed?ref=29812c3dcdfc7cb6b54480e796187ec57a27c18d",
                                    "patch": "@@ -0,0 +1 @@\n+Fixed bug where module.wait states were detected as running legacy module.run syntax"
                                },
                                {
                                    "sha": "5ad87b053d2767b18bc579961ea6267cca3d9c5e",
                                    "filename": "salt/states/module.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 2,
                                    "changes": 6,
                                    "blob_url": "https://github.com/saltstack/salt/blob/29812c3dcdfc7cb6b54480e796187ec57a27c18d/salt%2Fstates%2Fmodule.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/29812c3dcdfc7cb6b54480e796187ec57a27c18d/salt%2Fstates%2Fmodule.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Fstates%2Fmodule.py?ref=29812c3dcdfc7cb6b54480e796187ec57a27c18d",
                                    "patch": "@@ -376,8 +376,10 @@ def run(**kwargs):\n     legacy_run = False\n \n     keys = list(kwargs)\n-    if \"name\" in keys:\n-        keys.remove(\"name\")\n+    ignored_kwargs = [\"name\", \"__reqs__\", \"sfun\"]\n+    for item in ignored_kwargs:\n+        if item in keys:\n+            keys.remove(item)\n \n     # The rest of the keys should be function names for new-style syntax\n     for name in keys:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "bf2410ab529de6d8379a5e5c2e3d52c338f165ef",
                                    "filename": "tests/pytests/functional/states/test_module.py",
                                    "status": "modified",
                                    "additions": 60,
                                    "deletions": 1,
                                    "changes": 61,
                                    "blob_url": "https://github.com/saltstack/salt/blob/29812c3dcdfc7cb6b54480e796187ec57a27c18d/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_module.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/29812c3dcdfc7cb6b54480e796187ec57a27c18d/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_module.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Ffunctional%2Fstates%2Ftest_module.py?ref=29812c3dcdfc7cb6b54480e796187ec57a27c18d",
                                    "patch": "@@ -82,7 +82,66 @@ def test_issue_58763_b(tmp_path, modules, state_tree, caplog):\n                 mods=\"issue-58763\",\n             )\n             assert len(ret.raw) == 1\n-            print(ret)\n             for k in ret.raw:\n                 assert ret.raw[k][\"result\"] is True\n             assert \"Detected legacy module.run syntax: test.ping\" in caplog.messages\n+\n+\n+@pytest.mark.slow_test\n+def test_issue_62988_a(tmp_path, modules, state_tree, caplog):\n+\n+    venv_dir = tmp_path / \"issue-2028-pip-installed\"\n+\n+    sls_contents = dedent(\n+        \"\"\"\n+    test_foo:\n+      test.succeed_with_changes\n+\n+    run_new:\n+      module.wait:\n+        - test.random_hash:\n+          - size: 10\n+          - hash_type: md5\n+        - watch:\n+          - test: test_foo\n+    \"\"\"\n+    )\n+    with pytest.helpers.temp_file(\"issue-62988.sls\", sls_contents, state_tree):\n+        with caplog.at_level(logging.DEBUG):\n+            ret = modules.state.sls(\n+                mods=\"issue-62988\",\n+            )\n+            assert len(ret.raw) == 2\n+            for k in ret.raw:\n+                assert ret.raw[k][\"result\"] is True\n+            assert \"Using new style module.run syntax: run_new\" in caplog.messages\n+\n+\n+@pytest.mark.slow_test\n+def test_issue_62988_b(tmp_path, modules, state_tree, caplog):\n+\n+    venv_dir = tmp_path / \"issue-2028-pip-installed\"\n+\n+    sls_contents = dedent(\n+        \"\"\"\n+    test_foo:\n+      test.succeed_with_changes:\n+        - watch_in:\n+          - module: run_new\n+\n+    run_new:\n+      module.wait:\n+        - test.random_hash:\n+          - size: 10\n+          - hash_type: md5\n+    \"\"\"\n+    )\n+    with pytest.helpers.temp_file(\"issue-62988.sls\", sls_contents, state_tree):\n+        with caplog.at_level(logging.DEBUG):\n+            ret = modules.state.sls(\n+                mods=\"issue-62988\",\n+            )\n+            assert len(ret.raw) == 2\n+            for k in ret.raw:\n+                assert ret.raw[k][\"result\"] is True\n+            assert \"Using new style module.run syntax: run_new\" in caplog.messages"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"test-3(coverage=False)\"  -- tests/pytests/functional/states/test_module.py --run-slow",
                "testStepsFull": "pipenv run python -m nox -e \"test-3(coverage=False)\"  --  --run-slow"
            },
            {
                "id": 62336,
                "created_at": "2022-07-17T17:02:54Z",
                "closed_at": "2022-07-20T19:13:50Z",
                "title": "[BUG] no access to opts and sls vars in pyobjects renderer",
                "labels": "Bug,Renderers",
                "text_based": false,
                "commits": [
                    {
                        "hash": "58e3546613a8b2d2614c1e6676742436faa948d2",
                        "commit_date": "2022-07-20T19:13:39Z",
                        "parents": "6ddad5a10c3124a79e51b424e7b7aeee7a9cc598",
                        "stat": {
                            "total": 0,
                            "additions": 96,
                            "deletions": 96,
                            "files": [
                                {
                                    "sha": "300de31a0176d754a639f96f33e86df666bb19f4",
                                    "filename": "changelog/62336.fixed",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/58e3546613a8b2d2614c1e6676742436faa948d2/changelog%2F62336.fixed",
                                    "raw_url": "https://github.com/saltstack/salt/raw/58e3546613a8b2d2614c1e6676742436faa948d2/changelog%2F62336.fixed",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F62336.fixed?ref=58e3546613a8b2d2614c1e6676742436faa948d2",
                                    "patch": "@@ -0,0 +1 @@\n+Fix pyobjects renderer access to opts and sls"
                                },
                                {
                                    "sha": "b65898d1bd588b9d0aec50de923f7bae5276a1c6",
                                    "filename": "salt/renderers/pyobjects.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/saltstack/salt/blob/58e3546613a8b2d2614c1e6676742436faa948d2/salt%2Frenderers%2Fpyobjects.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/58e3546613a8b2d2614c1e6676742436faa948d2/salt%2Frenderers%2Fpyobjects.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Frenderers%2Fpyobjects.py?ref=58e3546613a8b2d2614c1e6676742436faa948d2",
                                    "patch": "@@ -210,6 +210,24 @@\n     value = __salt__['config.get']('foo:bar:baz', 'qux')\n \n \n+Opts dictionary and SLS name\n+----------------------------\n+\n+Pyobjects provides variable access to the minion options dictionary and the SLS\n+name that the code resides in. These variables are the same as the `opts` and\n+`sls` variables available in the Jinja renderer.\n+\n+The following lines show how to access that information.\n+\n+.. code-block:: python\n+   :linenos:\n+\n+    #!pyobjects\n+\n+    test_mode = __opts__[\"test\"]\n+    sls_name = __sls__\n+\n+\n Map Data\n --------\n \n@@ -400,6 +418,8 @@ def render(template, saltenv=\"base\", sls=\"\", salt_data=True, **kwargs):\n                 \"__salt__\": __salt__,\n                 \"__pillar__\": __pillar__,\n                 \"__grains__\": __grains__,\n+                \"__opts__\": __opts__,\n+                \"__sls__\": sls,\n             }\n         )\n     except NameError:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "5efd7f606da6ff67cb6f8aa838ed432225bdd04e",
                                    "filename": "tests/pytests/unit/utils/test_pyobjects.py",
                                    "status": "added",
                                    "additions": 75,
                                    "deletions": 0,
                                    "changes": 75,
                                    "blob_url": "https://github.com/saltstack/salt/blob/58e3546613a8b2d2614c1e6676742436faa948d2/tests%2Fpytests%2Funit%2Futils%2Ftest_pyobjects.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/58e3546613a8b2d2614c1e6676742436faa948d2/tests%2Fpytests%2Funit%2Futils%2Ftest_pyobjects.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Funit%2Futils%2Ftest_pyobjects.py?ref=58e3546613a8b2d2614c1e6676742436faa948d2",
                                    "patch": "@@ -0,0 +1,75 @@\n+import logging\n+\n+import pytest\n+import salt.config\n+import salt.renderers.pyobjects as pyobjects\n+from salt.utils.odict import OrderedDict\n+from tests.support.mock import MagicMock\n+\n+log = logging.getLogger(__name__)\n+\n+\n+@pytest.fixture\n+def cache_dir(tmp_path):\n+    cachedir = tmp_path / \"cachedir\"\n+    cachedir.mkdir()\n+    return cachedir\n+\n+\n+@pytest.fixture\n+def minion_config(cache_dir):\n+    opts = salt.config.DEFAULT_MINION_OPTS.copy()\n+    opts[\"cachedir\"] = str(cache_dir)\n+    opts[\"file_client\"] = \"local\"\n+    opts[\"id\"] = \"testminion\"\n+    return opts\n+\n+\n+@pytest.fixture()\n+def configure_loader_modules(minion_config):\n+    pillar = MagicMock(return_value={})\n+    return {\n+        pyobjects: {\n+            \"__opts__\": minion_config,\n+            \"__pillar__\": pillar,\n+            \"__salt__\": {\n+                \"config.get\": MagicMock(),\n+                \"grains.get\": MagicMock(),\n+                \"mine.get\": MagicMock(),\n+                \"pillar.get\": MagicMock(),\n+            },\n+        },\n+    }\n+\n+\n+@pytest.fixture\n+def pyobjects_template():\n+    class Template:\n+        def readlines():  # pylint: disable=no-method-argument\n+            return [\n+                \"#!pyobjects\",\n+                \"state_id = __sls__ + '_' + __opts__['id']\",\n+                \"File.directory(state_id, name='/tmp', mode='1777', owner='root', group='root')\",\n+            ]\n+\n+    return Template\n+\n+\n+@pytest.mark.slow_test\n+def test_opts_and_sls_access(pyobjects_template):\n+    ret = pyobjects.render(pyobjects_template, sls=\"pyobj.runtest\")\n+    assert ret == OrderedDict(\n+        [\n+            (\n+                \"pyobj.runtest_testminion\",\n+                {\n+                    \"file.directory\": [\n+                        {\"group\": \"root\"},\n+                        {\"mode\": \"1777\"},\n+                        {\"name\": \"/tmp\"},\n+                        {\"owner\": \"root\"},\n+                    ]\n+                },\n+            ),\n+        ]\n+    )"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"pytest-3(coverage=False)\"  -- tests/pytests/unit/utils/test_pyobjects.py --run-slow",
                "testStepsFull": "pipenv run python -m nox -e \"pytest-3(coverage=False)\""
            },
            {
                "id": 60391,
                "created_at": "2021-06-17T10:18:08Z",
                "closed_at": "2022-04-18T11:15:03Z",
                "title": "[BUG] TornadoTimeoutError cannot be caught",
                "labels": "Bug,severity-medium,Core,dependency",
                "text_based": false,
                "commits": [
                    {
                        "hash": "aba1503f14a0b7279a615e3fb3760b6a353b4284",
                        "commit_date": "2021-10-22T11:20:39Z",
                        "parents": "d3dc1eeed1d153c5ab5db042bf4ecd6dc7018cae",
                        "stat": {
                            "total": 36,
                            "additions": 143,
                            "deletions": 107,
                            "files": [
                                {
                                    "sha": "97a11dd9241bc4ec90343a455a0ffdb5c7a05995",
                                    "filename": "changelog/60391.fixed",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/aba1503f14a0b7279a615e3fb3760b6a353b4284/changelog%2F60391.fixed",
                                    "raw_url": "https://github.com/saltstack/salt/raw/aba1503f14a0b7279a615e3fb3760b6a353b4284/changelog%2F60391.fixed",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F60391.fixed?ref=aba1503f14a0b7279a615e3fb3760b6a353b4284",
                                    "patch": "@@ -0,0 +1 @@\n+Handle signals and properly exit, instead of raising exceptions."
                                },
                                {
                                    "sha": "97a11dd9241bc4ec90343a455a0ffdb5c7a05995",
                                    "filename": "changelog/60963.fixed",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/aba1503f14a0b7279a615e3fb3760b6a353b4284/changelog%2F60963.fixed",
                                    "raw_url": "https://github.com/saltstack/salt/raw/aba1503f14a0b7279a615e3fb3760b6a353b4284/changelog%2F60963.fixed",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/changelog%2F60963.fixed?ref=aba1503f14a0b7279a615e3fb3760b6a353b4284",
                                    "patch": "@@ -0,0 +1 @@\n+Handle signals and properly exit, instead of raising exceptions."
                                },
                                {
                                    "sha": "93eab0f702e0a1381b00172f2257ec8b419a0296",
                                    "filename": "salt/scripts.py",
                                    "status": "modified",
                                    "additions": 21,
                                    "deletions": 35,
                                    "changes": 56,
                                    "blob_url": "https://github.com/saltstack/salt/blob/aba1503f14a0b7279a615e3fb3760b6a353b4284/salt%2Fscripts.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/aba1503f14a0b7279a615e3fb3760b6a353b4284/salt%2Fscripts.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Fscripts.py?ref=aba1503f14a0b7279a615e3fb3760b6a353b4284",
                                    "patch": "@@ -22,31 +22,7 @@\n     raise SystemExit(salt.defaults.exitcodes.EX_GENERIC)\n \n \n-def _handle_interrupt(exc, original_exc, hardfail=False, trace=\"\"):\n-    \"\"\"\n-    if hardfailing:\n-        If we got the original stacktrace, log it\n-        If all cases, raise the original exception\n-        but this is logically part the initial\n-        stack.\n-    else just let salt exit gracefully\n-\n-    \"\"\"\n-    if hardfail:\n-        if trace:\n-            log.error(trace)\n-        raise original_exc\n-    else:\n-        raise exc\n-\n-\n def _handle_signals(client, signum, sigframe):\n-    try:\n-        # This raises AttributeError on Python 3.4 and 3.5 if there is no current exception.\n-        # Ref: https://bugs.python.org/issue23003\n-        trace = traceback.format_exc()\n-    except AttributeError:\n-        trace = \"\"\n     try:\n         hardcrash = client.options.hard_crash\n     except (AttributeError, KeyError):\n@@ -69,17 +45,25 @@ def _handle_signals(client, signum, sigframe):\n     else:\n         exit_msg = None\n \n-    _handle_interrupt(\n-        SystemExit(exit_msg),\n-        Exception(\"\\nExiting with hard crash on Ctrl-c\"),\n-        hardcrash,\n-        trace=trace,\n-    )\n+    if exit_msg is None and hardcrash:\n+        exit_msg = \"\\nExiting with hard crash on Ctrl-c\"\n+    if exit_msg:\n+        print(exit_msg, file=sys.stderr, flush=True)\n+    if hardcrash:\n+        try:\n+            # This raises AttributeError on Python 3.4 and 3.5 if there is no current exception.\n+            # Ref: https://bugs.python.org/issue23003\n+            trace = traceback.format_exc()\n+            log.error(trace)\n+        except AttributeError:\n+            pass\n+        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n+    sys.exit(salt.defaults.exitcodes.EX_OK)\n \n \n def _install_signal_handlers(client):\n     # Install the SIGINT/SIGTERM handlers if not done so far\n-    if signal.getsignal(signal.SIGINT) is signal.SIG_DFL:\n+    if signal.getsignal(signal.SIGINT) in (signal.SIG_DFL, signal.default_int_handler):\n         # No custom signal handling was added, install our own\n         signal.signal(signal.SIGINT, functools.partial(_handle_signals, client))\n \n@@ -474,12 +458,14 @@ def salt_ssh():\n         _install_signal_handlers(client)\n         client.run()\n     except SaltClientError as err:\n-        trace = traceback.format_exc()\n+        print(str(err), file=sys.stderr, flush=True)\n         try:\n-            hardcrash = client.options.hard_crash\n+            if client.options.hard_crash:\n+                trace = traceback.format_exc()\n+                log.error(trace)\n         except (AttributeError, KeyError):\n-            hardcrash = False\n-        _handle_interrupt(SystemExit(err), err, hardcrash, trace=trace)\n+            pass\n+        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n \n \n def salt_cloud():"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e5de16d6d4232eeb79a5867a623b3f5eaaf56386",
                                    "filename": "tests/pytests/integration/cli/test_salt.py",
                                    "status": "modified",
                                    "additions": 84,
                                    "deletions": 1,
                                    "changes": 85,
                                    "blob_url": "https://github.com/saltstack/salt/blob/aba1503f14a0b7279a615e3fb3760b6a353b4284/tests%2Fpytests%2Fintegration%2Fcli%2Ftest_salt.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/aba1503f14a0b7279a615e3fb3760b6a353b4284/tests%2Fpytests%2Fintegration%2Fcli%2Ftest_salt.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Fintegration%2Fcli%2Ftest_salt.py?ref=aba1503f14a0b7279a615e3fb3760b6a353b4284",
                                    "patch": "@@ -1,14 +1,19 @@\n \"\"\"\n :codeauthor: Thayne Harbaugh (tharbaug@adobe.com)\n \"\"\"\n-\n import logging\n import os\n import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+import time\n \n import pytest\n import salt.defaults.exitcodes\n import salt.utils.path\n+from saltfactories.utils.processes import ProcessResult\n \n log = logging.getLogger(__name__)\n \n@@ -124,3 +129,81 @@ def test_exit_status_correct_usage(salt_cli, salt_minion):\n     \"\"\"\n     ret = salt_cli.run(\"test.ping\", minion_tgt=salt_minion.id)\n     assert ret.exitcode == salt.defaults.exitcodes.EX_OK, ret\n+\n+\n+@pytest.mark.slow_test\n+@pytest.mark.skip_on_windows(reason=\"Windows does not support SIGINT\")\n+def test_interrupt_on_long_running_job(salt_cli, salt_master, salt_minion):\n+    \"\"\"\n+    Ensure that a call to ``salt`` that is taking too long, when a user\n+    hits CTRL-C, that the JID is printed to the console.\n+\n+    Refer to https://github.com/saltstack/salt/issues/60963 for more details\n+    \"\"\"\n+    ret = salt_cli.run(\"test.sleep\", \"1\", minion_tgt=salt_minion.id)\n+    assert ret.exitcode == 0\n+    assert ret.json is True\n+    terminal_stdout = tempfile.SpooledTemporaryFile(512000, buffering=0)\n+    terminal_stderr = tempfile.SpooledTemporaryFile(512000, buffering=0)\n+    cmdline = [\n+        sys.executable,\n+        salt_cli.get_script_path(),\n+        \"--config-dir={}\".format(salt_master.config_dir),\n+        salt_minion.id,\n+        \"test.sleep\",\n+        \"30\",\n+    ]\n+    proc = subprocess.Popen(\n+        cmdline,\n+        shell=False,\n+        stdout=terminal_stdout,\n+        stderr=terminal_stderr,\n+        universal_newlines=True,\n+    )\n+    try:\n+        # Make sure it actually starts\n+        proc.wait(1)\n+    except subprocess.TimeoutExpired:\n+        pass\n+    else:\n+        pytest.fail(\"The test process failed to start\")\n+\n+    time.sleep(2)\n+    # Send CTRL-C to the process\n+    proc.send_signal(signal.SIGINT)\n+    with proc:\n+        # Wait for the process to terminate, to avoid zombies.\n+        # Shouldn't really take the 30 seconds\n+        proc.wait(30)\n+        # poll the terminal so the right returncode is set on the popen object\n+        proc.poll()\n+        # This call shouldn't really be necessary\n+        proc.communicate()\n+\n+    terminal_stdout.flush()\n+    terminal_stdout.seek(0)\n+    if sys.version_info < (3, 6):  # pragma: no cover\n+        stdout = proc._translate_newlines(\n+            terminal_stdout.read(), __salt_system_encoding__\n+        )\n+    else:\n+        stdout = proc._translate_newlines(\n+            terminal_stdout.read(), __salt_system_encoding__, sys.stdout.errors\n+        )\n+    terminal_stdout.close()\n+\n+    terminal_stderr.flush()\n+    terminal_stderr.seek(0)\n+    if sys.version_info < (3, 6):  # pragma: no cover\n+        stderr = proc._translate_newlines(\n+            terminal_stderr.read(), __salt_system_encoding__\n+        )\n+    else:\n+        stderr = proc._translate_newlines(\n+            terminal_stderr.read(), __salt_system_encoding__, sys.stderr.errors\n+        )\n+    terminal_stderr.close()\n+    ret = ProcessResult(proc.returncode, stdout, stderr, cmdline=proc.args)\n+    log.debug(ret)\n+    assert \"Exiting gracefully on Ctrl-c\" in ret.stderr\n+    assert \"This job's jid is\" in ret.stderr"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "raceCondition": "true",
                "testSteps": "pipenv run python -m nox -e \"pytest-3(coverage=False)\"  -- tests/pytests/integration/cli/test_salt.py --run-slow",
                "testStepsFull": "pipenv run python -m nox -e \"pytest-3(coverage=False)\"  -- --run-slow\n"
            },
            {
                "id": 59705,
                "created_at": "2021-03-04T14:46:05Z",
                "closed_at": "2022-04-06T19:37:02Z",
                "title": "[BUG] (?) regression (!)(?) in 3002.5 for linux yum pkg version numbers - AttributeError: 'float' object has no attribute 'lstrip'",
                "labels": "Bug,severity-medium,Regression,P2,centos",
                "text_based": false,
                "commits": [
                    {
                        "hash": "a8ca0092f3e294b134f2afb479d736571a0fe6ad",
                        "commit_date": "2022-04-06T19:37:00Z",
                        "parents": "d1153c4009aefc2889931e9d6d2a560d098ef286",
                        "stat": {
                            "total": 16,
                            "additions": 28,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "763426c59cfaf3d5011a3a527dbcddd2e65243eb",
                                    "filename": "salt/modules/network.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 15,
                                    "changes": 27,
                                    "blob_url": "https://github.com/saltstack/salt/blob/a8ca0092f3e294b134f2afb479d736571a0fe6ad/salt%2Fmodules%2Fnetwork.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/a8ca0092f3e294b134f2afb479d736571a0fe6ad/salt%2Fmodules%2Fnetwork.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/salt%2Fmodules%2Fnetwork.py?ref=a8ca0092f3e294b134f2afb479d736571a0fe6ad",
                                    "patch": "@@ -9,7 +9,7 @@\n import re\n import socket\n import time\n-from multiprocessing.pool import ThreadPool\n+from concurrent.futures import ThreadPoolExecutor\n \n import salt.utils.decorators.path\n import salt.utils.functools\n@@ -2096,7 +2096,7 @@ def _lookup_fqdn(ip):\n                 log.debug(\"Unable to resolve address %s: %s\", ip, err)\n             else:\n                 log.error(\"Failed to resolve address %s: %s\", ip, err)\n-        except (OSError, socket.gaierror, socket.timeout) as err:\n+        except Exception as err:  # pylint: disable=broad-except\n             log.error(\"Failed to resolve address %s: %s\", ip, err)\n \n     start = time.time()\n@@ -2110,24 +2110,21 @@ def _lookup_fqdn(ip):\n         )\n     )\n \n-    # Create a ThreadPool to process the underlying calls to 'socket.gethostbyaddr' in parallel.\n-    # This avoid blocking the execution when the \"fqdn\" is not defined for certains IP addresses, which was causing\n-    # that \"socket.timeout\" was reached multiple times secuencially, blocking execution for several seconds.\n+    # Create a ThreadPool to process the underlying calls to\n+    # 'socket.gethostbyaddr' in parallel.  This avoid blocking the execution\n+    # when the \"fqdn\" is not defined for certains IP addresses, which was\n+    # causing that \"socket.timeout\" was reached multiple times sequentially,\n+    # blocking execution for several seconds.\n \n-    results = []\n     try:\n-        pool = ThreadPool(8)\n-        results = pool.map(_lookup_fqdn, addresses)\n-        pool.close()\n-        pool.join()\n+        with ThreadPoolExecutor(8) as pool:\n+            for item in pool.map(_lookup_fqdn, addresses):\n+                if item:\n+                    fqdns.update(item)\n     except Exception as exc:  # pylint: disable=broad-except\n         log.error(\"Exception while creating a ThreadPool for resolving FQDNs: %s\", exc)\n \n-    for item in results:\n-        if item:\n-            fqdns.update(item)\n-\n     elapsed = time.time() - start\n     log.debug(\"Elapsed time getting FQDNs: %s seconds\", elapsed)\n \n-    return {\"fqdns\": sorted(list(fqdns))}\n+    return {\"fqdns\": sorted(fqdns)}"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "15fd5545a0b500c431f1109d4c37bfe305665384",
                                    "filename": "tests/pytests/unit/modules/test_network.py",
                                    "status": "modified",
                                    "additions": 0,
                                    "deletions": 1,
                                    "changes": 1,
                                    "blob_url": "https://github.com/saltstack/salt/blob/a8ca0092f3e294b134f2afb479d736571a0fe6ad/tests%2Fpytests%2Funit%2Fmodules%2Ftest_network.py",
                                    "raw_url": "https://github.com/saltstack/salt/raw/a8ca0092f3e294b134f2afb479d736571a0fe6ad/tests%2Fpytests%2Funit%2Fmodules%2Ftest_network.py",
                                    "contents_url": "https://api.github.com/repos/saltstack/salt/contents/tests%2Fpytests%2Funit%2Fmodules%2Ftest_network.py?ref=a8ca0092f3e294b134f2afb479d736571a0fe6ad",
                                    "patch": "@@ -49,7 +49,6 @@ def fake_ips():\n         yield\n \n \n-@pytest.mark.xfail\n def test_when_errors_happen_looking_up_fqdns_threads_should_not_leak(socket_errors):\n     before_threads = threading.active_count()\n     networkmod.fqdns()"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m nox -e \"pytest-3(coverage=False)\"  -- tests/pytests/unit/modules/test_network.py",
                "testStepsFull": "pipenv run python -m nox -e \"pytest-3(coverage=False)\""
            }
        ],
        "installSteps": "pipenv --python 3.8\npipenv run python -m pip install -e .\npipenv run pip install nox\n"
    },
    {
        "_id": "64ac002ffc4a3b4d7444d9bc",
        "username": "numpy",
        "repository": "numpy",
        "issues": [
            {
                "id": 22714,
                "created_at": "2022-12-02T16:38:29Z",
                "closed_at": "2022-12-06T08:55:54Z",
                "title": "BUG: `numpy.median()` does not respect `keepdims=True` when the `out` argument is specified",
                "labels": "00 - Bug,component: numpy.lib",
                "commits": [
                    {
                        "hash": "91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                        "commit_date": "2022-12-05T22:53:32Z",
                        "parents": "f74b3720ca6148574da40ed96d4712e221ee84bd",
                        "stat": {
                            "total": 33,
                            "additions": 208,
                            "deletions": 175,
                            "files": [
                                {
                                    "sha": "35a3b3543c285df86cec75184d0bfafc613d7e73",
                                    "filename": "numpy/lib/function_base.py",
                                    "status": "modified",
                                    "additions": 33,
                                    "deletions": 18,
                                    "changes": 51,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ffunction_base.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ffunction_base.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Flib%2Ffunction_base.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -3689,7 +3689,7 @@ def msort(a):\n     return b\n \n \n-def _ureduce(a, func, **kwargs):\n+def _ureduce(a, func, keepdims=False, **kwargs):\n     \"\"\"\n     Internal Function.\n     Call `func` with `a` as first argument swapping the axes to use extended\n@@ -3717,13 +3717,20 @@ def _ureduce(a, func, **kwargs):\n     \"\"\"\n     a = np.asanyarray(a)\n     axis = kwargs.get('axis', None)\n+    out = kwargs.get('out', None)\n+\n+    if keepdims is np._NoValue:\n+        keepdims = False\n+\n+    nd = a.ndim\n     if axis is not None:\n-        keepdim = list(a.shape)\n-        nd = a.ndim\n         axis = _nx.normalize_axis_tuple(axis, nd)\n \n-        for ax in axis:\n-            keepdim[ax] = 1\n+        if keepdims:\n+            if out is not None:\n+                index_out = tuple(\n+                    0 if i in axis else slice(None) for i in range(nd))\n+                kwargs['out'] = out[(Ellipsis, ) + index_out]\n \n         if len(axis) == 1:\n             kwargs['axis'] = axis[0]\n@@ -3736,12 +3743,27 @@ def _ureduce(a, func, **kwargs):\n             # merge reduced axis\n             a = a.reshape(a.shape[:nkeep] + (-1,))\n             kwargs['axis'] = -1\n-        keepdim = tuple(keepdim)\n     else:\n-        keepdim = (1,) * a.ndim\n+        if keepdims:\n+            if out is not None:\n+                index_out = (0, ) * nd\n+                kwargs['out'] = out[(Ellipsis, ) + index_out]\n \n     r = func(a, **kwargs)\n-    return r, keepdim\n+\n+    if out is not None:\n+        return out\n+\n+    if keepdims:\n+        if axis is None:\n+            index_r = (np.newaxis, ) * nd\n+        else:\n+            index_r = tuple(\n+                np.newaxis if i in axis else slice(None)\n+                for i in range(nd))\n+        r = r[(Ellipsis, ) + index_r]\n+\n+    return r\n \n \n def _median_dispatcher(\n@@ -3831,12 +3853,8 @@ def median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n     >>> assert not np.all(a==b)\n \n     \"\"\"\n-    r, k = _ureduce(a, func=_median, axis=axis, out=out,\n+    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n                     overwrite_input=overwrite_input)\n-    if keepdims:\n-        return r.reshape(k)\n-    else:\n-        return r\n \n \n def _median(a, axis=None, out=None, overwrite_input=False):\n@@ -4452,17 +4470,14 @@ def _quantile_unchecked(a,\n                         method=\"linear\",\n                         keepdims=False):\n     \"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\n-    r, k = _ureduce(a,\n+    return _ureduce(a,\n                     func=_quantile_ureduce_func,\n                     q=q,\n+                    keepdims=keepdims,\n                     axis=axis,\n                     out=out,\n                     overwrite_input=overwrite_input,\n                     method=method)\n-    if keepdims:\n-        return r.reshape(q.shape + k)\n-    else:\n-        return r\n \n \n def _quantile_is_valid(q):"
                                },
                                {
                                    "sha": "ae2dfa1657ff8804509bed4340401e7ded3c7dec",
                                    "filename": "numpy/lib/nanfunctions.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 10,
                                    "changes": 14,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Fnanfunctions.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Fnanfunctions.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Flib%2Fnanfunctions.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -1214,12 +1214,9 @@ def nanmedian(a, axis=None, out=None, overwrite_input=False, keepdims=np._NoValu\n     if a.size == 0:\n         return np.nanmean(a, axis, out=out, keepdims=keepdims)\n \n-    r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n+    return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n+                                  axis=axis, out=out,\n                                   overwrite_input=overwrite_input)\n-    if keepdims and keepdims is not np._NoValue:\n-        return r.reshape(k)\n-    else:\n-        return r\n \n \n def _nanpercentile_dispatcher(\n@@ -1556,17 +1553,14 @@ def _nanquantile_unchecked(\n     # so deal them upfront\n     if a.size == 0:\n         return np.nanmean(a, axis, out=out, keepdims=keepdims)\n-    r, k = function_base._ureduce(a,\n+    return function_base._ureduce(a,\n                                   func=_nanquantile_ureduce_func,\n                                   q=q,\n+                                  keepdims=keepdims,\n                                   axis=axis,\n                                   out=out,\n                                   overwrite_input=overwrite_input,\n                                   method=method)\n-    if keepdims and keepdims is not np._NoValue:\n-        return r.reshape(q.shape + k)\n-    else:\n-        return r\n \n \n def _nanquantile_ureduce_func(a, q, axis=None, out=None, overwrite_input=False,"
                                },
                                {
                                    "sha": "41bce0f22328d91f2eae2437d73becc99fa9ca87",
                                    "filename": "numpy/ma/extras.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 5,
                                    "changes": 6,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Fma%2Fextras.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Fma%2Fextras.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Fma%2Fextras.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -732,12 +732,8 @@ def median(a, axis=None, out=None, overwrite_input=False, keepdims=False):\n         else:\n             return m\n \n-    r, k = _ureduce(a, func=_median, axis=axis, out=out,\n+    return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,\n                     overwrite_input=overwrite_input)\n-    if keepdims:\n-        return r.reshape(k)\n-    else:\n-        return r\n \n \n def _median(a, axis=None, out=None, overwrite_input=False):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "1bb4c4efaaf4416376a6d390fe2e44e7a09ffbd5",
                                    "filename": "numpy/lib/tests/test_function_base.py",
                                    "status": "modified",
                                    "additions": 50,
                                    "deletions": 0,
                                    "changes": 50,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ftests%2Ftest_function_base.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ftests%2Ftest_function_base.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Flib%2Ftests%2Ftest_function_base.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -25,6 +25,7 @@\n     i0, insert, interp, kaiser, meshgrid, msort, piecewise, place, rot90,\n     select, setxor1d, sinc, trapz, trim_zeros, unwrap, unique, vectorize\n     )\n+from numpy.core.numeric import normalize_axis_tuple\n \n \n def get_mat(n):\n@@ -3331,6 +3332,32 @@ def test_keepdims(self):\n         assert_equal(np.percentile(d, [1, 7], axis=(0, 3),\n                                    keepdims=True).shape, (2, 1, 5, 7, 1))\n \n+    @pytest.mark.parametrize('q', [7, [1, 7]])\n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1,),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, q, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        shape_out = np.shape(q) + shape_out\n+\n+        out = np.empty(shape_out)\n+        result = np.percentile(d, q, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_out(self):\n         o = np.zeros((4,))\n         d = np.ones((3, 4))\n@@ -3843,6 +3870,29 @@ def test_keepdims(self):\n         assert_equal(np.median(d, axis=(0, 1, 3), keepdims=True).shape,\n                      (1, 1, 7, 1))\n \n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1, ),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        out = np.empty(shape_out)\n+        result = np.median(d, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n \n class TestAdd_newdoc_ufunc:\n "
                                },
                                {
                                    "sha": "64464edccfc09b1a64903b1479af238b8fa58f97",
                                    "filename": "numpy/lib/tests/test_nanfunctions.py",
                                    "status": "modified",
                                    "additions": 58,
                                    "deletions": 0,
                                    "changes": 58,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ftests%2Ftest_nanfunctions.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Flib%2Ftests%2Ftest_nanfunctions.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Flib%2Ftests%2Ftest_nanfunctions.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -3,6 +3,7 @@\n import inspect\n \n import numpy as np\n+from numpy.core.numeric import normalize_axis_tuple\n from numpy.lib.nanfunctions import _nan_mask, _replace_nan\n from numpy.testing import (\n     assert_, assert_equal, assert_almost_equal, assert_raises,\n@@ -807,6 +808,33 @@ def test_keepdims(self):\n             res = np.nanmedian(d, axis=(0, 1, 3), keepdims=True)\n             assert_equal(res.shape, (1, 1, 7, 1))\n \n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1, ),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        # Randomly set some elements to NaN:\n+        w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n+        w = w.astype(np.intp)\n+        d[tuple(w)] = np.nan\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        out = np.empty(shape_out)\n+        result = np.nanmedian(d, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_out(self):\n         mat = np.random.rand(3, 3)\n         nan_mat = np.insert(mat, [0, 2], np.nan, axis=1)\n@@ -982,6 +1010,36 @@ def test_keepdims(self):\n             res = np.nanpercentile(d, 90, axis=(0, 1, 3), keepdims=True)\n             assert_equal(res.shape, (1, 1, 7, 1))\n \n+    @pytest.mark.parametrize('q', [7, [1, 7]])\n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1,),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, q, axis):\n+        d = np.ones((3, 5, 7, 11))\n+        # Randomly set some elements to NaN:\n+        w = np.random.random((4, 200)) * np.array(d.shape)[:, None]\n+        w = w.astype(np.intp)\n+        d[tuple(w)] = np.nan\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        shape_out = np.shape(q) + shape_out\n+\n+        out = np.empty(shape_out)\n+        result = np.nanpercentile(d, q, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_out(self):\n         mat = np.random.rand(3, 3)\n         nan_mat = np.insert(mat, [0, 2], np.nan, axis=1)"
                                },
                                {
                                    "sha": "38603fb8433a5a1ab0b0cfce428dbd46533fb75b",
                                    "filename": "numpy/ma/tests/test_extras.py",
                                    "status": "modified",
                                    "additions": 29,
                                    "deletions": 0,
                                    "changes": 29,
                                    "blob_url": "https://github.com/numpy/numpy/blob/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Fma%2Ftests%2Ftest_extras.py",
                                    "raw_url": "https://github.com/numpy/numpy/raw/91432a36a3611c2374ea9e2d45592f0ac5e71adb/numpy%2Fma%2Ftests%2Ftest_extras.py",
                                    "contents_url": "https://api.github.com/repos/numpy/numpy/contents/numpy%2Fma%2Ftests%2Ftest_extras.py?ref=91432a36a3611c2374ea9e2d45592f0ac5e71adb",
                                    "patch": "@@ -12,6 +12,7 @@\n import pytest\n \n import numpy as np\n+from numpy.core.numeric import normalize_axis_tuple\n from numpy.testing import (\n     assert_warns, suppress_warnings\n     )\n@@ -989,6 +990,34 @@ def test_out(self):\n             assert_(r is out)\n             assert_(type(r) is MaskedArray)\n \n+    @pytest.mark.parametrize(\n+        argnames='axis',\n+        argvalues=[\n+            None,\n+            1,\n+            (1, ),\n+            (0, 1),\n+            (-3, -1),\n+        ]\n+    )\n+    def test_keepdims_out(self, axis):\n+        mask = np.zeros((3, 5, 7, 11), dtype=bool)\n+        # Randomly set some elements to True:\n+        w = np.random.random((4, 200)) * np.array(mask.shape)[:, None]\n+        w = w.astype(np.intp)\n+        mask[tuple(w)] = np.nan\n+        d = masked_array(np.ones(mask.shape), mask=mask)\n+        if axis is None:\n+            shape_out = (1,) * d.ndim\n+        else:\n+            axis_norm = normalize_axis_tuple(axis, d.ndim)\n+            shape_out = tuple(\n+                1 if i in axis_norm else d.shape[i] for i in range(d.ndim))\n+        out = masked_array(np.empty(shape_out))\n+        result = median(d, axis=axis, keepdims=True, out=out)\n+        assert result is out\n+        assert_equal(result.shape, shape_out)\n+\n     def test_single_non_masked_value_on_axis(self):\n         data = [[1., 0.],\n                 [0., 3.],"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python runtests.py -t numpy.ma.tests.test_extras numpy.lib.tests.test_function_base numpy.lib.tests.test_nanfunctions numpy.ma.tests.test_extras",
                "testStepsFull": "python runtests.py"
            }
        ],
        "installSteps": "git submodule update --init\npipenv --python 3.9\npipenv run python -m pip install -r test_requirements.txt\npipenv run python setup.py build_ext --inplace -j 4\n"
    },
    {
        "_id": "64b4f2999d05145a3db52b14",
        "username": "Rapptz",
        "repository": "discord.py",
        "issues": [
            {
                "id": 7818,
                "created_at": "2022-04-01T11:44:03Z",
                "closed_at": "2022-04-01T13:14:02Z",
                "title": "Unable to create nested groups with explicit parent set",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "25ad5b675c044e67c160e8a18a4753e93efec26d",
                        "commit_date": "2022-04-01T13:13:27Z",
                        "parents": "c67130821151266e6b6855b8ce0e031850bf6fbc",
                        "stat": {
                            "total": 4,
                            "additions": 50,
                            "deletions": 46,
                            "files": [
                                {
                                    "sha": "11af21f8fc3623e5c6e3d318fb9144a0ee34f0b5",
                                    "filename": "discord/app_commands/commands.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 2,
                                    "changes": 15,
                                    "blob_url": "https://github.com/Rapptz/discord.py/blob/25ad5b675c044e67c160e8a18a4753e93efec26d/discord%2Fapp_commands%2Fcommands.py",
                                    "raw_url": "https://github.com/Rapptz/discord.py/raw/25ad5b675c044e67c160e8a18a4753e93efec26d/discord%2Fapp_commands%2Fcommands.py",
                                    "contents_url": "https://api.github.com/repos/Rapptz/discord.py/contents/discord%2Fapp_commands%2Fcommands.py?ref=25ad5b675c044e67c160e8a18a4753e93efec26d",
                                    "patch": "@@ -971,6 +971,7 @@ def __init__(\n         self.name: str = validate_name(name) if name is not MISSING else cls.__discord_app_commands_group_name__\n         self.description: str = description or cls.__discord_app_commands_group_description__\n         self._attr: Optional[str] = None\n+        self._owner_cls: Optional[Type[Any]] = None\n         self._guild_ids: Optional[List[int]] = guild_ids\n \n         if not self.description:\n@@ -1004,12 +1005,15 @@ def __init__(\n             if copy._attr and not cls.__discord_app_commands_skip_init_binding__:\n                 setattr(self, copy._attr, copy)\n \n-        if parent is not None and parent.parent is not None:\n-            raise ValueError('groups can only be nested at most one level')\n+        if parent is not None:\n+            if parent.parent is not None:\n+                raise ValueError('groups can only be nested at most one level')\n+            parent.add_command(self)\n \n     def __set_name__(self, owner: Type[Any], name: str) -> None:\n         self._attr = name\n         self.module = owner.__module__\n+        self._owner_cls = owner\n \n     def _copy_with(\n         self,\n@@ -1029,6 +1033,7 @@ def _copy_with(\n         copy.parent = parent\n         copy.module = self.module\n         copy._attr = self._attr\n+        copy._owner_cls = self._owner_cls\n         copy._children = {}\n \n         bindings[self] = copy\n@@ -1038,6 +1043,12 @@ def _copy_with(\n             child_copy.parent = copy\n             copy._children[child_copy.name] = child_copy\n \n+            if isinstance(child_copy, Group) and child_copy._attr and set_on_binding:\n+                if binding.__class__ is child_copy._owner_cls:\n+                    setattr(binding, child_copy._attr, child_copy)\n+                elif child_copy._owner_cls is copy.__class__:\n+                    setattr(copy, child_copy._attr, child_copy)\n+\n         if copy._attr and set_on_binding:\n             setattr(parent or binding, copy._attr, copy)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "827c38d5d5e8a7c1d80e6ab175f0f29b21b2f99d",
                                    "filename": "tests/test_app_commands_group.py",
                                    "status": "modified",
                                    "additions": 33,
                                    "deletions": 2,
                                    "changes": 35,
                                    "blob_url": "https://github.com/Rapptz/discord.py/blob/25ad5b675c044e67c160e8a18a4753e93efec26d/tests%2Ftest_app_commands_group.py",
                                    "raw_url": "https://github.com/Rapptz/discord.py/raw/25ad5b675c044e67c160e8a18a4753e93efec26d/tests%2Ftest_app_commands_group.py",
                                    "contents_url": "https://api.github.com/repos/Rapptz/discord.py/contents/tests%2Ftest_app_commands_group.py?ref=25ad5b675c044e67c160e8a18a4753e93efec26d",
                                    "patch": "@@ -95,8 +95,7 @@ async def my_group_command(self, interaction: discord.Interaction) -> None:\n     assert my_group.my_group_command.parent is my_group\n     assert my_group.my_group_command.binding is my_group\n     assert my_group.sub_group.my_sub_group_command.parent is my_group.sub_group\n-    print(my_group.sub_group.my_sub_group_command.binding)\n-    print(MyGroup.sub_group)\n+    assert not hasattr(my_group, 'my_sub_group_command')\n     assert my_group.sub_group.my_sub_group_command.binding is my_group.sub_group\n \n \n@@ -127,6 +126,32 @@ async def my_command(self, interaction: discord.Interaction) -> None:\n     assert cog.my_command.binding is cog\n \n \n+def test_cog_with_nested_group_with_commands():\n+    class MyCog(commands.Cog):\n+        first = app_commands.Group(name='test', description='Test 1')\n+        second = app_commands.Group(name='test2', parent=first, description='Test 2')\n+\n+        @first.command(name='cmd')\n+        async def test_cmd(self, interaction: discord.Interaction) -> None:\n+            ...\n+\n+        @second.command(name='cmd2')\n+        async def test2_cmd(self, interaction: discord.Interaction) -> None:\n+            ...\n+\n+    cog = MyCog()\n+\n+    assert len(MyCog.__cog_app_commands__) == 1\n+    assert cog.first.parent is None\n+    assert cog.first is not MyCog.first\n+    assert cog.second is not MyCog.second\n+    assert cog.second.parent is cog.first\n+    assert cog.test_cmd.parent is cog.first\n+    assert cog.test2_cmd.parent is cog.second\n+    assert cog.test_cmd.binding is cog\n+    assert cog.test2_cmd.binding is cog\n+\n+\n def test_cog_with_group_subclass_with_commands():\n     class MyGroup(app_commands.Group, name='mygroup'):\n         @app_commands.command()\n@@ -175,6 +200,8 @@ async def my_cog_command(self, interaction: discord.Interaction) -> None:\n     assert cog.my_group.my_command is not MyGroup.my_command\n     assert cog.my_cog_command is not MyCog.my_cog_command\n     assert not hasattr(cog.my_group, 'my_cog_command')\n+    assert not hasattr(cog, 'sub_group')\n+    assert not hasattr(cog, 'my_command')\n     assert cog.my_group.parent is None\n     assert cog.my_group.sub_group.parent is cog.my_group\n     assert cog.my_group.my_command.parent is cog.my_group.sub_group\n@@ -215,6 +242,10 @@ async def my_sub_group_cog_command(self, interaction: discord.Interaction) -> No\n     assert cog.my_group.sub_group is not MyGroup.sub_group\n     assert cog.my_cog_command is not MyCog.my_cog_command\n     assert not hasattr(cog.my_group, 'my_cog_command')\n+    assert not hasattr(cog, 'sub_group')\n+    assert not hasattr(cog, 'my_group_command')\n+    assert not hasattr(cog, 'my_sub_group_command')\n+    assert not hasattr(cog.my_group, 'my_sub_group_command')\n     assert cog.my_group.sub_group.my_sub_group_command is not MyGroup.sub_group.my_sub_group_command\n     assert cog.my_group.sub_group.my_sub_group_command is not MySubGroup.my_sub_group_command\n     assert cog.my_group.sub_group.parent is cog.my_group"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_app_commands_group.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7676,
                "created_at": "2022-03-13T18:22:23Z",
                "closed_at": "2022-03-14T10:09:27Z",
                "title": "tasks.loop display too many warning.",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "abcec5da9d501a1b54b73ac0fa3870df773fa63c",
                        "commit_date": "2022-03-14T08:54:56Z",
                        "parents": "a4676804ec15d3a9c67a76563918f866cf1b49b9",
                        "stat": {
                            "total": 7,
                            "additions": 46,
                            "deletions": 39,
                            "files": [
                                {
                                    "sha": "93f6969b8c890537b7c0efce722e36c4ada60b35",
                                    "filename": "discord/ext/tasks/__init__.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 6,
                                    "changes": 19,
                                    "blob_url": "https://github.com/Rapptz/discord.py/blob/abcec5da9d501a1b54b73ac0fa3870df773fa63c/discord%2Fext%2Ftasks%2F__init__.py",
                                    "raw_url": "https://github.com/Rapptz/discord.py/raw/abcec5da9d501a1b54b73ac0fa3870df773fa63c/discord%2Fext%2Ftasks%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/Rapptz/discord.py/contents/discord%2Fext%2Ftasks%2F__init__.py?ref=abcec5da9d501a1b54b73ac0fa3870df773fa63c",
                                    "patch": "@@ -632,13 +632,14 @@ def _get_next_sleep_time(self, now: datetime.datetime = MISSING) -> datetime.dat\n \n         if index is None:\n             time = self._time[0]\n-            tomorrow = now + datetime.timedelta(days=1)\n+            tomorrow = now.astimezone(time.tzinfo) + datetime.timedelta(days=1)\n             date = tomorrow.date()\n         else:\n-            date = now.date()\n             time = self._time[index]\n+            date = now.astimezone(time.tzinfo).date()\n \n-        return resolve_datetime(datetime.datetime.combine(date, time, tzinfo=time.tzinfo or datetime.timezone.utc))\n+        dt = datetime.datetime.combine(date, time, tzinfo=time.tzinfo)\n+        return resolve_datetime(dt)\n \n     def _start_time_relative_to(self, now: datetime.datetime) -> Optional[int]:\n         # now kwarg should be a datetime.datetime representing the time \"now\"\n@@ -651,10 +652,16 @@ def _start_time_relative_to(self, now: datetime.datetime) -> Optional[int]:\n         # For example, if given a list of times [0, 3, 18]\n         # If it's 04:00 today then we know we have to wait until 18:00 today\n         # If it's 19:00 today then we know we we have to wait until 00:00 tomorrow\n-        date = now.date()\n+        # Note that timezones need to be taken into consideration for this to work.\n+        # If the timezone is set to UTC+9 and the now timezone is UTC\n+        # A conversion needs to be done.\n+        # i.e. 03:00 UTC+9 -> 18:00 UTC the previous day\n         for idx, time in enumerate(self._time):\n-            start_time = datetime.datetime.combine(date, time, tzinfo=time.tzinfo)\n-            if start_time >= now:\n+            # Convert the current time to the target timezone\n+            # e.g. 18:00 UTC -> 03:00 UTC+9\n+            # Then compare the time instances to see if they're the same\n+            start = now.astimezone(time.tzinfo)\n+            if time >= start.timetz():\n                 return idx\n         else:\n             return None"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "04e0c7ea6e857029f22056dbee7f159e8d0c6d58",
                                    "filename": "tests/test_ext_tasks.py",
                                    "status": "modified",
                                    "additions": 26,
                                    "deletions": 1,
                                    "changes": 27,
                                    "blob_url": "https://github.com/Rapptz/discord.py/blob/abcec5da9d501a1b54b73ac0fa3870df773fa63c/tests%2Ftest_ext_tasks.py",
                                    "raw_url": "https://github.com/Rapptz/discord.py/raw/abcec5da9d501a1b54b73ac0fa3870df773fa63c/tests%2Ftest_ext_tasks.py",
                                    "contents_url": "https://api.github.com/repos/Rapptz/discord.py/contents/tests%2Ftest_ext_tasks.py?ref=abcec5da9d501a1b54b73ac0fa3870df773fa63c",
                                    "patch": "@@ -102,7 +102,32 @@ async def loop():\n \n     for before, expected_time in zip(minute_before, times):\n         expected = datetime.datetime.combine(today, expected_time, tzinfo=jst)\n-        assert loop._get_next_sleep_time(before) == expected\n+        actual = loop._get_next_sleep_time(before)\n+        assert actual == expected\n+\n+\n+def test_task_regression_issue7676():\n+    jst = datetime.timezone(datetime.timedelta(hours=9))\n+\n+    # 00:00, 03:00, 06:00, 09:00, 12:00, 15:00, 18:00, 21:00\n+    times = [datetime.time(hour=h, tzinfo=jst) for h in range(0, 24, 3)]\n+\n+    @tasks.loop(time=times)\n+    async def loop():\n+        pass\n+\n+    # Create pseudo UTC times\n+    now = utils.utcnow()\n+    today = now.date()\n+    times_before_in_utc = [\n+        datetime.datetime.combine(today, time, tzinfo=jst).astimezone(datetime.timezone.utc) - datetime.timedelta(minutes=1)\n+        for time in times\n+    ]\n+\n+    for before, expected_time in zip(times_before_in_utc, times):\n+        actual = loop._get_next_sleep_time(before)\n+        actual_time = actual.timetz()\n+        assert actual_time == expected_time\n \n \n @pytest.mark.skipif(sys.version_info < (3, 9), reason=\"zoneinfo requires 3.9\")"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/test_ext_tasks.py",
                "testStepsFull": "pipenv run pytest"
            }
        ],
        "installSteps": "pipenv --python 3.8\npipenv run python -m pip install -e .[test]"
    },
    {
        "_id": "64b7e33ceb69ce1fbc9d944c",
        "username": "python-poetry",
        "repository": "poetry",
        "issues": [
            {
                "id": 5141,
                "created_at": "2022-02-02T19:29:48Z",
                "closed_at": "2022-04-03T17:46:14Z",
                "title": "Incorrect `poetry export` output with dependency conditional on Python version",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "fb13b3a676f476177f7937ffa480ee5cff9a90a5",
                        "commit_date": "2022-04-03T17:46:12Z",
                        "parents": "eb27f816f643c905eb8e8086691454d4490e3dfe",
                        "stat": {
                            "total": 193,
                            "additions": 779,
                            "deletions": 586,
                            "files": [
                                {
                                    "sha": "46eae599982083d02aa5d5d7fc43df87dddeb139",
                                    "filename": "src/poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 78,
                                    "deletions": 104,
                                    "changes": 182,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/fb13b3a676f476177f7937ffa480ee5cff9a90a5/src%2Fpoetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/fb13b3a676f476177f7937ffa480ee5cff9a90a5/src%2Fpoetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/src%2Fpoetry%2Fpackages%2Flocker.py?ref=fb13b3a676f476177f7937ffa480ee5cff9a90a5",
                                    "patch": "@@ -32,6 +32,8 @@\n \n \n if TYPE_CHECKING:\n+    from poetry.core.semver.version_constraint import VersionConstraint\n+    from poetry.core.version.markers import BaseMarker\n     from tomlkit.items import InlineTable\n     from tomlkit.toml_document import TOMLDocument\n \n@@ -203,152 +205,130 @@ def locked_repository(self, with_dev_reqs: bool = False) -> Repository:\n \n     @staticmethod\n     def __get_locked_package(\n-        _dependency: Dependency, packages_by_name: dict[str, list[Package]]\n+        dependency: Dependency,\n+        packages_by_name: dict[str, list[Package]],\n+        decided: dict[Package, Dependency] | None = None,\n     ) -> Package | None:\n         \"\"\"\n         Internal helper to identify corresponding locked package using dependency\n         version constraints.\n         \"\"\"\n-        for _package in packages_by_name.get(_dependency.name, []):\n-            if _dependency.constraint.allows(_package.version):\n-                return _package\n-        return None\n+        decided = decided or {}\n+\n+        # Get the packages that are consistent with this dependency.\n+        packages = [\n+            package\n+            for package in packages_by_name.get(dependency.name, [])\n+            if package.python_constraint.allows_all(dependency.python_constraint)\n+            and dependency.constraint.allows(package.version)\n+        ]\n+\n+        # If we've previously made a choice that is compatible with the current\n+        # requirement, stick with it.\n+        for package in packages:\n+            old_decision = decided.get(package)\n+            if (\n+                old_decision is not None\n+                and not old_decision.marker.intersect(dependency.marker).is_empty()\n+            ):\n+                return package\n+\n+        return next(iter(packages), None)\n \n     @classmethod\n-    def __walk_dependency_level(\n+    def __walk_dependencies(\n         cls,\n         dependencies: list[Dependency],\n-        level: int,\n-        pinned_versions: bool,\n         packages_by_name: dict[str, list[Package]],\n-        project_level_dependencies: set[str],\n-        nested_dependencies: dict[tuple[str, str], Dependency],\n-    ) -> dict[tuple[str, str], Dependency]:\n-        if not dependencies:\n-            return nested_dependencies\n-\n-        next_level_dependencies = []\n+    ) -> dict[Package, Dependency]:\n+        nested_dependencies: dict[Package, Dependency] = {}\n \n-        for requirement in dependencies:\n-            key = (requirement.name, requirement.pretty_constraint)\n-            locked_package = cls.__get_locked_package(requirement, packages_by_name)\n-\n-            if locked_package:\n-                # create dependency from locked package to retain dependency metadata\n-                # if this is not done, we can end-up with incorrect nested dependencies\n-                constraint = requirement.constraint\n-                pretty_constraint = requirement.pretty_constraint\n-                marker = requirement.marker\n-                requirement = locked_package.to_dependency()\n-                requirement.marker = requirement.marker.intersect(marker)\n-\n-                key = (requirement.name, pretty_constraint)\n+        visited: set[tuple[Dependency, BaseMarker]] = set()\n+        while dependencies:\n+            requirement = dependencies.pop(0)\n+            if (requirement, requirement.marker) in visited:\n+                continue\n+            visited.add((requirement, requirement.marker))\n \n-                if not pinned_versions:\n-                    requirement.set_constraint(constraint)\n+            locked_package = cls.__get_locked_package(\n+                requirement, packages_by_name, nested_dependencies\n+            )\n \n-                for require in locked_package.requires:\n-                    if require.marker.is_empty():\n-                        require.marker = requirement.marker\n-                    else:\n-                        require.marker = require.marker.intersect(requirement.marker)\n+            if not locked_package:\n+                raise RuntimeError(f\"Dependency walk failed at {requirement}\")\n \n-                    require.marker = require.marker.intersect(locked_package.marker)\n+            # create dependency from locked package to retain dependency metadata\n+            # if this is not done, we can end-up with incorrect nested dependencies\n+            constraint = requirement.constraint\n+            marker = requirement.marker\n+            extras = requirement.extras\n+            requirement = locked_package.to_dependency()\n+            requirement.marker = requirement.marker.intersect(marker)\n \n-                    if key not in nested_dependencies:\n-                        next_level_dependencies.append(require)\n+            requirement.set_constraint(constraint)\n \n-            if requirement.name in project_level_dependencies and level == 0:\n-                # project level dependencies take precedence\n-                continue\n+            for require in locked_package.requires:\n+                if require.in_extras and extras.isdisjoint(require.in_extras):\n+                    continue\n \n-            if not locked_package:\n-                # we make a copy to avoid any side-effects\n-                requirement = deepcopy(requirement)\n+                require = deepcopy(require)\n+                require.marker = require.marker.intersect(\n+                    requirement.marker.without_extras()\n+                )\n+                if not require.marker.is_empty():\n+                    dependencies.append(require)\n \n+            key = locked_package\n             if key not in nested_dependencies:\n                 nested_dependencies[key] = requirement\n             else:\n                 nested_dependencies[key].marker = nested_dependencies[key].marker.union(\n                     requirement.marker\n                 )\n \n-        return cls.__walk_dependency_level(\n-            dependencies=next_level_dependencies,\n-            level=level + 1,\n-            pinned_versions=pinned_versions,\n-            packages_by_name=packages_by_name,\n-            project_level_dependencies=project_level_dependencies,\n-            nested_dependencies=nested_dependencies,\n-        )\n+        return nested_dependencies\n \n     @classmethod\n     def get_project_dependencies(\n         cls,\n         project_requires: list[Dependency],\n         locked_packages: list[Package],\n-        pinned_versions: bool = False,\n-        with_nested: bool = False,\n-    ) -> Iterable[Dependency]:\n+    ) -> Iterable[tuple[Package, Dependency]]:\n         # group packages entries by name, this is required because requirement might use\n-        # different constraints\n+        # different constraints.\n         packages_by_name: dict[str, list[Package]] = {}\n         for pkg in locked_packages:\n             if pkg.name not in packages_by_name:\n                 packages_by_name[pkg.name] = []\n             packages_by_name[pkg.name].append(pkg)\n \n-        project_level_dependencies = set()\n-        dependencies = []\n-\n-        for dependency in project_requires:\n-            dependency = deepcopy(dependency)\n-            locked_package = cls.__get_locked_package(dependency, packages_by_name)\n-            if locked_package:\n-                locked_dependency = locked_package.to_dependency()\n-                locked_dependency.marker = dependency.marker.intersect(\n-                    locked_package.marker\n-                )\n-\n-                if not pinned_versions:\n-                    locked_dependency.set_constraint(dependency.constraint)\n-\n-                dependency = locked_dependency\n-\n-            project_level_dependencies.add(dependency.name)\n-            dependencies.append(dependency)\n-\n-        if not with_nested:\n-            # return only with project level dependencies\n-            return dependencies\n+        # Put higher versions first so that we prefer them.\n+        for packages in packages_by_name.values():\n+            packages.sort(key=lambda package: package.version, reverse=True)\n \n-        nested_dependencies = cls.__walk_dependency_level(\n-            dependencies=dependencies,\n-            level=0,\n-            pinned_versions=pinned_versions,\n+        nested_dependencies = cls.__walk_dependencies(\n+            dependencies=project_requires,\n             packages_by_name=packages_by_name,\n-            project_level_dependencies=project_level_dependencies,\n-            nested_dependencies={},\n         )\n \n-        # Merge same dependencies using marker union\n-        for requirement in dependencies:\n-            key = (requirement.name, requirement.pretty_constraint)\n-            if key not in nested_dependencies:\n-                nested_dependencies[key] = requirement\n-            else:\n-                nested_dependencies[key].marker = nested_dependencies[key].marker.union(\n-                    requirement.marker\n-                )\n-\n-        return sorted(nested_dependencies.values(), key=lambda x: x.name.lower())\n+        return nested_dependencies.items()\n \n     def get_project_dependency_packages(\n         self,\n         project_requires: list[Dependency],\n+        project_python_marker: VersionConstraint | None = None,\n         dev: bool = False,\n         extras: bool | Sequence[str] | None = None,\n     ) -> Iterator[DependencyPackage]:\n+        # Apply the project python marker to all requirements.\n+        if project_python_marker is not None:\n+            marked_requires: list[Dependency] = []\n+            for require in project_requires:\n+                require = deepcopy(require)\n+                require.marker = require.marker.intersect(project_python_marker)\n+                marked_requires.append(require)\n+            project_requires = marked_requires\n+\n         repository = self.locked_repository(with_dev_reqs=dev)\n \n         # Build a set of all packages required by our selected extras\n@@ -379,16 +359,10 @@ def get_project_dependency_packages(\n \n             selected.append(dependency)\n \n-        for dependency in self.get_project_dependencies(\n+        for package, dependency in self.get_project_dependencies(\n             project_requires=selected,\n             locked_packages=repository.packages,\n-            with_nested=True,\n         ):\n-            try:\n-                package = repository.find_packages(dependency=dependency)[0]\n-            except IndexError:\n-                continue\n-\n             for extra in dependency.extras:\n                 package.requires_extras.append(extra)\n "
                                },
                                {
                                    "sha": "c219a8b85222e626ba52841801eb05b4e3543210",
                                    "filename": "src/poetry/utils/exporter.py",
                                    "status": "modified",
                                    "additions": 14,
                                    "deletions": 14,
                                    "changes": 28,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/fb13b3a676f476177f7937ffa480ee5cff9a90a5/src%2Fpoetry%2Futils%2Fexporter.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/fb13b3a676f476177f7937ffa480ee5cff9a90a5/src%2Fpoetry%2Futils%2Fexporter.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/src%2Fpoetry%2Futils%2Fexporter.py?ref=fb13b3a676f476177f7937ffa480ee5cff9a90a5",
                                    "patch": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import itertools\n import urllib.parse\n \n from typing import TYPE_CHECKING\n@@ -70,21 +69,22 @@ def _export_requirements_txt(\n         content = \"\"\n         dependency_lines = set()\n \n-        for package, groups in itertools.groupby(\n-            self._poetry.locker.get_project_dependency_packages(\n-                project_requires=self._poetry.package.all_requires,\n-                dev=dev,\n-                extras=extras,\n-            ),\n-            lambda dependency_package: dependency_package.package,\n+        # Get project dependencies.\n+        root_package = (\n+            self._poetry.package.clone()\n+            if dev\n+            else self._poetry.package.with_dependency_groups([\"default\"], only=True)\n+        )\n+\n+        for dependency_package in self._poetry.locker.get_project_dependency_packages(\n+            project_requires=root_package.all_requires,\n+            project_python_marker=root_package.python_marker,\n+            dev=dev,\n+            extras=extras,\n         ):\n             line = \"\"\n-            dependency_packages = list(groups)\n-            dependency = dependency_packages[0].dependency\n-            marker = dependency.marker\n-            for dep_package in dependency_packages[1:]:\n-                marker = marker.union(dep_package.dependency.marker)\n-            dependency.marker = marker\n+            dependency = dependency_package.dependency\n+            package = dependency_package.package\n \n             if package.develop:\n                 line += \"-e \""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "6e704b5338a49329e1ea321e67fe8e631b8a1bfe",
                                    "filename": "tests/console/commands/test_export.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 5,
                                    "changes": 20,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/fb13b3a676f476177f7937ffa480ee5cff9a90a5/tests%2Fconsole%2Fcommands%2Ftest_export.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/fb13b3a676f476177f7937ffa480ee5cff9a90a5/tests%2Fconsole%2Fcommands%2Ftest_export.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fconsole%2Fcommands%2Ftest_export.py?ref=fb13b3a676f476177f7937ffa480ee5cff9a90a5",
                                    "patch": "@@ -84,7 +84,9 @@ def _export_requirements(tester: CommandTester, poetry: Poetry) -> None:\n     assert poetry.locker.lock.exists()\n \n     expected = \"\"\"\\\n-foo==1.0.0\n+foo==1.0.0 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.4\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -113,7 +115,9 @@ def test_export_fails_on_invalid_format(tester: CommandTester, do_lock: None):\n def test_export_prints_to_stdout_by_default(tester: CommandTester, do_lock: None):\n     tester.execute(\"--format requirements.txt\")\n     expected = \"\"\"\\\n-foo==1.0.0\n+foo==1.0.0 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.4\" and python_version < \"4.0\"\n \"\"\"\n     assert tester.io.fetch_output() == expected\n \n@@ -123,16 +127,22 @@ def test_export_uses_requirements_txt_format_by_default(\n ):\n     tester.execute()\n     expected = \"\"\"\\\n-foo==1.0.0\n+foo==1.0.0 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.4\" and python_version < \"4.0\"\n \"\"\"\n     assert tester.io.fetch_output() == expected\n \n \n def test_export_includes_extras_by_flag(tester: CommandTester, do_lock: None):\n     tester.execute(\"--format requirements.txt --extras feature_bar\")\n     expected = \"\"\"\\\n-bar==1.1.0\n-foo==1.0.0\n+bar==1.1.0 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.4\" and python_version < \"4.0\"\n+foo==1.0.0 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.4\" and python_version < \"4.0\"\n \"\"\"\n     assert tester.io.fetch_output() == expected\n "
                                },
                                {
                                    "sha": "1c1cd3f97d2ed7a3a5d1f92d752c9181e94ff016",
                                    "filename": "tests/utils/test_exporter.py",
                                    "status": "modified",
                                    "additions": 479,
                                    "deletions": 70,
                                    "changes": 549,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/fb13b3a676f476177f7937ffa480ee5cff9a90a5/tests%2Futils%2Ftest_exporter.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/fb13b3a676f476177f7937ffa480ee5cff9a90a5/tests%2Futils%2Ftest_exporter.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_exporter.py?ref=fb13b3a676f476177f7937ffa480ee5cff9a90a5",
                                    "patch": "@@ -1,7 +1,6 @@\n from __future__ import annotations\n \n import sys\n-import textwrap\n \n from pathlib import Path\n from typing import TYPE_CHECKING\n@@ -126,8 +125,12 @@ def test_exporter_can_export_requirements_txt_with_standard_packages(\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6\n-foo==1.2.3\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -181,9 +184,15 @@ def test_exporter_can_export_requirements_txt_with_standard_packages_and_markers\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6\n-baz==7.8.9 ; sys_platform == \"win32\"\n-foo==1.2.3 ; python_version < \"3.7\"\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+baz==7.8.9 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" and sys_platform == \"win32\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" and sys_platform == \"win32\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"3.7\"\n \"\"\"\n \n     assert content == expected\n@@ -286,17 +295,29 @@ def test_exporter_can_export_requirements_txt_poetry(tmp_dir: str, poetry: Poetr\n     # \u2502       \u251c\u2500\u2500 cryptography >=2.0\n     # \u2502       \u2502   \u2514\u2500\u2500 six >=1.4.1\n     # \u2502       \u2514\u2500\u2500 jeepney >=0.6 (circular dependency aborted here)\n+    python27 = 'python_version >= \"2.7\" and python_version < \"2.8\"'\n+    python36 = 'python_version >= \"3.6\" and python_version < \"4.0\"'\n+    linux = 'sys_platform==\"linux\"'\n     expected = {\n-        \"poetry\": Dependency.create_from_pep_508(\"poetry==1.1.4\"),\n-        \"junit-xml\": Dependency.create_from_pep_508(\"junit-xml==1.9\"),\n-        \"keyring\": Dependency.create_from_pep_508(\"keyring==21.8.0\"),\n+        \"poetry\": Dependency.create_from_pep_508(\n+            f\"poetry==1.1.4; {python27} or {python36}\"\n+        ),\n+        \"junit-xml\": Dependency.create_from_pep_508(\n+            f\"junit-xml==1.9 ; {python27} or {python36}\"\n+        ),\n+        \"keyring\": Dependency.create_from_pep_508(\n+            f\"keyring==21.8.0 ; {python27} or {python36}\"\n+        ),\n         \"secretstorage\": Dependency.create_from_pep_508(\n-            \"secretstorage==3.3.0 ; sys_platform=='linux'\"\n+            f\"secretstorage==3.3.0 ; {python27} and {linux} or {python36} and {linux}\"\n         ),\n         \"cryptography\": Dependency.create_from_pep_508(\n-            \"cryptography==3.2 ; sys_platform=='linux'\"\n+            f\"cryptography==3.2 ; {python27} and {linux} or {python36} and {linux}\"\n+        ),\n+        \"six\": Dependency.create_from_pep_508(\n+            f\"six==1.15.0 ; {python27} or {python36} or {python27} and {linux} or\"\n+            f\" {python36} and {linux}\"\n         ),\n-        \"six\": Dependency.create_from_pep_508(\"six==1.15.0\"),\n     }\n \n     for line in content.strip().split(\"\\n\"):\n@@ -368,11 +389,19 @@ def test_exporter_can_export_requirements_txt_pyinstaller(tmp_dir: str, poetry:\n     # \u251c\u2500\u2500 altgraph *      dependencies into a single package.\n     # \u251c\u2500\u2500 macholib >=1.8 -- only on Darwin\n     # \u2502   \u2514\u2500\u2500 altgraph >=0.15\n+    python27 = 'python_version >= \"2.7\" and python_version < \"2.8\"'\n+    python36 = 'python_version >= \"3.6\" and python_version < \"4.0\"'\n+    darwin = 'sys_platform==\"darwin\"'\n     expected = {\n-        \"pyinstaller\": Dependency.create_from_pep_508(\"pyinstaller==4.0\"),\n-        \"altgraph\": Dependency.create_from_pep_508(\"altgraph==0.17\"),\n+        \"pyinstaller\": Dependency.create_from_pep_508(\n+            f\"pyinstaller==4.0 ; {python27} or {python36}\"\n+        ),\n+        \"altgraph\": Dependency.create_from_pep_508(\n+            f\"altgraph==0.17 ; {python27} or {python36} or {python27} and {darwin} or\"\n+            f\" {python36} and {darwin}\"\n+        ),\n         \"macholib\": Dependency.create_from_pep_508(\n-            \"macholib==1.8 ; sys_platform == 'darwin'\"\n+            f\"macholib==1.8 ; {python27} and {darwin} or {python36} and {darwin}\"\n         ),\n     }\n \n@@ -441,17 +470,21 @@ def test_exporter_can_export_requirements_txt_with_nested_packages_and_markers(\n     with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n         content = f.read()\n \n+    python27 = 'python_version >= \"2.7\" and python_version < \"2.8\"'\n+    python36 = 'python_version >= \"3.6\" and python_version < \"3.7\"'\n+    windows = 'platform_system == \"Windows\"'\n+    win32 = 'sys_platform == \"win32\"'\n     expected = {\n-        \"a\": Dependency.create_from_pep_508(\"a==1.2.3 ; python_version < '3.7'\"),\n+        \"a\": Dependency.create_from_pep_508(f\"a==1.2.3 ; {python27} or {python36}\"),\n         \"b\": Dependency.create_from_pep_508(\n-            \"b==4.5.6 ; platform_system == 'Windows' and python_version < '3.7'\"\n+            f\"b==4.5.6 ; {python27} and {windows} or {python36} and {windows}\"\n         ),\n         \"c\": Dependency.create_from_pep_508(\n-            \"c==7.8.9 ; sys_platform == 'win32' and python_version < '3.7'\"\n+            f\"c==7.8.9 ; {python27} and {win32} or {python36} and {win32}\"\n         ),\n         \"d\": Dependency.create_from_pep_508(\n-            \"d==0.0.1 ; platform_system == 'Windows' and python_version < '3.7' or\"\n-            \" sys_platform == 'win32' and python_version < '3.7'\"\n+            f\"d==0.0.1 ; {python27} and {windows} or {python36} and {windows} or\"\n+            f\" {python27} and {win32} or {python36} and {win32}\"\n         ),\n     }\n \n@@ -467,7 +500,25 @@ def test_exporter_can_export_requirements_txt_with_nested_packages_and_markers(\n \n @pytest.mark.parametrize(\n     [\"dev\", \"lines\"],\n-    [(False, ['a==1.2.3 ; python_version < \"3.8\"']), (True, [\"a==1.2.3\", \"b==4.5.6\"])],\n+    [\n+        (\n+            False,\n+            [\n+                'a==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"3.8\"'\n+            ],\n+        ),\n+        (\n+            True,\n+            [\n+                'a==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"3.8\" or python_version'\n+                ' >= \"3.6\" and python_version < \"4.0\"',\n+                'b==4.5.6 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+            ],\n+        ),\n+    ],\n )\n def test_exporter_can_export_requirements_txt_with_nested_packages_and_markers_any(\n     tmp_dir: str, poetry: Poetry, dev: bool, lines: list[str]\n@@ -560,9 +611,13 @@ def test_exporter_can_export_requirements_txt_with_standard_packages_and_hashes(\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -609,8 +664,12 @@ def test_exporter_can_export_requirements_txt_with_standard_packages_and_hashes_\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6\n-foo==1.2.3\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -654,7 +713,9 @@ def test_exporter_exports_requirements_txt_without_dev_packages_by_default(\n         content = f.read()\n \n     expected = \"\"\"\\\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -699,9 +760,13 @@ def test_exporter_exports_requirements_txt_with_dev_packages_if_opted_in(\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -746,7 +811,9 @@ def test_exporter_exports_requirements_txt_without_optional_packages(\n         content = f.read()\n \n     expected = \"\"\"\\\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -756,10 +823,42 @@ def test_exporter_exports_requirements_txt_without_optional_packages(\n @pytest.mark.parametrize(\n     [\"extras\", \"lines\"],\n     [\n-        (None, [\"foo==1.2.3\"]),\n-        (False, [\"foo==1.2.3\"]),\n-        (True, [\"bar==4.5.6\", \"foo==1.2.3\", \"spam==0.1.0\"]),\n-        ([\"feature_bar\"], [\"bar==4.5.6\", \"foo==1.2.3\", \"spam==0.1.0\"]),\n+        (\n+            None,\n+            [\n+                'foo==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"'\n+            ],\n+        ),\n+        (\n+            False,\n+            [\n+                'foo==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"'\n+            ],\n+        ),\n+        (\n+            True,\n+            [\n+                'bar==4.5.6 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'foo==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'spam==0.1.0 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+            ],\n+        ),\n+        (\n+            [\"feature_bar\"],\n+            [\n+                'bar==4.5.6 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'foo==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'spam==0.1.0 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+            ],\n+        ),\n     ],\n )\n def test_exporter_exports_requirements_txt_with_optional_packages(\n@@ -859,7 +958,9 @@ def test_exporter_can_export_requirements_txt_with_git_packages(\n         content = f.read()\n \n     expected = \"\"\"\\\n-foo @ git+https://github.com/foo/foo.git@123456\n+foo @ git+https://github.com/foo/foo.git@123456 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -909,8 +1010,12 @@ def test_exporter_can_export_requirements_txt_with_nested_packages(\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6\n-foo @ git+https://github.com/foo/foo.git@123456\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo @ git+https://github.com/foo/foo.git@123456 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -964,9 +1069,15 @@ def test_exporter_can_export_requirements_txt_with_nested_packages_cyclic(\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6\n-baz==7.8.9\n-foo==1.2.3\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+baz==7.8.9 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -1036,13 +1147,19 @@ def test_exporter_can_export_requirements_txt_with_nested_packages_and_multiple_\n     with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n         content = f.read()\n \n-    expected = textwrap.dedent(\n-        \"\"\"\\\n-        bar==7.8.9\n-        baz==10.11.13 ; platform_system == \"Windows\"\n-        foo==1.2.3\n-        \"\"\"\n-    )\n+    expected = \"\"\"\\\n+bar==7.8.9 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" and platform_system != \"Windows\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" and platform_system != \"Windows\" or\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" and platform_system == \"Windows\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" and platform_system == \"Windows\"\n+baz==10.11.13 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" and platform_system == \"Windows\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" and platform_system == \"Windows\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+\"\"\"\n \n     assert content == expected\n \n@@ -1084,7 +1201,9 @@ def test_exporter_can_export_requirements_txt_with_git_packages_and_markers(\n         content = f.read()\n \n     expected = \"\"\"\\\n-foo @ git+https://github.com/foo/foo.git@123456 ; python_version < \"3.7\"\n+foo @ git+https://github.com/foo/foo.git@123456 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"3.7\"\n \"\"\"\n \n     assert content == expected\n@@ -1126,7 +1245,9 @@ def test_exporter_can_export_requirements_txt_with_directory_packages(\n         content = f.read()\n \n     expected = f\"\"\"\\\n-foo @ {working_directory.as_uri()}/tests/fixtures/sample_project\n+foo @ {working_directory.as_uri()}/tests/fixtures/sample_project ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -1192,9 +1313,15 @@ def test_exporter_can_export_requirements_txt_with_nested_directory_packages(\n         content = f.read()\n \n     expected = f\"\"\"\\\n-bar @ {working_directory.as_uri()}/tests/fixtures/project_with_nested_local/bar\n-baz @ {working_directory.as_uri()}/tests/fixtures/project_with_nested_local\n-foo @ {working_directory.as_uri()}/tests/fixtures/sample_project\n+bar @ {working_directory.as_uri()}/tests/fixtures/project_with_nested_local/bar ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+baz @ {working_directory.as_uri()}/tests/fixtures/project_with_nested_local ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo @ {working_directory.as_uri()}/tests/fixtures/sample_project ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -1237,8 +1364,9 @@ def test_exporter_can_export_requirements_txt_with_directory_packages_and_marker\n         content = f.read()\n \n     expected = f\"\"\"\\\n-foo @ {working_directory.as_uri()}/tests/fixtures/sample_project\\\n- ; python_version < \"3.7\"\n+foo @ {working_directory.as_uri()}/tests/fixtures/sample_project ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"3.7\"\n \"\"\"\n \n     assert content == expected\n@@ -1280,7 +1408,9 @@ def test_exporter_can_export_requirements_txt_with_file_packages(\n         content = f.read()\n \n     expected = f\"\"\"\\\n-foo @ {working_directory.as_uri()}/tests/fixtures/distributions/demo-0.1.0.tar.gz\n+foo @ {working_directory.as_uri()}/tests/fixtures/distributions/demo-0.1.0.tar.gz ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert content == expected\n@@ -1323,8 +1453,9 @@ def test_exporter_can_export_requirements_txt_with_file_packages_and_markers(\n         content = f.read()\n \n     expected = f\"\"\"\\\n-foo @ {working_directory.as_uri()}/tests/fixtures/distributions/demo-0.1.0.tar.gz\\\n- ; python_version < \"3.7\"\n+foo @ {working_directory.as_uri()}/tests/fixtures/distributions/demo-0.1.0.tar.gz ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"3.7\"\n \"\"\"\n \n     assert content == expected\n@@ -1381,9 +1512,13 @@ def test_exporter_exports_requirements_txt_with_legacy_packages(\n     expected = \"\"\"\\\n --extra-index-url https://example.com/simple\n \n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -1439,9 +1574,13 @@ def test_exporter_exports_requirements_txt_with_url_false(tmp_dir: str, poetry:\n         content = f.read()\n \n     expected = \"\"\"\\\n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -1492,7 +1631,9 @@ def test_exporter_exports_requirements_txt_with_legacy_packages_trusted_host(\n --trusted-host example.com\n --extra-index-url http://example.com/simple\n \n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n \"\"\"\n \n@@ -1502,8 +1643,26 @@ def test_exporter_exports_requirements_txt_with_legacy_packages_trusted_host(\n @pytest.mark.parametrize(\n     [\"dev\", \"expected\"],\n     [\n-        (True, [\"bar==1.2.2\", \"baz==1.2.3\", \"foo==1.2.1\"]),\n-        (False, [\"bar==1.2.2\", \"foo==1.2.1\"]),\n+        (\n+            True,\n+            [\n+                'bar==1.2.2 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'baz==1.2.3 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'foo==1.2.1 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+            ],\n+        ),\n+        (\n+            False,\n+            [\n+                'bar==1.2.2 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+                'foo==1.2.1 ; python_version >= \"2.7\" and python_version < \"2.8\" or'\n+                ' python_version >= \"3.6\" and python_version < \"4.0\"',\n+            ],\n+        ),\n     ],\n )\n def test_exporter_exports_requirements_txt_with_dev_extras(\n@@ -1636,11 +1795,17 @@ def test_exporter_exports_requirements_txt_with_legacy_packages_and_duplicate_so\n --extra-index-url https://example.com/simple\n --extra-index-url https://foobaz.com/simple\n \n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-baz==7.8.9 \\\\\n+baz==7.8.9 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:24680\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -1707,9 +1872,13 @@ def test_exporter_exports_requirements_txt_with_legacy_packages_and_credentials(\n     expected = \"\"\"\\\n --extra-index-url https://foo:bar@example.com/simple\n \n-bar==4.5.6 \\\\\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:67890\n-foo==1.2.3 \\\\\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\" \\\\\n     --hash=sha256:12345\n \"\"\"\n \n@@ -1752,8 +1921,248 @@ def test_exporter_exports_requirements_txt_to_standard_output(\n \n     out, err = capsys.readouterr()\n     expected = \"\"\"\\\n-bar==4.5.6\n-foo==1.2.3\n+bar==4.5.6 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+foo==1.2.3 ;\\\n+ python_version >= \"2.7\" and python_version < \"2.8\" or\\\n+ python_version >= \"3.6\" and python_version < \"4.0\"\n+\"\"\"\n+\n+    assert out == expected\n+\n+\n+def test_exporter_doesnt_confuse_repeated_packages(\n+    tmp_dir: str, poetry: Poetry, capsys: CaptureFixture\n+):\n+    # Testcase derived from <https://github.com/python-poetry/poetry/issues/5141>.\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"celery\",\n+                    \"version\": \"5.1.2\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"<3.7\",\n+                    \"dependencies\": {\n+                        \"click\": \">=7.0,<8.0\",\n+                        \"click-didyoumean\": \">=0.0.3\",\n+                        \"click-plugins\": \">=1.1.1\",\n+                    },\n+                },\n+                {\n+                    \"name\": \"celery\",\n+                    \"version\": \"5.2.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \">=3.7\",\n+                    \"dependencies\": {\n+                        \"click\": \">=8.0.3,<9.0\",\n+                        \"click-didyoumean\": \">=0.0.3\",\n+                        \"click-plugins\": \">=1.1.1\",\n+                    },\n+                },\n+                {\n+                    \"name\": \"click\",\n+                    \"version\": \"7.1.2\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": (\n+                        \">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*\"\n+                    ),\n+                },\n+                {\n+                    \"name\": \"click\",\n+                    \"version\": \"8.0.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \">=3.6\",\n+                    \"dependencies\": {},\n+                },\n+                {\n+                    \"name\": \"click-didyoumean\",\n+                    \"version\": \"0.0.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"click\": \"*\"},\n+                },\n+                {\n+                    \"name\": \"click-didyoumean\",\n+                    \"version\": \"0.3.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \">=3.6.2,<4.0.0\",\n+                    \"dependencies\": {\"click\": \">=7\"},\n+                },\n+                {\n+                    \"name\": \"click-plugins\",\n+                    \"version\": \"1.1.1\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"click\": \">=4.0\"},\n+                },\n+            ],\n+            \"metadata\": {\n+                \"lock-version\": \"1.1\",\n+                \"python-versions\": \"^3.6\",\n+                \"content-hash\": (\n+                    \"832b13a88e5020c27cbcd95faa577bf0dbf054a65c023b45dc9442b640d414e6\"\n+                ),\n+                \"hashes\": {\n+                    \"celery\": [],\n+                    \"click-didyoumean\": [],\n+                    \"click-plugins\": [],\n+                    \"click\": [],\n+                },\n+            },\n+        }\n+    )\n+    root = poetry.package.with_dependency_groups([], only=True)\n+    root.python_versions = \"^3.6\"\n+    root.add_dependency(\n+        Factory.create_dependency(\n+            name=\"celery\", constraint={\"version\": \"5.1.2\", \"python\": \"<3.7\"}\n+        )\n+    )\n+    root.add_dependency(\n+        Factory.create_dependency(\n+            name=\"celery\", constraint={\"version\": \"5.2.3\", \"python\": \">=3.7\"}\n+        )\n+    )\n+    poetry._package = root\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), sys.stdout)\n+\n+    out, err = capsys.readouterr()\n+    expected = \"\"\"\\\n+celery==5.1.2 ; python_version >= \"3.6\" and python_version < \"3.7\"\n+celery==5.2.3 ; python_version >= \"3.7\" and python_version < \"4.0\"\n+click-didyoumean==0.0.3 ; python_version >= \"3.6\" and python_version < \"3.7\"\n+click-didyoumean==0.3.0 ; python_version >= \"3.7\" and python_full_version < \"4.0.0\"\n+click-plugins==1.1.1 ;\\\n+ python_version >= \"3.6\" and python_version < \"3.7\" or\\\n+ python_version >= \"3.7\" and python_version < \"4.0\"\n+click==7.1.2 ; python_version >= \"3.6\" and python_version < \"3.7\"\n+click==8.0.3 ;\\\n+ python_version >= \"3.7\" and python_version < \"4.0\" or\\\n+ python_version >= \"3.7\" and python_full_version < \"4.0.0\"\n+\"\"\"\n+\n+    assert out == expected\n+\n+\n+def test_exporter_handles_extras_next_to_non_extras(\n+    tmp_dir: str, poetry: Poetry, capsys: CaptureFixture\n+):\n+    # Testcase similar to the solver testcase added at #5305.\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"localstack\",\n+                    \"python-versions\": \"*\",\n+                    \"version\": \"1.0.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"dependencies\": {\n+                        \"localstack-ext\": [\n+                            {\"version\": \">=1.0.0\"},\n+                            {\n+                                \"version\": \">=1.0.0\",\n+                                \"extras\": [\"bar\"],\n+                                \"markers\": 'extra == \"foo\"',\n+                            },\n+                        ]\n+                    },\n+                    \"extras\": {\"foo\": [\"localstack-ext (>=1.0.0)\"]},\n+                },\n+                {\n+                    \"name\": \"localstack-ext\",\n+                    \"python-versions\": \"*\",\n+                    \"version\": \"1.0.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"dependencies\": {\n+                        \"something\": \"*\",\n+                        \"something-else\": {\n+                            \"version\": \">=1.0.0\",\n+                            \"markers\": 'extra == \"bar\"',\n+                        },\n+                        \"another-thing\": {\n+                            \"version\": \">=1.0.0\",\n+                            \"markers\": 'extra == \"baz\"',\n+                        },\n+                    },\n+                    \"extras\": {\n+                        \"bar\": [\"something-else (>=1.0.0)\"],\n+                        \"baz\": [\"another-thing (>=1.0.0)\"],\n+                    },\n+                },\n+                {\n+                    \"name\": \"something\",\n+                    \"python-versions\": \"*\",\n+                    \"version\": \"1.0.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"dependencies\": {},\n+                },\n+                {\n+                    \"name\": \"something-else\",\n+                    \"python-versions\": \"*\",\n+                    \"version\": \"1.0.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"dependencies\": {},\n+                },\n+                {\n+                    \"name\": \"another-thing\",\n+                    \"python-versions\": \"*\",\n+                    \"version\": \"1.0.0\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"dependencies\": {},\n+                },\n+            ],\n+            \"metadata\": {\n+                \"lock-version\": \"1.1\",\n+                \"python-versions\": \"^3.6\",\n+                \"content-hash\": (\n+                    \"832b13a88e5020c27cbcd95faa577bf0dbf054a65c023b45dc9442b640d414e6\"\n+                ),\n+                \"hashes\": {\n+                    \"localstack\": [],\n+                    \"localstack-ext\": [],\n+                    \"something\": [],\n+                    \"something-else\": [],\n+                    \"another-thing\": [],\n+                },\n+            },\n+        }\n+    )\n+    root = poetry.package.with_dependency_groups([], only=True)\n+    root.python_versions = \"^3.6\"\n+    root.add_dependency(\n+        Factory.create_dependency(\n+            name=\"localstack\", constraint={\"version\": \"^1.0.0\", \"extras\": [\"foo\"]}\n+        )\n+    )\n+    poetry._package = root\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), sys.stdout)\n+\n+    out, err = capsys.readouterr()\n+    expected = \"\"\"\\\n+localstack-ext==1.0.0 ; python_version >= \"3.6\" and python_version < \"4.0\"\n+localstack==1.0.0 ; python_version >= \"3.6\" and python_version < \"4.0\"\n+something-else==1.0.0 ; python_version >= \"3.6\" and python_version < \"4.0\"\n+something==1.0.0 ; python_version >= \"3.6\" and python_version < \"4.0\"\n \"\"\"\n \n     assert out == expected"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/console/commands/test_export.py\npoetry run pytest tests/utils/test_exporter.py"
            },
            {
                "id": 3303,
                "created_at": "2020-10-28T16:59:37Z",
                "closed_at": "2021-03-25T19:03:47Z",
                "title": "Regression: 403 Auth Failure for S3-hosted (Cloudfront) Private Repos",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "3c9ced2e12618f9a9946a76c0430b8c80c0d0374",
                        "commit_date": "2021-03-25T19:03:45Z",
                        "parents": "931cf120c80d1400cd5b9933db7cf078ab3718f4",
                        "stat": {
                            "total": 15,
                            "additions": 28,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "f0551e5adf89a523ea4bf36dfe8f08d614281a46",
                                    "filename": "poetry/repositories/legacy_repository.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 7,
                                    "changes": 13,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/3c9ced2e12618f9a9946a76c0430b8c80c0d0374/poetry%2Frepositories%2Flegacy_repository.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/3c9ced2e12618f9a9946a76c0430b8c80c0d0374/poetry%2Frepositories%2Flegacy_repository.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Frepositories%2Flegacy_repository.py?ref=3c9ced2e12618f9a9946a76c0430b8c80c0d0374",
                                    "patch": "@@ -423,19 +423,18 @@ def _get(self, endpoint: str) -> Optional[Page]:\n         url = self._url + endpoint\n         try:\n             response = self.session.get(url)\n+            if response.status_code in (401, 403):\n+                self._log(\n+                    \"Authorization error accessing {url}\".format(url=url),\n+                    level=\"warning\",\n+                )\n+                return\n             if response.status_code == 404:\n                 return\n             response.raise_for_status()\n         except requests.HTTPError as e:\n             raise RepositoryError(e)\n \n-        if response.status_code in (401, 403):\n-            self._log(\n-                \"Authorization error accessing {url}\".format(url=response.url),\n-                level=\"warn\",\n-            )\n-            return\n-\n         if response.url != url:\n             self._log(\n                 \"Response URL {response_url} differs from request URL {url}\".format("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a1db15952e1e9d07e18810ce76fb0b33d8d52588",
                                    "filename": "tests/repositories/test_legacy_repository.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 8,
                                    "changes": 15,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/3c9ced2e12618f9a9946a76c0430b8c80c0d0374/tests%2Frepositories%2Ftest_legacy_repository.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/3c9ced2e12618f9a9946a76c0430b8c80c0d0374/tests%2Frepositories%2Ftest_legacy_repository.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Frepositories%2Ftest_legacy_repository.py?ref=3c9ced2e12618f9a9946a76c0430b8c80c0d0374",
                                    "patch": "@@ -338,19 +338,18 @@ def test_get_200_returns_page(http):\n     assert repo._get(\"/foo\")\n \n \n-def test_get_404_returns_none(http):\n-    repo = MockHttpRepository({\"/foo\": 404}, http)\n+@pytest.mark.parametrize(\"status_code\", [401, 403, 404])\n+def test_get_40x_and_returns_none(http, status_code):\n+    repo = MockHttpRepository({\"/foo\": status_code}, http)\n \n     assert repo._get(\"/foo\") is None\n \n \n-def test_get_4xx_and_5xx_raises(http):\n-    endpoints = {\"/{}\".format(code): code for code in {401, 403, 500}}\n-    repo = MockHttpRepository(endpoints, http)\n+def test_get_5xx_raises(http):\n+    repo = MockHttpRepository({\"/foo\": 500}, http)\n \n-    for endpoint in endpoints:\n-        with pytest.raises(RepositoryError):\n-            repo._get(endpoint)\n+    with pytest.raises(RepositoryError):\n+        repo._get(\"/foo\")\n \n \n def test_get_redirected_response_url(http, monkeypatch):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/repositories/test_legacy_repository.py"
            },
            {
                "id": 3263,
                "created_at": "2020-10-21T18:40:02Z",
                "closed_at": "2020-10-23T21:22:15Z",
                "title": " Poetry attempts to install all VCS deps as editable, even when `develop = false`.",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "7c728f0aacb0abf1084c7cc189f2ed24e43386b3",
                        "commit_date": "2020-10-23T21:22:09Z",
                        "parents": "3067fd4af68addc13aad37048637617a0c68e322",
                        "stat": {
                            "total": 1,
                            "additions": 13,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "f1637407068cb0fa659d393ef78be8c91f44c026",
                                    "filename": "poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/poetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/poetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpackages%2Flocker.py?ref=7c728f0aacb0abf1084c7cc189f2ed24e43386b3",
                                    "patch": "@@ -582,7 +582,7 @@ def _dump_package(self, package):  # type: (Package) -> dict\n             if package.source_resolved_reference:\n                 data[\"source\"][\"resolved_reference\"] = package.source_resolved_reference\n \n-            if package.source_type == \"directory\":\n+            if package.source_type in [\"directory\", \"git\"]:\n                 data[\"develop\"] = package.develop\n \n         return data"
                                },
                                {
                                    "sha": "c05efbd68447ce011e97e65bc5dc95c736532f07",
                                    "filename": "poetry/puzzle/provider.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/poetry%2Fpuzzle%2Fprovider.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/poetry%2Fpuzzle%2Fprovider.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpuzzle%2Fprovider.py?ref=7c728f0aacb0abf1084c7cc189f2ed24e43386b3",
                                    "patch": "@@ -168,6 +168,7 @@ def search_for_vcs(self, dependency):  # type: (VCSDependency) -> List[Package]\n             rev=dependency.rev,\n             name=dependency.name,\n         )\n+        package.develop = dependency.develop\n \n         dependency._constraint = package.version\n         dependency._pretty_constraint = package.version.text"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7fa89a44962630a95a23278a2e03a8944367d046",
                                    "filename": "tests/packages/test_locker.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/tests%2Fpackages%2Ftest_locker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/tests%2Fpackages%2Ftest_locker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fpackages%2Ftest_locker.py?ref=7c728f0aacb0abf1084c7cc189f2ed24e43386b3",
                                    "patch": "@@ -73,6 +73,7 @@ def test_lock_file_data_is_ordered(locker, root):\n category = \"main\"\n optional = false\n python-versions = \"*\"\n+develop = true\n \n [package.source]\n type = \"git\""
                                },
                                {
                                    "sha": "ecab7f3ab2d8d92883d3817cd0af311fd06e903f",
                                    "filename": "tests/puzzle/test_provider.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/tests%2Fpuzzle%2Ftest_provider.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/7c728f0aacb0abf1084c7cc189f2ed24e43386b3/tests%2Fpuzzle%2Ftest_provider.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fpuzzle%2Ftest_provider.py?ref=7c728f0aacb0abf1084c7cc189f2ed24e43386b3",
                                    "patch": "@@ -47,6 +47,15 @@ def provider(root, pool):\n     return Provider(root, pool, NullIO())\n \n \n+@pytest.mark.parametrize(\"value\", [True, False])\n+def test_search_for_vcs_retains_develop_flag(provider, value):\n+    dependency = VCSDependency(\n+        \"demo\", \"git\", \"https://github.com/demo/demo.git\", develop=value\n+    )\n+    package = provider.search_for_vcs(dependency)[0]\n+    assert package.develop == value\n+\n+\n def test_search_for_vcs_setup_egg_info(provider):\n     dependency = VCSDependency(\"demo\", \"git\", \"https://github.com/demo/demo.git\")\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/packages/test_locker.py\npoetry run pytest tests/puzzle/test_provider.py"
            },
            {
                "id": 3224,
                "created_at": "2020-10-16T12:56:56Z",
                "closed_at": "2020-10-23T21:22:14Z",
                "title": "Extra dependencies for package on private repo uninstalled when running `poetry install`.",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "89e1d7c11c7566e5735173e17a041a4894f870a4",
                        "commit_date": "2020-10-23T21:22:09Z",
                        "parents": "68f2cc7032d738981a4af52fdff159df95e55a12",
                        "stat": {
                            "total": 7,
                            "additions": 229,
                            "deletions": 222,
                            "files": [
                                {
                                    "sha": "ac7915374508207c02f8141b41c5fe4c071c1523",
                                    "filename": "poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 17,
                                    "deletions": 6,
                                    "changes": 23,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/89e1d7c11c7566e5735173e17a041a4894f870a4/poetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/89e1d7c11c7566e5735173e17a041a4894f870a4/poetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpackages%2Flocker.py?ref=89e1d7c11c7566e5735173e17a041a4894f870a4",
                                    "patch": "@@ -24,12 +24,14 @@\n \n import poetry.repositories\n \n+from poetry.core.packages import dependency_from_pep_508\n from poetry.core.packages.package import Dependency\n from poetry.core.packages.package import Package\n from poetry.core.semver import parse_constraint\n from poetry.core.semver.version import Version\n from poetry.core.toml.file import TOMLFile\n from poetry.core.version.markers import parse_marker\n+from poetry.core.version.requirements import InvalidRequirement\n from poetry.packages import DependencyPackage\n from poetry.utils._compat import OrderedDict\n from poetry.utils._compat import Path\n@@ -142,11 +144,18 @@ def locked_repository(\n                     package.extras[name] = []\n \n                     for dep in deps:\n-                        m = re.match(r\"^(.+?)(?:\\s+\\((.+)\\))?$\", dep)\n-                        dep_name = m.group(1)\n-                        constraint = m.group(2) or \"*\"\n-\n-                        package.extras[name].append(Dependency(dep_name, constraint))\n+                        try:\n+                            dependency = dependency_from_pep_508(dep)\n+                        except InvalidRequirement:\n+                            # handle lock files with invalid PEP 508\n+                            m = re.match(r\"^(.+?)(?:\\[(.+?)])?(?:\\s+\\((.+)\\))?$\", dep)\n+                            dep_name = m.group(1)\n+                            extras = m.group(2) or \"\"\n+                            constraint = m.group(3) or \"*\"\n+                            dependency = Dependency(\n+                                dep_name, constraint, extras=extras.split(\",\")\n+                            )\n+                        package.extras[name].append(dependency)\n \n             if \"marker\" in info:\n                 package.marker = parse_marker(info[\"marker\"])\n@@ -543,8 +552,10 @@ def _dump_package(self, package):  # type: (Package) -> dict\n         if package.extras:\n             extras = {}\n             for name, deps in package.extras.items():\n+                # TODO: This should use dep.to_pep_508() once this is fixed\n+                # https://github.com/python-poetry/poetry-core/pull/102\n                 extras[name] = [\n-                    str(dep) if not dep.constraint.is_any() else dep.name\n+                    dep.base_pep_508_name if not dep.constraint.is_any() else dep.name\n                     for dep in deps\n                 ]\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "042e29670e184a8b243f9fcf31292de337e501f4",
                                    "filename": "tests/installation/fixtures/with-dependencies-extras.test",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-extras.test",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-extras.test",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-extras.test?ref=89e1d7c11c7566e5735173e17a041a4894f870a4",
                                    "patch": "@@ -18,7 +18,7 @@ python-versions = \"*\"\n C = {version = \"^1.0\", optional = true}\n \n [package.extras]\n-foo = [\"C (^1.0)\"]\n+foo = [\"C (>=1.0,<2.0)\"]\n \n [[package]]\n name = \"C\""
                                },
                                {
                                    "sha": "48a22a7c7f3e6566507efb76fca27e40b4d43d01",
                                    "filename": "tests/installation/fixtures/with-dependencies-nested-extras.test",
                                    "status": "added",
                                    "additions": 45,
                                    "deletions": 0,
                                    "changes": 45,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-nested-extras.test",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-nested-extras.test",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ffixtures%2Fwith-dependencies-nested-extras.test?ref=89e1d7c11c7566e5735173e17a041a4894f870a4",
                                    "patch": "@@ -0,0 +1,45 @@\n+[[package]]\n+name = \"A\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[package.dependencies]\n+B = {version = \"^1.0\", optional = true, extras = [\"C\"]}\n+\n+[package.extras]\n+B = [\"B[C] (>=1.0,<2.0)\"]\n+\n+[[package]]\n+name = \"B\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[package.dependencies]\n+C = {version = \"^1.0\", optional = true}\n+\n+[package.extras]\n+C = [\"C (>=1.0,<2.0)\"]\n+\n+[[package]]\n+name = \"C\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[metadata]\n+python-versions = \"*\"\n+lock-version = \"1.1\"\n+content-hash = \"123456789\"\n+\n+[metadata.files]\n+\"A\" = []\n+\"B\" = []\n+\"C\" = []"
                                },
                                {
                                    "sha": "106efde6e9c4782318fce8284a7ab58fa553359b",
                                    "filename": "tests/installation/test_installer.py",
                                    "status": "modified",
                                    "additions": 29,
                                    "deletions": 0,
                                    "changes": 29,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ftest_installer.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Finstallation%2Ftest_installer.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ftest_installer.py?ref=89e1d7c11c7566e5735173e17a041a4894f870a4",
                                    "patch": "@@ -639,6 +639,35 @@ def test_run_with_dependencies_extras(installer, locker, repo, package):\n     assert locker.written_data == expected\n \n \n+def test_run_with_dependencies_nested_extras(installer, locker, repo, package):\n+    package_a = get_package(\"A\", \"1.0\")\n+    package_b = get_package(\"B\", \"1.0\")\n+    package_c = get_package(\"C\", \"1.0\")\n+\n+    dependency_c = Factory.create_dependency(\"C\", {\"version\": \"^1.0\", \"optional\": True})\n+    dependency_b = Factory.create_dependency(\n+        \"B\", {\"version\": \"^1.0\", \"optional\": True, \"extras\": [\"C\"]}\n+    )\n+    dependency_a = Factory.create_dependency(\"A\", {\"version\": \"^1.0\", \"extras\": [\"B\"]})\n+\n+    package_b.extras = {\"C\": [dependency_c]}\n+    package_b.add_dependency(dependency_c)\n+\n+    package_a.add_dependency(dependency_b)\n+    package_a.extras = {\"B\": [dependency_b]}\n+\n+    repo.add_package(package_a)\n+    repo.add_package(package_b)\n+    repo.add_package(package_c)\n+\n+    package.add_dependency(dependency_a)\n+\n+    installer.run()\n+    expected = fixture(\"with-dependencies-nested-extras\")\n+\n+    assert locker.written_data == expected\n+\n+\n def test_run_does_not_install_extras_if_not_requested(installer, locker, repo, package):\n     package.extras[\"foo\"] = [get_dependency(\"D\")]\n     package_a = get_package(\"A\", \"1.0\")"
                                },
                                {
                                    "sha": "a4aa17971f84d3e86a11e52062a1a773925d3ae4",
                                    "filename": "tests/packages/test_locker.py",
                                    "status": "modified",
                                    "additions": 130,
                                    "deletions": 0,
                                    "changes": 130,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Fpackages%2Ftest_locker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/89e1d7c11c7566e5735173e17a041a4894f870a4/tests%2Fpackages%2Ftest_locker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fpackages%2Ftest_locker.py?ref=89e1d7c11c7566e5735173e17a041a4894f870a4",
                                    "patch": "@@ -142,6 +142,136 @@ def test_locker_properly_loads_extras(locker):\n     assert lockfile_dep.name == \"lockfile\"\n \n \n+def test_locker_properly_loads_nested_extras(locker):\n+    content = \"\"\"\\\n+[[package]]\n+name = \"a\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[package.dependencies]\n+b = {version = \"^1.0\", optional = true, extras = \"c\"}\n+\n+[package.extras]\n+b = [\"b[c] (>=1.0,<2.0)\"]\n+\n+[[package]]\n+name = \"b\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[package.dependencies]\n+c = {version = \"^1.0\", optional = true}\n+\n+[package.extras]\n+c = [\"c (>=1.0,<2.0)\"]\n+\n+[[package]]\n+name = \"c\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[metadata]\n+python-versions = \"*\"\n+lock-version = \"1.1\"\n+content-hash = \"123456789\"\n+\n+[metadata.files]\n+\"a\" = []\n+\"b\" = []\n+\"c\" = []\n+\"\"\"\n+\n+    locker.lock.write(tomlkit.parse(content))\n+\n+    repository = locker.locked_repository()\n+    assert 3 == len(repository.packages)\n+\n+    packages = repository.find_packages(get_dependency(\"a\", \"1.0\"))\n+    assert len(packages) == 1\n+\n+    package = packages[0]\n+    assert len(package.requires) == 1\n+    assert len(package.extras) == 1\n+\n+    dependency_b = package.extras[\"b\"][0]\n+    assert dependency_b.name == \"b\"\n+    assert dependency_b.extras == frozenset({\"c\"})\n+\n+    packages = repository.find_packages(dependency_b)\n+    assert len(packages) == 1\n+\n+    package = packages[0]\n+    assert len(package.requires) == 1\n+    assert len(package.extras) == 1\n+\n+    dependency_c = package.extras[\"c\"][0]\n+    assert dependency_c.name == \"c\"\n+    assert dependency_c.extras == frozenset()\n+\n+    packages = repository.find_packages(dependency_c)\n+    assert len(packages) == 1\n+\n+\n+def test_locker_properly_loads_extras_legacy(locker):\n+    content = \"\"\"\\\n+[[package]]\n+name = \"a\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[package.dependencies]\n+b = {version = \"^1.0\", optional = true}\n+\n+[package.extras]\n+b = [\"b (^1.0)\"]\n+\n+[[package]]\n+name = \"b\"\n+version = \"1.0\"\n+description = \"\"\n+category = \"main\"\n+optional = false\n+python-versions = \"*\"\n+\n+[metadata]\n+python-versions = \"*\"\n+lock-version = \"1.1\"\n+content-hash = \"123456789\"\n+\n+[metadata.files]\n+\"a\" = []\n+\"b\" = []\n+\"\"\"\n+\n+    locker.lock.write(tomlkit.parse(content))\n+\n+    repository = locker.locked_repository()\n+    assert 2 == len(repository.packages)\n+\n+    packages = repository.find_packages(get_dependency(\"a\", \"1.0\"))\n+    assert len(packages) == 1\n+\n+    package = packages[0]\n+    assert len(package.requires) == 1\n+    assert len(package.extras) == 1\n+\n+    dependency_b = package.extras[\"b\"][0]\n+    assert dependency_b.name == \"b\"\n+\n+\n def test_lock_packages_with_null_description(locker, root):\n     package_a = get_package(\"A\", \"1.0.0\")\n     package_a.description = None"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/installation/test_installer.py\npoetry run pytest tests/packages/test_locker.py "
            },
            {
                "id": 3213,
                "created_at": "2020-10-15T08:19:46Z",
                "closed_at": "2020-10-23T21:22:14Z",
                "title": "[RecursionError] maximum recursion depth exceeded while calling a Python object",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "e878a5d4e7aeaf022201a55254d5878d1972b63c",
                        "commit_date": "2020-10-23T21:22:09Z",
                        "parents": "89e1d7c11c7566e5735173e17a041a4894f870a4",
                        "stat": {
                            "total": 27,
                            "additions": 108,
                            "deletions": 81,
                            "files": [
                                {
                                    "sha": "d0d03a567795ebdd4e3671ff1f544cdc067413d4",
                                    "filename": "poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 25,
                                    "deletions": 27,
                                    "changes": 52,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/e878a5d4e7aeaf022201a55254d5878d1972b63c/poetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/e878a5d4e7aeaf022201a55254d5878d1972b63c/poetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpackages%2Flocker.py?ref=e878a5d4e7aeaf022201a55254d5878d1972b63c",
                                    "patch": "@@ -226,45 +226,43 @@ def __walk_dependency_level(\n         next_level_dependencies = []\n \n         for requirement in dependencies:\n+            key = (requirement.name, requirement.pretty_constraint)\n             locked_package = cls.__get_locked_package(requirement, packages_by_name)\n \n             if locked_package:\n-                for require in locked_package.requires:\n-                    if require.marker.is_empty():\n-                        require.marker = requirement.marker\n-                    else:\n-                        require.marker = require.marker.intersect(requirement.marker)\n+                # create dependency from locked package to retain dependency metadata\n+                # if this is not done, we can end-up with incorrect nested dependencies\n+                marker = requirement.marker\n+                requirement = locked_package.to_dependency()\n+                requirement.marker = requirement.marker.intersect(marker)\n+\n+                key = (requirement.name, requirement.pretty_constraint)\n+\n+                if pinned_versions:\n+                    requirement.set_constraint(\n+                        locked_package.to_dependency().constraint\n+                    )\n+\n+                if key not in nested_dependencies:\n+                    for require in locked_package.requires:\n+                        if require.marker.is_empty():\n+                            require.marker = requirement.marker\n+                        else:\n+                            require.marker = require.marker.intersect(\n+                                requirement.marker\n+                            )\n \n-                    require.marker = require.marker.intersect(locked_package.marker)\n-                    next_level_dependencies.append(require)\n+                        require.marker = require.marker.intersect(locked_package.marker)\n+                        next_level_dependencies.append(require)\n \n             if requirement.name in project_level_dependencies and level == 0:\n                 # project level dependencies take precedence\n                 continue\n \n-            if locked_package:\n-                # create dependency from locked package to retain dependency metadata\n-                # if this is not done, we can end-up with incorrect nested dependencies\n-                marker = requirement.marker\n-                requirement = locked_package.to_dependency()\n-                requirement.marker = requirement.marker.intersect(marker)\n-            else:\n+            if not locked_package:\n                 # we make a copy to avoid any side-effects\n                 requirement = deepcopy(requirement)\n \n-            if pinned_versions:\n-                requirement.set_constraint(\n-                    cls.__get_locked_package(requirement, packages_by_name)\n-                    .to_dependency()\n-                    .constraint\n-                )\n-\n-            # dependencies use extra to indicate that it was activated via parent\n-            # package's extras, this is not required for nested exports as we assume\n-            # the resolver already selected this dependency\n-            requirement.marker = requirement.marker.without_extras()\n-\n-            key = (requirement.name, requirement.pretty_constraint)\n             if key not in nested_dependencies:\n                 nested_dependencies[key] = requirement\n             else:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "1e660ce69cdcab5e858e8acea3e8ccd3772e6438",
                                    "filename": "tests/utils/test_exporter.py",
                                    "status": "modified",
                                    "additions": 56,
                                    "deletions": 0,
                                    "changes": 56,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/e878a5d4e7aeaf022201a55254d5878d1972b63c/tests%2Futils%2Ftest_exporter.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/e878a5d4e7aeaf022201a55254d5878d1972b63c/tests%2Futils%2Ftest_exporter.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_exporter.py?ref=e878a5d4e7aeaf022201a55254d5878d1972b63c",
                                    "patch": "@@ -696,6 +696,62 @@ def test_exporter_can_export_requirements_txt_with_nested_packages(tmp_dir, poet\n     assert expected == content\n \n \n+def test_exporter_can_export_requirements_txt_with_nested_packages_cyclic(\n+    tmp_dir, poetry\n+):\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"foo\",\n+                    \"version\": \"1.2.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"bar\": {\"version\": \"4.5.6\"}},\n+                },\n+                {\n+                    \"name\": \"bar\",\n+                    \"version\": \"4.5.6\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"baz\": {\"version\": \"7.8.9\"}},\n+                },\n+                {\n+                    \"name\": \"baz\",\n+                    \"version\": \"7.8.9\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"foo\": {\"version\": \"1.2.3\"}},\n+                },\n+            ],\n+            \"metadata\": {\n+                \"python-versions\": \"*\",\n+                \"content-hash\": \"123456789\",\n+                \"hashes\": {\"foo\": [], \"bar\": [], \"baz\": []},\n+            },\n+        }\n+    )\n+    set_package_requires(poetry, skip={\"bar\", \"baz\"})\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), \"requirements.txt\")\n+\n+    with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n+        content = f.read()\n+\n+    expected = \"\"\"\\\n+bar==4.5.6\n+baz==7.8.9\n+foo==1.2.3\n+\"\"\"\n+\n+    assert expected == content\n+\n+\n def test_exporter_can_export_requirements_txt_with_git_packages_and_markers(\n     tmp_dir, poetry\n ):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/utils/test_exporter.py"
            },
            {
                "id": 3115,
                "created_at": "2020-10-07T11:49:00Z",
                "closed_at": "2020-10-14T17:32:04Z",
                "title": "Poetry 1.1.2 does not export dependency url for nested git dependencies",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "02307ba118ce96437b4ad50b8e376adb88a1901b",
                        "commit_date": "2020-10-14T17:31:59Z",
                        "parents": "9f7c9f8b15f8228cde459f143aea721dc835eb17",
                        "stat": {
                            "total": 5,
                            "additions": 71,
                            "deletions": 66,
                            "files": [
                                {
                                    "sha": "2a82fef816a84f7214f1a7c566e537271dfa123b",
                                    "filename": "poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 2,
                                    "changes": 11,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/02307ba118ce96437b4ad50b8e376adb88a1901b/poetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/02307ba118ce96437b4ad50b8e376adb88a1901b/poetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpackages%2Flocker.py?ref=02307ba118ce96437b4ad50b8e376adb88a1901b",
                                    "patch": "@@ -234,8 +234,15 @@ def __get_locked_package(\n                     # project level dependencies take precedence\n                     continue\n \n-                # we make a copy to avoid any side-effects\n-                requirement = deepcopy(requirement)\n+                locked_package = __get_locked_package(requirement)\n+                if locked_package:\n+                    # create dependency from locked package to retain dependency metadata\n+                    # if this is not done, we can end-up with incorrect nested dependencies\n+                    requirement = locked_package.to_dependency()\n+                else:\n+                    # we make a copy to avoid any side-effects\n+                    requirement = deepcopy(requirement)\n+\n                 requirement._category = pkg.category\n \n                 if pinned_versions:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a75fb3da502a2cfaa1e8d968060c3d38d3bb3f38",
                                    "filename": "tests/utils/test_exporter.py",
                                    "status": "modified",
                                    "additions": 57,
                                    "deletions": 3,
                                    "changes": 60,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/02307ba118ce96437b4ad50b8e376adb88a1901b/tests%2Futils%2Ftest_exporter.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/02307ba118ce96437b4ad50b8e376adb88a1901b/tests%2Futils%2Ftest_exporter.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_exporter.py?ref=02307ba118ce96437b4ad50b8e376adb88a1901b",
                                    "patch": "@@ -59,13 +59,18 @@ def poetry(fixture_dir, locker):\n     return p\n \n \n-def set_package_requires(poetry):\n+def set_package_requires(poetry, skip=None):\n+    skip = skip or set()\n     packages = poetry.locker.locked_repository(with_dev_reqs=True).packages\n     poetry.package.requires = [\n-        pkg.to_dependency() for pkg in packages if pkg.category == \"main\"\n+        pkg.to_dependency()\n+        for pkg in packages\n+        if pkg.category == \"main\" and pkg.name not in skip\n     ]\n     poetry.package.dev_requires = [\n-        pkg.to_dependency() for pkg in packages if pkg.category == \"dev\"\n+        pkg.to_dependency()\n+        for pkg in packages\n+        if pkg.category == \"dev\" and pkg.name not in skip\n     ]\n \n \n@@ -503,6 +508,55 @@ def test_exporter_can_export_requirements_txt_with_git_packages(tmp_dir, poetry)\n     assert expected == content\n \n \n+def test_exporter_can_export_requirements_txt_with_nested_packages(tmp_dir, poetry):\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"foo\",\n+                    \"version\": \"1.2.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"source\": {\n+                        \"type\": \"git\",\n+                        \"url\": \"https://github.com/foo/foo.git\",\n+                        \"reference\": \"123456\",\n+                    },\n+                },\n+                {\n+                    \"name\": \"bar\",\n+                    \"version\": \"4.5.6\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"foo\": \"rev 123456\"},\n+                },\n+            ],\n+            \"metadata\": {\n+                \"python-versions\": \"*\",\n+                \"content-hash\": \"123456789\",\n+                \"hashes\": {\"foo\": [], \"bar\": []},\n+            },\n+        }\n+    )\n+    set_package_requires(poetry, skip={\"foo\"})\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), \"requirements.txt\")\n+\n+    with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n+        content = f.read()\n+\n+    expected = \"\"\"\\\n+bar==4.5.6\n+foo @ git+https://github.com/foo/foo.git@123456\n+\"\"\"\n+\n+    assert expected == content\n+\n+\n def test_exporter_can_export_requirements_txt_with_git_packages_and_markers(\n     tmp_dir, poetry\n ):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/utils/test_exporter.py"
            },
            {
                "id": 3112,
                "created_at": "2020-10-07T08:07:53Z",
                "closed_at": "2020-10-14T17:32:05Z",
                "title": "poetry lock/export wrong with sys_platform (works fine on ver 1.0.10)",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "1188b31287df42de615d25b1642ec894c2bb0c12",
                        "commit_date": "2020-10-14T17:31:59Z",
                        "parents": "02307ba118ce96437b4ad50b8e376adb88a1901b",
                        "stat": {
                            "total": 27,
                            "additions": 226,
                            "deletions": 199,
                            "files": [
                                {
                                    "sha": "26c2b5845044c9ceb7eda1a2d125e466ec92c04b",
                                    "filename": "poetry/packages/locker.py",
                                    "status": "modified",
                                    "additions": 59,
                                    "deletions": 27,
                                    "changes": 86,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/1188b31287df42de615d25b1642ec894c2bb0c12/poetry%2Fpackages%2Flocker.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/1188b31287df42de615d25b1642ec894c2bb0c12/poetry%2Fpackages%2Flocker.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fpackages%2Flocker.py?ref=1188b31287df42de615d25b1642ec894c2bb0c12",
                                    "patch": "@@ -215,30 +215,62 @@ def __get_locked_package(\n \n         for dependency in project_requires:\n             dependency = deepcopy(dependency)\n-            if pinned_versions:\n-                locked_package = __get_locked_package(dependency)\n-                if locked_package:\n-                    dependency.set_constraint(locked_package.to_dependency().constraint)\n+            locked_package = __get_locked_package(dependency)\n+            if locked_package:\n+                locked_dependency = locked_package.to_dependency()\n+                locked_dependency.marker = dependency.marker.intersect(\n+                    locked_package.marker\n+                )\n+\n+                if not pinned_versions:\n+                    locked_dependency.set_constraint(dependency.constraint)\n+\n+                dependency = locked_dependency\n+\n             project_level_dependencies.add(dependency.name)\n             dependencies.append(dependency)\n \n         if not with_nested:\n             # return only with project level dependencies\n             return dependencies\n \n-        nested_dependencies = list()\n+        nested_dependencies = dict()\n \n-        for pkg in packages:  # type: Package\n-            for requirement in pkg.requires:  # type: Dependency\n-                if requirement.name in project_level_dependencies:\n+        def __walk_level(\n+            __dependencies, __level\n+        ):  # type: (List[Dependency], int) -> None\n+            if not __dependencies:\n+                return\n+\n+            __next_level = []\n+\n+            for requirement in __dependencies:\n+                __locked_package = __get_locked_package(requirement)\n+\n+                if __locked_package:\n+                    for require in __locked_package.requires:\n+                        if require.marker.is_empty():\n+                            require.marker = requirement.marker\n+                        else:\n+                            require.marker = require.marker.intersect(\n+                                requirement.marker\n+                            )\n+\n+                        require.marker = require.marker.intersect(\n+                            __locked_package.marker\n+                        )\n+                        __next_level.append(require)\n+\n+                if requirement.name in project_level_dependencies and __level == 0:\n                     # project level dependencies take precedence\n                     continue\n \n-                locked_package = __get_locked_package(requirement)\n-                if locked_package:\n+                if __locked_package:\n                     # create dependency from locked package to retain dependency metadata\n                     # if this is not done, we can end-up with incorrect nested dependencies\n-                    requirement = locked_package.to_dependency()\n+                    marker = requirement.marker\n+                    requirement = __locked_package.to_dependency()\n+                    requirement.marker = requirement.marker.intersect(marker)\n                 else:\n                     # we make a copy to avoid any side-effects\n                     requirement = deepcopy(requirement)\n@@ -251,26 +283,26 @@ def __get_locked_package(\n                     )\n \n                 # dependencies use extra to indicate that it was activated via parent\n-                # package's extras\n-                marker = requirement.marker.without_extras()\n-                for project_requirement in project_requires:\n-                    if (\n-                        pkg.name == project_requirement.name\n-                        and project_requirement.constraint.allows(pkg.version)\n-                    ):\n-                        requirement.marker = marker.intersect(\n-                            project_requirement.marker\n-                        )\n-                        break\n+                # package's extras, this is not required for nested exports as we assume\n+                # the resolver already selected this dependency\n+                requirement.marker = requirement.marker.without_extras().intersect(\n+                    pkg.marker\n+                )\n+\n+                key = (requirement.name, requirement.pretty_constraint)\n+                if key not in nested_dependencies:\n+                    nested_dependencies[key] = requirement\n                 else:\n-                    # this dependency was not from a project requirement\n-                    requirement.marker = marker.intersect(pkg.marker)\n+                    nested_dependencies[key].marker = nested_dependencies[\n+                        key\n+                    ].marker.intersect(requirement.marker)\n+\n+            return __walk_level(__next_level, __level + 1)\n \n-                if requirement not in nested_dependencies:\n-                    nested_dependencies.append(requirement)\n+        __walk_level(dependencies, 0)\n \n         return sorted(\n-            itertools.chain(dependencies, nested_dependencies),\n+            itertools.chain(dependencies, nested_dependencies.values()),\n             key=lambda x: x.name.lower(),\n         )\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "66b9e81ff53692f2f5795f0be2ddb0c157b61de3",
                                    "filename": "tests/utils/test_exporter.py",
                                    "status": "modified",
                                    "additions": 140,
                                    "deletions": 0,
                                    "changes": 140,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/1188b31287df42de615d25b1642ec894c2bb0c12/tests%2Futils%2Ftest_exporter.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/1188b31287df42de615d25b1642ec894c2bb0c12/tests%2Futils%2Ftest_exporter.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_exporter.py?ref=1188b31287df42de615d25b1642ec894c2bb0c12",
                                    "patch": "@@ -2,6 +2,7 @@\n \n import pytest\n \n+from poetry.core.packages import dependency_from_pep_508\n from poetry.core.toml.file import TOMLFile\n from poetry.factory import Factory\n from poetry.packages import Locker as BaseLocker\n@@ -175,6 +176,145 @@ def test_exporter_can_export_requirements_txt_with_standard_packages_and_markers\n     assert expected == content\n \n \n+def test_exporter_can_export_requirements_txt_with_nested_packages_and_markers(\n+    tmp_dir, poetry\n+):\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"a\",\n+                    \"version\": \"1.2.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"marker\": \"python_version < '3.7'\",\n+                    \"dependencies\": {\"b\": \">=0.0.0\", \"c\": \">=0.0.0\"},\n+                },\n+                {\n+                    \"name\": \"b\",\n+                    \"version\": \"4.5.6\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"marker\": \"platform_system == 'Windows'\",\n+                    \"dependencies\": {\"d\": \">=0.0.0\"},\n+                },\n+                {\n+                    \"name\": \"c\",\n+                    \"version\": \"7.8.9\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"marker\": \"sys_platform == 'win32'\",\n+                    \"dependencies\": {\"d\": \">=0.0.0\"},\n+                },\n+                {\n+                    \"name\": \"d\",\n+                    \"version\": \"0.0.1\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                },\n+            ],\n+            \"metadata\": {\n+                \"python-versions\": \"*\",\n+                \"content-hash\": \"123456789\",\n+                \"hashes\": {\"a\": [], \"b\": [], \"c\": [], \"d\": []},\n+            },\n+        }\n+    )\n+    set_package_requires(poetry, skip={\"b\", \"c\", \"d\"})\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), \"requirements.txt\")\n+\n+    with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n+        content = f.read()\n+\n+    expected = {\n+        \"a\": dependency_from_pep_508(\"a==1.2.3; python_version < '3.7'\"),\n+        \"b\": dependency_from_pep_508(\n+            \"b==4.5.6; platform_system == 'Windows' and python_version < '3.7'\"\n+        ),\n+        \"c\": dependency_from_pep_508(\n+            \"c==7.8.9; sys_platform == 'win32' and python_version < '3.7'\"\n+        ),\n+        \"d\": dependency_from_pep_508(\n+            \"d==0.0.1; python_version < '3.7' and platform_system == 'Windows' and sys_platform == 'win32'\"\n+        ),\n+    }\n+\n+    for line in content.strip().split(\"\\n\"):\n+        dependency = dependency_from_pep_508(line)\n+        assert dependency.name in expected\n+        expected_dependency = expected.pop(dependency.name)\n+        assert dependency == expected_dependency\n+        assert dependency.marker == expected_dependency.marker\n+\n+    assert expected == {}\n+\n+\n+def test_exporter_can_export_requirements_txt_with_nested_packages_and_markers_any(\n+    tmp_dir, poetry\n+):\n+    poetry.locker.mock_lock_data(\n+        {\n+            \"package\": [\n+                {\n+                    \"name\": \"a\",\n+                    \"version\": \"1.2.3\",\n+                    \"category\": \"main\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                },\n+                {\n+                    \"name\": \"b\",\n+                    \"version\": \"4.5.6\",\n+                    \"category\": \"dev\",\n+                    \"optional\": False,\n+                    \"python-versions\": \"*\",\n+                    \"dependencies\": {\"a\": \">=1.2.3\"},\n+                },\n+            ],\n+            \"metadata\": {\n+                \"python-versions\": \"*\",\n+                \"content-hash\": \"123456789\",\n+                \"hashes\": {\"a\": [], \"b\": []},\n+            },\n+        }\n+    )\n+\n+    poetry.package.requires = [\n+        Factory.create_dependency(\n+            name=\"a\", constraint=dict(version=\"^1.2.3\", python=\"<3.8\")\n+        ),\n+    ]\n+    poetry.package.dev_requires = [\n+        Factory.create_dependency(\n+            name=\"b\", constraint=dict(version=\"^4.5.6\"), category=\"dev\"\n+        ),\n+        Factory.create_dependency(name=\"a\", constraint=dict(version=\"^1.2.3\")),\n+    ]\n+\n+    exporter = Exporter(poetry)\n+\n+    exporter.export(\"requirements.txt\", Path(tmp_dir), \"requirements.txt\", dev=True)\n+\n+    with (Path(tmp_dir) / \"requirements.txt\").open(encoding=\"utf-8\") as f:\n+        content = f.read()\n+\n+    assert (\n+        content\n+        == \"\"\"\\\n+a==1.2.3\n+a==1.2.3; python_version < \"3.8\"\n+b==4.5.6\n+\"\"\"\n+    )\n+\n+\n def test_exporter_can_export_requirements_txt_with_standard_packages_and_hashes(\n     tmp_dir, poetry\n ):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/utils/test_exporter.py"
            },
            {
                "id": 3098,
                "created_at": "2020-10-06T09:34:33Z",
                "closed_at": "2020-10-06T19:59:17Z",
                "title": "Nested relative directory dependency failing to resolve.",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                        "commit_date": "2020-10-06T19:59:14Z",
                        "parents": "08ab6ca43d4de4aa3e4df4d46fb027dc46c1fcea",
                        "stat": {
                            "total": 39,
                            "additions": 186,
                            "deletions": 147,
                            "files": [
                                {
                                    "sha": "73034de16d32453784602ceb7a8f3c695a161523",
                                    "filename": "poetry/inspection/info.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/poetry%2Finspection%2Finfo.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/poetry%2Finspection%2Finfo.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Finspection%2Finfo.py?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -156,6 +156,15 @@ def to_package(\n         package.python_versions = self.requires_python or \"*\"\n         package.files = self.files\n \n+        if root_dir or (self._source_type in {\"directory\"} and self._source_url):\n+            # this is a local poetry project, this means we can extract \"richer\" requirement information\n+            # eg: development requirements etc.\n+            poetry_package = self._get_poetry_package(path=root_dir or self._source_url)\n+            if poetry_package:\n+                package.extras = poetry_package.extras\n+                package.requires = poetry_package.requires\n+                return package\n+\n         for req in self.requires_dist or []:\n             try:\n                 # Attempt to parse the PEP-508 requirement string"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e2b739369246d30b7d477b53401f6bb1b30f40a5",
                                    "filename": "tests/conftest.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 2,
                                    "changes": 9,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Fconftest.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Fconftest.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fconftest.py?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -141,9 +141,14 @@ def http():\n \n \n @pytest.fixture\n-def fixture_dir():\n+def fixture_base():\n+    return Path(__file__).parent / \"fixtures\"\n+\n+\n+@pytest.fixture\n+def fixture_dir(fixture_base):\n     def _fixture_dir(name):\n-        return Path(__file__).parent / \"fixtures\" / name\n+        return fixture_base / name\n \n     return _fixture_dir\n "
                                },
                                {
                                    "sha": "bf058b8813ae382e43d5108c3e8a9340a89a6c54",
                                    "filename": "tests/fixtures/project_with_nested_local/bar/pyproject.toml",
                                    "status": "added",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fbar%2Fpyproject.toml",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fbar%2Fpyproject.toml",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Ffixtures%2Fproject_with_nested_local%2Fbar%2Fpyproject.toml?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -0,0 +1,11 @@\n+[tool.poetry]\n+name = \"bar\"\n+version = \"1.2.3\"\n+description = \"Some description.\"\n+authors = [\"Poetry Maintainer <tests@python-poetry.org>\"]\n+license = \"MIT\"\n+\n+# Requirements\n+[tool.poetry.dependencies]\n+python = \"~2.7 || ^3.4\"\n+quix = { path = \"../quix\", develop = true }"
                                },
                                {
                                    "sha": "1aba06effe1f17237e127d593ce92f0095044d1a",
                                    "filename": "tests/fixtures/project_with_nested_local/foo/pyproject.toml",
                                    "status": "added",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Ffoo%2Fpyproject.toml",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Ffoo%2Fpyproject.toml",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Ffixtures%2Fproject_with_nested_local%2Ffoo%2Fpyproject.toml?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -0,0 +1,11 @@\n+[tool.poetry]\n+name = \"foo\"\n+version = \"1.2.3\"\n+description = \"Some description.\"\n+authors = [\"Poetry Maintainer <tests@python-poetry.org>\"]\n+license = \"MIT\"\n+\n+# Requirements\n+[tool.poetry.dependencies]\n+python = \"~2.7 || ^3.4\"\n+bar = { path = \"../bar\", develop = true }"
                                },
                                {
                                    "sha": "c6eb2c6b46c0024d256da6fd6fc949cef08fde62",
                                    "filename": "tests/fixtures/project_with_nested_local/pyproject.toml",
                                    "status": "added",
                                    "additions": 12,
                                    "deletions": 0,
                                    "changes": 12,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fpyproject.toml",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fpyproject.toml",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Ffixtures%2Fproject_with_nested_local%2Fpyproject.toml?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -0,0 +1,12 @@\n+[tool.poetry]\n+name = \"project-with-nested-local\"\n+version = \"1.2.3\"\n+description = \"Some description.\"\n+authors = [\"Poetry Maintainer <tests@python-poetry.org>\"]\n+license = \"MIT\"\n+\n+# Requirements\n+[tool.poetry.dependencies]\n+python = \"~2.7 || ^3.4\"\n+foo = { path = \"./foo\", develop = true }\n+bar = { path = \"./bar\", develop = true }"
                                },
                                {
                                    "sha": "a90d9bcc41f7ee4c8d929a4d1853fe5f58e44956",
                                    "filename": "tests/fixtures/project_with_nested_local/quix/pyproject.toml",
                                    "status": "added",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fquix%2Fpyproject.toml",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Ffixtures%2Fproject_with_nested_local%2Fquix%2Fpyproject.toml",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Ffixtures%2Fproject_with_nested_local%2Fquix%2Fpyproject.toml?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -0,0 +1,10 @@\n+[tool.poetry]\n+name = \"quix\"\n+version = \"1.2.3\"\n+description = \"Some description.\"\n+authors = [\"Poetry Maintainer <tests@python-poetry.org>\"]\n+license = \"MIT\"\n+\n+# Requirements\n+[tool.poetry.dependencies]\n+python = \"~2.7 || ^3.4\""
                                },
                                {
                                    "sha": "12431f62185baddc9bffca34b45b7e5777b72d09",
                                    "filename": "tests/installation/fixtures/with-directory-dependency-poetry.test",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ffixtures%2Fwith-directory-dependency-poetry.test",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ffixtures%2Fwith-directory-dependency-poetry.test",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ffixtures%2Fwith-directory-dependency-poetry.test?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -16,7 +16,7 @@ python-versions = \"*\"\n version = \"1.2.3\"\n \n [package.dependencies]\n-pendulum = {version = \">=1.4.4\", optional = true, markers = \"extra == \\\"extras_a\\\"\"}\n+pendulum = {version = \">=1.4.4\", optional = true}\n \n [package.extras]\n extras_a = [\"pendulum (>=1.4.4)\"]"
                                },
                                {
                                    "sha": "7a1660670ee3e6b1727d7b5eea7ac02b4d7f8f47",
                                    "filename": "tests/installation/test_installer.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 19,
                                    "changes": 35,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ftest_installer.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ftest_installer.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ftest_installer.py?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -29,9 +29,6 @@\n from tests.repositories.test_pypi_repository import MockRepository\n \n \n-fixtures_dir = Path(\"tests/fixtures\")\n-\n-\n class Installer(BaseInstaller):\n     def _get_installer(self):\n         return NoopInstaller()\n@@ -779,8 +776,8 @@ def test_installer_with_pypi_repository(package, locker, installed, config):\n     assert locker.written_data == expected\n \n \n-def test_run_installs_with_local_file(installer, locker, repo, package):\n-    file_path = fixtures_dir / \"distributions/demo-0.1.0-py2.py3-none-any.whl\"\n+def test_run_installs_with_local_file(installer, locker, repo, package, fixture_dir):\n+    file_path = fixture_dir(\"distributions/demo-0.1.0-py2.py3-none-any.whl\")\n     package.add_dependency(Factory.create_dependency(\"demo\", {\"file\": str(file_path)}))\n \n     repo.add_package(get_package(\"pendulum\", \"1.4.4\"))\n@@ -793,9 +790,11 @@ def test_run_installs_with_local_file(installer, locker, repo, package):\n     assert 2 == installer.executor.installations_count\n \n \n-def test_run_installs_wheel_with_no_requires_dist(installer, locker, repo, package):\n-    file_path = (\n-        fixtures_dir / \"wheel_with_no_requires_dist/demo-0.1.0-py2.py3-none-any.whl\"\n+def test_run_installs_wheel_with_no_requires_dist(\n+    installer, locker, repo, package, fixture_dir\n+):\n+    file_path = fixture_dir(\n+        \"wheel_with_no_requires_dist/demo-0.1.0-py2.py3-none-any.whl\"\n     )\n     package.add_dependency(Factory.create_dependency(\"demo\", {\"file\": str(file_path)}))\n \n@@ -809,31 +808,29 @@ def test_run_installs_wheel_with_no_requires_dist(installer, locker, repo, packa\n \n \n def test_run_installs_with_local_poetry_directory_and_extras(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    file_path = fixtures_dir / \"project_with_extras\"\n+    file_path = fixture_dir(\"project_with_extras\")\n     package.add_dependency(\n         Factory.create_dependency(\n             \"project-with-extras\", {\"path\": str(file_path), \"extras\": [\"extras_a\"]}\n         )\n     )\n-    print(package.requires[0].develop)\n \n     repo.add_package(get_package(\"pendulum\", \"1.4.4\"))\n \n     installer.run()\n \n     expected = fixture(\"with-directory-dependency-poetry\")\n-\n     assert locker.written_data == expected\n \n     assert 2 == installer.executor.installations_count\n \n \n def test_run_installs_with_local_poetry_directory_transitive(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    root_dir = fixtures_dir.joinpath(\"directory\")\n+    root_dir = fixture_dir(\"directory\")\n     package.root_dir = root_dir\n     locker.set_lock_path(root_dir)\n     directory = root_dir.joinpath(\"project_with_transitive_directory_dependencies\")\n@@ -858,12 +855,12 @@ def test_run_installs_with_local_poetry_directory_transitive(\n \n \n def test_run_installs_with_local_poetry_file_transitive(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    root_dir = fixtures_dir.joinpath(\"directory\")\n+    root_dir = fixture_dir(\"directory\")\n     package.root_dir = root_dir\n     locker.set_lock_path(root_dir)\n-    directory = fixtures_dir.joinpath(\"directory\").joinpath(\n+    directory = fixture_dir(\"directory\").joinpath(\n         \"project_with_transitive_file_dependencies\"\n     )\n     package.add_dependency(\n@@ -887,9 +884,9 @@ def test_run_installs_with_local_poetry_file_transitive(\n \n \n def test_run_installs_with_local_setuptools_directory(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    file_path = fixtures_dir / \"project_with_setup/\"\n+    file_path = fixture_dir(\"project_with_setup/\")\n     package.add_dependency(\n         Factory.create_dependency(\"project-with-setup\", {\"path\": str(file_path)})\n     )"
                                },
                                {
                                    "sha": "b92bdce460e81d4a2ee041b523e50490a1eac090",
                                    "filename": "tests/installation/test_installer_old.py",
                                    "status": "modified",
                                    "additions": 16,
                                    "deletions": 17,
                                    "changes": 33,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ftest_installer_old.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Finstallation%2Ftest_installer_old.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ftest_installer_old.py?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -27,9 +27,6 @@\n from tests.repositories.test_pypi_repository import MockRepository\n \n \n-fixtures_dir = Path(\"tests/fixtures\")\n-\n-\n class Installer(BaseInstaller):\n     def _get_installer(self):\n         return NoopInstaller()\n@@ -749,8 +746,8 @@ def test_installer_with_pypi_repository(package, locker, installed, config):\n     assert locker.written_data == expected\n \n \n-def test_run_installs_with_local_file(installer, locker, repo, package):\n-    file_path = fixtures_dir / \"distributions/demo-0.1.0-py2.py3-none-any.whl\"\n+def test_run_installs_with_local_file(installer, locker, repo, package, fixture_dir):\n+    file_path = fixture_dir(\"distributions/demo-0.1.0-py2.py3-none-any.whl\")\n     package.add_dependency(Factory.create_dependency(\"demo\", {\"file\": str(file_path)}))\n \n     repo.add_package(get_package(\"pendulum\", \"1.4.4\"))\n@@ -764,9 +761,11 @@ def test_run_installs_with_local_file(installer, locker, repo, package):\n     assert len(installer.installer.installs) == 2\n \n \n-def test_run_installs_wheel_with_no_requires_dist(installer, locker, repo, package):\n-    file_path = (\n-        fixtures_dir / \"wheel_with_no_requires_dist/demo-0.1.0-py2.py3-none-any.whl\"\n+def test_run_installs_wheel_with_no_requires_dist(\n+    installer, locker, repo, package, fixture_dir\n+):\n+    file_path = fixture_dir(\n+        \"wheel_with_no_requires_dist/demo-0.1.0-py2.py3-none-any.whl\"\n     )\n     package.add_dependency(Factory.create_dependency(\"demo\", {\"file\": str(file_path)}))\n \n@@ -780,9 +779,9 @@ def test_run_installs_wheel_with_no_requires_dist(installer, locker, repo, packa\n \n \n def test_run_installs_with_local_poetry_directory_and_extras(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    file_path = fixtures_dir / \"project_with_extras\"\n+    file_path = fixture_dir(\"project_with_extras\")\n     package.add_dependency(\n         Factory.create_dependency(\n             \"project-with-extras\", {\"path\": str(file_path), \"extras\": [\"extras_a\"]}\n@@ -801,9 +800,9 @@ def test_run_installs_with_local_poetry_directory_and_extras(\n \n \n def test_run_installs_with_local_poetry_directory_transitive(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    root_dir = fixtures_dir.joinpath(\"directory\")\n+    root_dir = fixture_dir(\"directory\")\n     package.root_dir = root_dir\n     locker.set_lock_path(root_dir)\n     directory = root_dir.joinpath(\"project_with_transitive_directory_dependencies\")\n@@ -828,16 +827,16 @@ def test_run_installs_with_local_poetry_directory_transitive(\n \n \n def test_run_installs_with_local_poetry_file_transitive(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    root_dir = fixtures_dir.joinpath(\"directory\")\n+    root_dir = fixture_dir(\"directory\")\n     package.root_dir = root_dir\n     locker.set_lock_path(root_dir)\n     directory = root_dir.joinpath(\"project_with_transitive_file_dependencies\")\n     package.add_dependency(\n         Factory.create_dependency(\n             \"project-with-transitive-file-dependencies\",\n-            {\"path\": str(directory.relative_to(fixtures_dir.joinpath(\"directory\")))},\n+            {\"path\": str(directory.relative_to(fixture_dir(\"directory\")))},\n             root_dir=root_dir,\n         )\n     )\n@@ -855,9 +854,9 @@ def test_run_installs_with_local_poetry_file_transitive(\n \n \n def test_run_installs_with_local_setuptools_directory(\n-    installer, locker, repo, package, tmpdir\n+    installer, locker, repo, package, tmpdir, fixture_dir\n ):\n-    file_path = fixtures_dir / \"project_with_setup/\"\n+    file_path = fixture_dir(\"project_with_setup/\")\n     package.add_dependency(\n         Factory.create_dependency(\"project-with-setup\", {\"path\": str(file_path)})\n     )"
                                },
                                {
                                    "sha": "1393b13f1ef910258d8cef0c4c6158b705831cd5",
                                    "filename": "tests/puzzle/test_solver.py",
                                    "status": "modified",
                                    "additions": 54,
                                    "deletions": 0,
                                    "changes": 54,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Fpuzzle%2Ftest_solver.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/746d741d81f7b2dd39798ac95e867052d8a4d9d0/tests%2Fpuzzle%2Ftest_solver.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fpuzzle%2Ftest_solver.py?ref=746d741d81f7b2dd39798ac95e867052d8a4d9d0",
                                    "patch": "@@ -1672,6 +1672,60 @@ def test_solver_can_resolve_directory_dependencies(solver, repo, package):\n     assert op.package.source_url == path\n \n \n+def test_solver_can_resolve_directory_dependencies_nested_editable(\n+    solver, repo, pool, installed, locked, io\n+):\n+    base = Path(__file__).parent.parent / \"fixtures\" / \"project_with_nested_local\"\n+    poetry = Factory().create_poetry(cwd=base)\n+    package = poetry.package\n+\n+    solver = Solver(\n+        package, pool, installed, locked, io, provider=Provider(package, pool, io)\n+    )\n+\n+    ops = solver.solve()\n+\n+    check_solver_result(\n+        ops,\n+        [\n+            {\n+                \"job\": \"install\",\n+                \"package\": Package(\n+                    \"quix\",\n+                    \"1.2.3\",\n+                    source_type=\"directory\",\n+                    source_url=(base / \"quix\").as_posix(),\n+                ),\n+                \"skipped\": False,\n+            },\n+            {\n+                \"job\": \"install\",\n+                \"package\": Package(\n+                    \"bar\",\n+                    \"1.2.3\",\n+                    source_type=\"directory\",\n+                    source_url=(base / \"bar\").as_posix(),\n+                ),\n+                \"skipped\": False,\n+            },\n+            {\n+                \"job\": \"install\",\n+                \"package\": Package(\n+                    \"foo\",\n+                    \"1.2.3\",\n+                    source_type=\"directory\",\n+                    source_url=(base / \"foo\").as_posix(),\n+                ),\n+                \"skipped\": False,\n+            },\n+        ],\n+    )\n+\n+    for op in ops:\n+        assert op.package.source_type == \"directory\"\n+        assert op.package.develop is True\n+\n+\n def test_solver_can_resolve_directory_dependencies_with_extras(solver, repo, package):\n     pendulum = get_package(\"pendulum\", \"2.0.3\")\n     cleo = get_package(\"cleo\", \"1.0.0\")"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/puzzle/test_solver.py"
            },
            {
                "id": 3079,
                "created_at": "2020-10-05T08:10:49Z",
                "closed_at": "2020-10-23T21:22:13Z",
                "title": "PermissionError installing root package in 1.1.0",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                        "commit_date": "2020-10-23T21:22:09Z",
                        "parents": "a9387815dbb6297c8caf51a37b4a466269029e49",
                        "stat": {
                            "total": 40,
                            "additions": 258,
                            "deletions": 218,
                            "files": [
                                {
                                    "sha": "e44d7a6129778e2296a92fe258fdf30eaf6a5c0c",
                                    "filename": "poetry/installation/pip_installer.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Finstallation%2Fpip_installer.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Finstallation%2Fpip_installer.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Finstallation%2Fpip_installer.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -113,7 +113,9 @@ def remove(self, package):\n             raise\n \n         # This is a workaround for https://github.com/pypa/pip/issues/4176\n-        nspkg_pth_file = self._env.site_packages / \"{}-nspkg.pth\".format(package.name)\n+        nspkg_pth_file = self._env.site_packages.path / \"{}-nspkg.pth\".format(\n+            package.name\n+        )\n         if nspkg_pth_file.exists():\n             nspkg_pth_file.unlink()\n "
                                },
                                {
                                    "sha": "c31f657b656e41297ac719ed4d84fe700e33d4d7",
                                    "filename": "poetry/masonry/builders/editable.py",
                                    "status": "modified",
                                    "additions": 32,
                                    "deletions": 29,
                                    "changes": 61,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Fmasonry%2Fbuilders%2Feditable.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Fmasonry%2Fbuilders%2Feditable.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fmasonry%2Fbuilders%2Feditable.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -94,7 +94,6 @@ def _setup_build(self):\n                 os.remove(str(setup))\n \n     def _add_pth(self):\n-        pth_file = Path(self._module.name).with_suffix(\".pth\")\n         paths = set()\n         for include in self._module.includes:\n             if isinstance(include, PackageInclude) and (\n@@ -106,29 +105,25 @@ def _add_pth(self):\n         for path in paths:\n             content += decode(path + os.linesep)\n \n-        for site_package in [self._env.site_packages, self._env.usersite]:\n-            if not site_package:\n-                continue\n-\n-            try:\n-                site_package.mkdir(parents=True, exist_ok=True)\n-                path = site_package.joinpath(pth_file)\n-                self._debug(\n-                    \"  - Adding <c2>{}</c2> to <b>{}</b> for {}\".format(\n-                        path.name, site_package, self._poetry.file.parent\n-                    )\n+        pth_file = Path(self._module.name).with_suffix(\".pth\")\n+        try:\n+            pth_file = self._env.site_packages.write_text(\n+                pth_file, content, encoding=\"utf-8\"\n+            )\n+            self._debug(\n+                \"  - Adding <c2>{}</c2> to <b>{}</b> for {}\".format(\n+                    pth_file.name, pth_file.parent, self._poetry.file.parent\n                 )\n-                path.write_text(content, encoding=\"utf-8\")\n-                return [path]\n-            except PermissionError:\n-                self._debug(\"- <b>{}</b> is not writable trying next available site\")\n-\n-        self._io.error_line(\n-            \"  - Failed to create <c2>{}</c2> for {}\".format(\n-                pth_file.name, self._poetry.file.parent\n             )\n-        )\n-        return []\n+            return [pth_file]\n+        except OSError:\n+            # TODO: Replace with PermissionError\n+            self._io.error_line(\n+                \"  - Failed to create <c2>{}</c2> for {}\".format(\n+                    pth_file.name, self._poetry.file.parent\n+                )\n+            )\n+            return []\n \n     def _add_scripts(self):\n         added = []\n@@ -187,19 +182,27 @@ def _add_dist_info(self, added_files):\n         added_files = added_files[:]\n \n         builder = WheelBuilder(self._poetry)\n-        dist_info = self._env.site_packages.joinpath(builder.dist_info)\n+\n+        dist_info_path = Path(builder.dist_info)\n+        for dist_info in self._env.site_packages.find(\n+            dist_info_path, writable_only=True\n+        ):\n+            if dist_info.exists():\n+                self._debug(\n+                    \"  - Removing existing <c2>{}</c2> directory from <b>{}</b>\".format(\n+                        dist_info.name, dist_info.parent\n+                    )\n+                )\n+                shutil.rmtree(str(dist_info))\n+\n+        dist_info = self._env.site_packages.mkdir(dist_info_path)\n \n         self._debug(\n             \"  - Adding the <c2>{}</c2> directory to <b>{}</b>\".format(\n-                dist_info.name, self._env.site_packages\n+                dist_info.name, dist_info.parent\n             )\n         )\n \n-        if dist_info.exists():\n-            shutil.rmtree(str(dist_info))\n-\n-        dist_info.mkdir()\n-\n         with dist_info.joinpath(\"METADATA\").open(\"w\", encoding=\"utf-8\") as f:\n             builder._write_metadata_file(f)\n "
                                },
                                {
                                    "sha": "a81de75458d5854c037702a6beddd5e77835904b",
                                    "filename": "poetry/utils/env.py",
                                    "status": "modified",
                                    "additions": 127,
                                    "deletions": 2,
                                    "changes": 129,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Futils%2Fenv.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Futils%2Fenv.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Futils%2Fenv.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -7,6 +7,7 @@\n import shutil\n import sys\n import sysconfig\n+import tempfile\n import textwrap\n \n from contextlib import contextmanager\n@@ -39,6 +40,7 @@\n from poetry.utils._compat import encode\n from poetry.utils._compat import list_to_shell_command\n from poetry.utils._compat import subprocess\n+from poetry.utils.helpers import paths_csv\n \n \n GET_ENVIRONMENT_INFO = \"\"\"\\\n@@ -143,6 +145,125 @@ def _version_nodot(version):\n \"\"\"\n \n \n+class SitePackages:\n+    def __init__(\n+        self, path, fallbacks=None, skip_write_checks=False\n+    ):  # type: (Path, List[Path], bool) -> None\n+        self._path = path\n+        self._fallbacks = fallbacks or []\n+        self._skip_write_checks = skip_write_checks\n+        self._candidates = [self._path] + self._fallbacks\n+        self._writable_candidates = None if not skip_write_checks else self._candidates\n+\n+    @property\n+    def path(self):  # type: () -> Path\n+        return self._path\n+\n+    @property\n+    def candidates(self):  # type: () -> List[Path]\n+        return self._candidates\n+\n+    @property\n+    def writable_candidates(self):  # type: () -> List[Path]\n+        if self._writable_candidates is not None:\n+            return self._writable_candidates\n+\n+        self._writable_candidates = []\n+        for candidate in self._candidates:\n+            try:\n+                if not candidate.exists():\n+                    continue\n+\n+                with tempfile.TemporaryFile(dir=str(candidate)):\n+                    self._writable_candidates.append(candidate)\n+            except (IOError, OSError):\n+                pass\n+\n+        return self._writable_candidates\n+\n+    def make_candidates(\n+        self, path, writable_only=False\n+    ):  # type: (Path, bool) -> List[Path]\n+        candidates = self._candidates if not writable_only else self.writable_candidates\n+        if path.is_absolute():\n+            for candidate in candidates:\n+                try:\n+                    path.relative_to(candidate)\n+                    return [path]\n+                except ValueError:\n+                    pass\n+            else:\n+                raise ValueError(\n+                    \"{} is not relative to any discovered {}sites\".format(\n+                        path, \"writable \" if writable_only else \"\"\n+                    )\n+                )\n+\n+        return [candidate / path for candidate in candidates if candidate]\n+\n+    def _path_method_wrapper(\n+        self, path, method, *args, **kwargs\n+    ):  # type: (Path, str, *Any, **Any) -> Union[Tuple[Path, Any], List[Tuple[Path, Any]]]\n+\n+        # TODO: Move to parameters after dropping Python 2.7\n+        return_first = kwargs.pop(\"return_first\", True)\n+        writable_only = kwargs.pop(\"writable_only\", False)\n+\n+        candidates = self.make_candidates(path, writable_only=writable_only)\n+\n+        if not candidates:\n+            raise RuntimeError(\n+                'Unable to find a suitable destination for \"{}\" in {}'.format(\n+                    str(path), paths_csv(self._candidates)\n+                )\n+            )\n+\n+        results = []\n+\n+        for candidate in candidates:\n+            try:\n+                result = candidate, getattr(candidate, method)(*args, **kwargs)\n+                if return_first:\n+                    return result\n+                else:\n+                    results.append(result)\n+            except (IOError, OSError):\n+                # TODO: Replace with PermissionError\n+                pass\n+\n+        if results:\n+            return results\n+\n+        raise OSError(\"Unable to access any of {}\".format(paths_csv(candidates)))\n+\n+    def write_text(self, path, *args, **kwargs):  # type: (Path, *Any, **Any) -> Path\n+        return self._path_method_wrapper(path, \"write_text\", *args, **kwargs)[0]\n+\n+    def mkdir(self, path, *args, **kwargs):  # type: (Path, *Any, **Any) -> Path\n+        return self._path_method_wrapper(path, \"mkdir\", *args, **kwargs)[0]\n+\n+    def exists(self, path):  # type: (Path) -> bool\n+        return any(\n+            value[-1]\n+            for value in self._path_method_wrapper(path, \"exists\", return_first=False)\n+        )\n+\n+    def find(self, path, writable_only=False):  # type: (Path, bool) -> List[Path]\n+        return [\n+            value[0]\n+            for value in self._path_method_wrapper(\n+                path, \"exists\", return_first=False, writable_only=writable_only\n+            )\n+            if value[-1] is True\n+        ]\n+\n+    def __getattr__(self, item):\n+        try:\n+            return super(SitePackages, self).__getattribute__(item)\n+        except AttributeError:\n+            return getattr(self.path, item)\n+\n+\n class EnvError(Exception):\n \n     pass\n@@ -825,9 +946,13 @@ def pip_version(self):\n         return self._pip_version\n \n     @property\n-    def site_packages(self):  # type: () -> Path\n+    def site_packages(self):  # type: () -> SitePackages\n         if self._site_packages is None:\n-            self._site_packages = self.purelib\n+            # we disable write checks if no user site exist\n+            fallbacks = [self.usersite] if self.usersite else []\n+            self._site_packages = SitePackages(\n+                self.purelib, fallbacks, skip_write_checks=False if fallbacks else True\n+            )\n         return self._site_packages\n \n     @property"
                                },
                                {
                                    "sha": "e6162d508dbfb9461309f85219b98c517d0bf0c7",
                                    "filename": "poetry/utils/helpers.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Futils%2Fhelpers.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/poetry%2Futils%2Fhelpers.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Futils%2Fhelpers.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -5,6 +5,7 @@\n import tempfile\n \n from contextlib import contextmanager\n+from typing import List\n from typing import Optional\n \n import requests\n@@ -113,3 +114,7 @@ def get_package_version_display_string(\n         )\n \n     return package.full_pretty_version\n+\n+\n+def paths_csv(paths):  # type: (List[Path]) -> str\n+    return \", \".join('\"{}\"'.format(str(c)) for c in paths)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "d0e2e5a4dcdf08f4e8106e155866a85927b3fc6c",
                                    "filename": "tests/installation/test_pip_installer.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 1,
                                    "changes": 4,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Finstallation%2Ftest_pip_installer.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Finstallation%2Ftest_pip_installer.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Finstallation%2Ftest_pip_installer.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -189,7 +189,9 @@ def test_uninstall_git_package_nspkg_pth_cleanup(mocker, tmp_venv, pool):\n     )\n \n     # we do this here because the virtual env might not be usable if failure case is triggered\n-    pth_file_candidate = tmp_venv.site_packages / \"{}-nspkg.pth\".format(package.name)\n+    pth_file_candidate = tmp_venv.site_packages.path / \"{}-nspkg.pth\".format(\n+        package.name\n+    )\n \n     # in order to reproduce the scenario where the git source is removed prior to proper\n     # clean up of nspkg.pth file, we need to make sure the fixture is copied and not"
                                },
                                {
                                    "sha": "daeff0e77772984a2a2656716bd19ad4b96398df",
                                    "filename": "tests/masonry/builders/test_editable_builder.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 5,
                                    "changes": 10,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -76,14 +76,14 @@ def test_builder_installs_proper_files_for_standard_packages(simple_poetry, tmp_\n     builder.build()\n \n     assert tmp_venv._bin_dir.joinpath(\"foo\").exists()\n-    assert tmp_venv.site_packages.joinpath(\"simple_project.pth\").exists()\n-    assert simple_poetry.file.parent.resolve().as_posix() == tmp_venv.site_packages.joinpath(\n+    assert tmp_venv.site_packages.path.joinpath(\"simple_project.pth\").exists()\n+    assert simple_poetry.file.parent.resolve().as_posix() == tmp_venv.site_packages.path.joinpath(\n         \"simple_project.pth\"\n     ).read_text().strip(\n         os.linesep\n     )\n \n-    dist_info = tmp_venv.site_packages.joinpath(\"simple_project-1.2.3.dist-info\")\n+    dist_info = tmp_venv.site_packages.path.joinpath(\"simple_project-1.2.3.dist-info\")\n     assert dist_info.exists()\n     assert dist_info.joinpath(\"INSTALLER\").exists()\n     assert dist_info.joinpath(\"METADATA\").exists()\n@@ -130,7 +130,7 @@ def test_builder_installs_proper_files_for_standard_packages(simple_poetry, tmp_\n     assert metadata == dist_info.joinpath(\"METADATA\").read_text(encoding=\"utf-8\")\n \n     records = dist_info.joinpath(\"RECORD\").read_text()\n-    assert str(tmp_venv.site_packages.joinpath(\"simple_project.pth\")) in records\n+    assert str(tmp_venv.site_packages.path.joinpath(\"simple_project.pth\")) in records\n     assert str(tmp_venv._bin_dir.joinpath(\"foo\")) in records\n     assert str(tmp_venv._bin_dir.joinpath(\"baz\")) in records\n     assert str(dist_info.joinpath(\"METADATA\")) in records\n@@ -202,7 +202,7 @@ def test_builder_installs_proper_files_when_packages_configured(\n     builder = EditableBuilder(project_with_include, tmp_venv, NullIO())\n     builder.build()\n \n-    pth_file = tmp_venv.site_packages.joinpath(\"with_include.pth\")\n+    pth_file = tmp_venv.site_packages.path.joinpath(\"with_include.pth\")\n     assert pth_file.is_file()\n \n     paths = set()"
                                },
                                {
                                    "sha": "8934ae43427780108ef9dac8ee8e2221b08798c0",
                                    "filename": "tests/utils/test_env.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Futils%2Ftest_env.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Futils%2Ftest_env.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_env.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -866,7 +866,7 @@ def test_system_env_has_correct_paths():\n     assert paths.get(\"purelib\") is not None\n     assert paths.get(\"platlib\") is not None\n     assert paths.get(\"scripts\") is not None\n-    assert env.site_packages == Path(paths[\"purelib\"])\n+    assert env.site_packages.path == Path(paths[\"purelib\"])\n \n \n @pytest.mark.parametrize(\n@@ -886,4 +886,4 @@ def test_venv_has_correct_paths(tmp_venv):\n     assert paths.get(\"purelib\") is not None\n     assert paths.get(\"platlib\") is not None\n     assert paths.get(\"scripts\") is not None\n-    assert tmp_venv.site_packages == Path(paths[\"purelib\"])\n+    assert tmp_venv.site_packages.path == Path(paths[\"purelib\"])"
                                },
                                {
                                    "sha": "a13089160e652f1d6f60379d4f1e838c6ca291ca",
                                    "filename": "tests/utils/test_env_site.py",
                                    "status": "added",
                                    "additions": 41,
                                    "deletions": 0,
                                    "changes": 41,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Futils%2Ftest_env_site.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/4c81bcc60c99963fc67cea41883913a3d3a4cc65/tests%2Futils%2Ftest_env_site.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Futils%2Ftest_env_site.py?ref=4c81bcc60c99963fc67cea41883913a3d3a4cc65",
                                    "patch": "@@ -0,0 +1,41 @@\n+import uuid\n+\n+from poetry.utils._compat import Path\n+from poetry.utils._compat import decode\n+from poetry.utils.env import SitePackages\n+\n+\n+def test_env_site_simple(tmp_dir):\n+    site_packages = SitePackages(Path(\"/non-existent\"), fallbacks=[Path(tmp_dir)])\n+    candidates = site_packages.make_candidates(Path(\"hello.txt\"), writable_only=True)\n+    hello = Path(tmp_dir) / \"hello.txt\"\n+\n+    assert len(candidates) == 1\n+    assert candidates[0].as_posix() == hello.as_posix()\n+\n+    content = decode(str(uuid.uuid4()))\n+    site_packages.write_text(Path(\"hello.txt\"), content, encoding=\"utf-8\")\n+\n+    assert hello.read_text(encoding=\"utf-8\") == content\n+\n+    assert not (site_packages.path / \"hello.txt\").exists()\n+\n+\n+def test_env_site_select_first(tmp_dir):\n+    path = Path(tmp_dir)\n+    fallback = path / \"fallback\"\n+    fallback.mkdir(parents=True)\n+\n+    site_packages = SitePackages(path, fallbacks=[fallback])\n+    candidates = site_packages.make_candidates(Path(\"hello.txt\"), writable_only=True)\n+\n+    assert len(candidates) == 2\n+    assert len(site_packages.find(Path(\"hello.txt\"))) == 0\n+\n+    content = decode(str(uuid.uuid4()))\n+    site_packages.write_text(Path(\"hello.txt\"), content, encoding=\"utf-8\")\n+\n+    assert (site_packages.path / \"hello.txt\").exists()\n+    assert not (fallback / \"hello.txt\").exists()\n+\n+    assert len(site_packages.find(Path(\"hello.txt\"))) == 1"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/installation/test_pip_installer.py\npoetry run pytest tests/masonry/builders/test_editable_builder.py\npoetry run pytest tests/utils/test_env.py\npoetry run pytest tests/utils/test_env_site.py"
            },
            {
                "id": 3077,
                "created_at": "2020-10-05T06:47:35Z",
                "closed_at": "2020-10-23T21:22:13Z",
                "title": "Poetry Never Updates pywin32",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "68f2cc7032d738981a4af52fdff159df95e55a12",
                        "commit_date": "2020-10-23T21:22:09Z",
                        "parents": "5d0c7965a9c1afacd96faeacf722ae361983af36",
                        "stat": {
                            "total": 2,
                            "additions": 46,
                            "deletions": 44,
                            "files": [
                                {
                                    "sha": "1320fdd66983979c71e6e116c4de3b7cd4c19c97",
                                    "filename": "poetry/repositories/installed_repository.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 2,
                                    "changes": 12,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/68f2cc7032d738981a4af52fdff159df95e55a12/poetry%2Frepositories%2Finstalled_repository.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/68f2cc7032d738981a4af52fdff159df95e55a12/poetry%2Frepositories%2Finstalled_repository.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Frepositories%2Finstalled_repository.py?ref=68f2cc7032d738981a4af52fdff159df95e55a12",
                                    "patch": "@@ -138,14 +138,22 @@ def load(cls, env):  # type: (Env) -> InstalledRepository\n                     if path.name.endswith(\".dist-info\"):\n                         paths = cls.get_package_paths(env=env, name=package.pretty_name)\n                         if paths:\n+                            is_editable_package = False\n                             for src in paths:\n                                 if cls.is_vcs_package(src, env):\n                                     cls.set_package_vcs_properties(package, env)\n                                     break\n+\n+                                if not (\n+                                    is_editable_package\n+                                    or env.is_path_relative_to_lib(src)\n+                                ):\n+                                    is_editable_package = True\n                             else:\n                                 # TODO: handle multiple source directories?\n-                                package._source_type = \"directory\"\n-                                package._source_url = paths.pop().as_posix()\n+                                if is_editable_package:\n+                                    package._source_type = \"directory\"\n+                                    package._source_url = paths.pop().as_posix()\n                     continue\n \n                 if cls.is_vcs_package(path, env):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "245121d496d2808ec3ad2dccfe6f0113eeab0bdc",
                                    "filename": "tests/repositories/fixtures/installed/lib/python3.7/site-packages/standard-1.2.3.dist-info/METADATA",
                                    "status": "added",
                                    "additions": 22,
                                    "deletions": 0,
                                    "changes": 22,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard-1.2.3.dist-info%2FMETADATA",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard-1.2.3.dist-info%2FMETADATA",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard-1.2.3.dist-info%2FMETADATA?ref=68f2cc7032d738981a4af52fdff159df95e55a12",
                                    "patch": "@@ -0,0 +1,22 @@\n+Metadata-Version: 2.1\n+Name: standard\n+Version: 1.2.3\n+Summary: Standard description.\n+License: MIT\n+Keywords: cli,commands\n+Author: Foo Bar\n+Author-email: foo@bar.com\n+Requires-Python: >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*\n+Classifier: License :: OSI Approved :: MIT License\n+Classifier: Programming Language :: Python :: 2\n+Classifier: Programming Language :: Python :: 2.7\n+Classifier: Programming Language :: Python :: 3\n+Classifier: Programming Language :: Python :: 3.4\n+Classifier: Programming Language :: Python :: 3.5\n+Classifier: Programming Language :: Python :: 3.6\n+Classifier: Programming Language :: Python :: 3.7\n+Classifier: Programming Language :: Python :: 3.8\n+Description-Content-Type: text/x-rst\n+\n+Editable\n+####"
                                },
                                {
                                    "sha": "aa0bc074b621cdf27d027d734eaf87a10c65f346",
                                    "filename": "tests/repositories/fixtures/installed/lib/python3.7/site-packages/standard.pth",
                                    "status": "added",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard.pth",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard.pth",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Frepositories%2Ffixtures%2Finstalled%2Flib%2Fpython3.7%2Fsite-packages%2Fstandard.pth?ref=68f2cc7032d738981a4af52fdff159df95e55a12",
                                    "patch": "@@ -0,0 +1 @@\n+standard\n\\ No newline at end of file"
                                },
                                {
                                    "sha": "3caa702a09cccf03b6b2a1d79e87a793abcd6acf",
                                    "filename": "tests/repositories/test_installed_repository.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ftest_installed_repository.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/68f2cc7032d738981a4af52fdff159df95e55a12/tests%2Frepositories%2Ftest_installed_repository.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Frepositories%2Ftest_installed_repository.py?ref=68f2cc7032d738981a4af52fdff159df95e55a12",
                                    "patch": "@@ -26,6 +26,7 @@\n         zipp.Path(str(SITE_PURELIB / \"foo-0.1.0-py3.8.egg\"), \"EGG-INFO\")\n     ),\n     metadata.PathDistribution(VENDOR_DIR / \"attrs-19.3.0.dist-info\"),\n+    metadata.PathDistribution(SITE_PURELIB / \"standard-1.2.3.dist-info\"),\n     metadata.PathDistribution(SITE_PURELIB / \"editable-2.3.4.dist-info\"),\n     metadata.PathDistribution(SITE_PURELIB / \"editable-with-import-2.3.4.dist-info\"),\n     metadata.PathDistribution(SITE_PLATLIB / \"lib64-2.3.4.dist-info\"),\n@@ -158,3 +159,13 @@ def test_load_editable_with_import_package(repository):\n     assert editable.version.text == \"2.3.4\"\n     assert editable.source_type is None\n     assert editable.source_url is None\n+\n+\n+def test_load_standard_package_with_pth_file(repository):\n+    # test standard packages with .pth file is not treated as editable\n+    standard = get_package_from_repository(\"standard\", repository)\n+    assert standard is not None\n+    assert standard.name == \"standard\"\n+    assert standard.version.text == \"1.2.3\"\n+    assert standard.source_type is None\n+    assert standard.source_url is None"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/repositories/test_installed_repository.py"
            },
            {
                "id": 2657,
                "created_at": "2020-07-12T20:07:20Z",
                "closed_at": "2020-07-23T22:53:01Z",
                "title": "Project package in 'src/' folder is not added to python path after install",
                "labels": "kind/bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401",
                        "commit_date": "2020-07-21T12:21:06Z",
                        "parents": "d5a99f004cc3f6b2e82f9b40949f4d24ca4eb3e3",
                        "stat": {
                            "total": 4,
                            "additions": 52,
                            "deletions": 48,
                            "files": [
                                {
                                    "sha": "21a5d049f084dd90df713264e4315609ed59912a",
                                    "filename": "poetry/masonry/builders/editable.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 1,
                                    "changes": 12,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401/poetry%2Fmasonry%2Fbuilders%2Feditable.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401/poetry%2Fmasonry%2Fbuilders%2Feditable.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/poetry%2Fmasonry%2Fbuilders%2Feditable.py?ref=c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401",
                                    "patch": "@@ -8,6 +8,7 @@\n \n from poetry.core.masonry.builders.builder import Builder\n from poetry.core.masonry.builders.sdist import SdistBuilder\n+from poetry.core.masonry.utils.package_include import PackageInclude\n from poetry.core.semver.version import Version\n from poetry.utils._compat import WINDOWS\n from poetry.utils._compat import Path\n@@ -91,8 +92,17 @@ def _add_pth(self):\n                 pth.name, self._env.site_packages, self._poetry.file.parent\n             )\n         )\n+\n+        paths = set()\n+        for include in self._module.includes:\n+            if isinstance(include, PackageInclude) and (\n+                include.is_module() or include.is_package()\n+            ):\n+                paths.add(include.base.resolve().as_posix())\n+\n         with pth.open(\"w\", encoding=\"utf-8\") as f:\n-            f.write(decode(str(self._poetry.file.parent.resolve())))\n+            for path in paths:\n+                f.write(decode(path + os.linesep))\n \n         return [pth]\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "5489da5744d35f7d83df29ae60dfa852c973ce7d",
                                    "filename": "tests/masonry/builders/test_editable_builder.py",
                                    "status": "modified",
                                    "additions": 37,
                                    "deletions": 3,
                                    "changes": 40,
                                    "blob_url": "https://github.com/python-poetry/poetry/blob/c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py",
                                    "raw_url": "https://github.com/python-poetry/poetry/raw/c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py",
                                    "contents_url": "https://api.github.com/repos/python-poetry/poetry/contents/tests%2Fmasonry%2Fbuilders%2Ftest_editable_builder.py?ref=c264bc0ffd26cfcf9a4610dc7cd5f07e1f7a0401",
                                    "patch": "@@ -1,6 +1,7 @@\n # -*- coding: utf-8 -*-\n from __future__ import unicode_literals\n \n+import os\n import shutil\n \n import pytest\n@@ -23,6 +24,15 @@ def simple_poetry():\n     return poetry\n \n \n+@pytest.fixture()\n+def project_with_include():\n+    poetry = Factory().create_poetry(\n+        Path(__file__).parent.parent.parent / \"fixtures\" / \"with-include\"\n+    )\n+\n+    return poetry\n+\n+\n @pytest.fixture()\n def extended_poetry():\n     poetry = Factory().create_poetry(\n@@ -56,9 +66,10 @@ def test_builder_installs_proper_files_for_standard_packages(simple_poetry, tmp_\n \n     assert tmp_venv._bin_dir.joinpath(\"foo\").exists()\n     assert tmp_venv.site_packages.joinpath(\"simple_project.pth\").exists()\n-    assert (\n-        str(simple_poetry.file.parent.resolve())\n-        == tmp_venv.site_packages.joinpath(\"simple_project.pth\").read_text()\n+    assert simple_poetry.file.parent.resolve().as_posix() == tmp_venv.site_packages.joinpath(\n+        \"simple_project.pth\"\n+    ).read_text().strip(\n+        os.linesep\n     )\n \n     dist_info = tmp_venv.site_packages.joinpath(\"simple_project-1.2.3.dist-info\")\n@@ -159,3 +170,26 @@ def test_builder_falls_back_on_setup_and_pip_for_packages_with_build_scripts(\n             \"--no-deps\",\n         ]\n     ] == env.executed\n+\n+\n+def test_builder_installs_proper_files_when_packages_configured(\n+    project_with_include, tmp_venv\n+):\n+    builder = EditableBuilder(project_with_include, tmp_venv, NullIO())\n+    builder.build()\n+\n+    pth_file = tmp_venv.site_packages.joinpath(\"with_include.pth\")\n+    assert pth_file.is_file()\n+\n+    paths = set()\n+    with pth_file.open() as f:\n+        for line in f.readlines():\n+            line = line.strip(os.linesep)\n+            if line:\n+                paths.add(line)\n+\n+    project_root = project_with_include.file.parent.resolve()\n+    expected = {project_root.as_posix(), project_root.joinpath(\"src\").as_posix()}\n+\n+    assert paths.issubset(expected)\n+    assert len(paths) == len(expected)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "poetry run pytest tests/masonry/builders/test_editable_builder.py"
            }
        ],
        "installSteps": "pipx install poetry==1.2.0 --force\npoetry env use python3.8\npoetry install"
    },
    {
        "_id": "64b990571d0358d36f0c0727",
        "username": "freqtrade",
        "repository": "freqtrade",
        "issues": [
            {
                "id": 8758,
                "created_at": "2023-06-11T06:21:20Z",
                "closed_at": "2023-06-11T17:38:03Z",
                "title": "Change logic how stoploss_on_exchange orders are build",
                "labels": "Bug,Stoploss",
                "text_based": false,
                "commits": [
                    {
                        "hash": "5844756ba15714e4d1a0bdfed064a85d54220d42",
                        "commit_date": "2023-06-11T15:20:35Z",
                        "parents": "4a800fe46700f62f35b2f4876f1e04c9b34b4acb",
                        "stat": {
                            "total": 3,
                            "additions": 14,
                            "deletions": 11,
                            "files": [
                                {
                                    "sha": "ef3bea53728798862b5f437506e1a222ad713f50",
                                    "filename": "freqtrade/exchange/exchange.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/5844756ba15714e4d1a0bdfed064a85d54220d42/freqtrade%2Fexchange%2Fexchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/5844756ba15714e4d1a0bdfed064a85d54220d42/freqtrade%2Fexchange%2Fexchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fexchange.py?ref=5844756ba15714e4d1a0bdfed064a85d54220d42",
                                    "patch": "@@ -1148,8 +1148,8 @@ def _get_stop_limit_rate(self, stop_price: float, order_types: Dict, side: str)\n         else:\n             limit_rate = stop_price * (2 - limit_price_pct)\n \n-        bad_stop_price = ((stop_price <= limit_rate) if side ==\n-                          \"sell\" else (stop_price >= limit_rate))\n+        bad_stop_price = ((stop_price < limit_rate) if side ==\n+                          \"sell\" else (stop_price > limit_rate))\n         # Ensure rate is less than stop price\n         if bad_stop_price:\n             # This can for example happen if the stop / liquidation price is set to 0"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "5fa2755d2182a814b4fce4c4780238faa74b4498",
                                    "filename": "tests/exchange/test_exchange.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 1,
                                    "changes": 10,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/5844756ba15714e4d1a0bdfed064a85d54220d42/tests%2Fexchange%2Ftest_exchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/5844756ba15714e4d1a0bdfed064a85d54220d42/tests%2Fexchange%2Ftest_exchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_exchange.py?ref=5844756ba15714e4d1a0bdfed064a85d54220d42",
                                    "patch": "@@ -3492,14 +3492,22 @@ def test_stoploss_order_unsupported_exchange(default_conf, mocker):\n @pytest.mark.parametrize('side,ratio,expected', [\n     ('sell', 0.99, 99.0),  # Default\n     ('sell', 0.999, 99.9),\n+    ('sell', 1, 100),\n+    ('sell', 1.1, InvalidOrderException),\n     ('buy', 0.99, 101.0),  # Default\n     ('buy', 0.999, 100.1),\n+    ('buy', 1, 100),\n+    ('buy', 1.1, InvalidOrderException),\n     ])\n def test__get_stop_limit_rate(default_conf_usdt, mocker, side, ratio, expected):\n     exchange = get_patched_exchange(mocker, default_conf_usdt, id='binance')\n \n     order_types = {'stoploss_on_exchange_limit_ratio': ratio}\n-    assert exchange._get_stop_limit_rate(100, order_types, side) == expected\n+    if isinstance(expected, type) and issubclass(expected, Exception):\n+        with pytest.raises(expected):\n+            exchange._get_stop_limit_rate(100, order_types, side)\n+    else:\n+        assert exchange._get_stop_limit_rate(100, order_types, side) == expected\n \n \n def test_merge_ft_has_dict(default_conf, mocker):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/exchange/test_exchange.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 8755,
                "created_at": "2023-06-09T17:35:47Z",
                "closed_at": "2023-06-10T14:56:54Z",
                "title": "OKX: Create order fails while adjusting position due to inability to change leverage while open orders are active",
                "labels": "Bug,Non-spot",
                "text_based": false,
                "commits": [
                    {
                        "hash": "cfe88f06d2b990d7df7f8a1364b36ce126d6cb78",
                        "commit_date": "2023-06-10T14:56:41Z",
                        "parents": "4f15b30339445bf57842eb54d82b990340d1001d",
                        "stat": {
                            "total": 2,
                            "additions": 25,
                            "deletions": 23,
                            "files": [
                                {
                                    "sha": "8ad3c2cdb35803bb916c638078ab39a870a71685",
                                    "filename": "freqtrade/exchange/okx.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 2,
                                    "changes": 21,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/cfe88f06d2b990d7df7f8a1364b36ce126d6cb78/freqtrade%2Fexchange%2Fokx.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/cfe88f06d2b990d7df7f8a1364b36ce126d6cb78/freqtrade%2Fexchange%2Fokx.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fokx.py?ref=cfe88f06d2b990d7df7f8a1364b36ce126d6cb78",
                                    "patch": "@@ -125,6 +125,20 @@ def _get_params(\n             params['posSide'] = self._get_posSide(side, reduceOnly)\n         return params\n \n+    def __fetch_leverage_already_set(self, pair: str, leverage: float, side: BuySell) -> bool:\n+        try:\n+            res_lev = self._api.fetch_leverage(symbol=pair, params={\n+                    \"mgnMode\": self.margin_mode.value,\n+                    \"posSide\": self._get_posSide(side, False),\n+                })\n+            self._log_exchange_response('get_leverage', res_lev)\n+            already_set = all(float(x['lever']) == leverage for x in res_lev['data'])\n+            return already_set\n+\n+        except ccxt.BaseError:\n+            # Assume all errors as \"not set yet\"\n+            return False\n+\n     @retrier\n     def _lev_prep(self, pair: str, leverage: float, side: BuySell, accept_fail: bool = False):\n         if self.trading_mode != TradingMode.SPOT and self.margin_mode is not None:\n@@ -141,8 +155,11 @@ def _lev_prep(self, pair: str, leverage: float, side: BuySell, accept_fail: bool\n             except ccxt.DDoSProtection as e:\n                 raise DDosProtection(e) from e\n             except (ccxt.NetworkError, ccxt.ExchangeError) as e:\n-                raise TemporaryError(\n-                    f'Could not set leverage due to {e.__class__.__name__}. Message: {e}') from e\n+                already_set = self.__fetch_leverage_already_set(pair, leverage, side)\n+                if not already_set:\n+                    raise TemporaryError(\n+                        f'Could not set leverage due to {e.__class__.__name__}. Message: {e}'\n+                        ) from e\n             except ccxt.BaseError as e:\n                 raise OperationalException(e) from e\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "378466ae4f5bbcf647f25de82a3a2b53a28aae81",
                                    "filename": "tests/exchange/test_okx.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/cfe88f06d2b990d7df7f8a1364b36ce126d6cb78/tests%2Fexchange%2Ftest_okx.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/cfe88f06d2b990d7df7f8a1364b36ce126d6cb78/tests%2Fexchange%2Ftest_okx.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_okx.py?ref=cfe88f06d2b990d7df7f8a1364b36ce126d6cb78",
                                    "patch": "@@ -499,7 +499,11 @@ def test__set_leverage_okx(mocker, default_conf):\n     assert api_mock.set_leverage.call_args_list[0][1]['params'] == {\n         'mgnMode': 'isolated',\n         'posSide': 'net'}\n+    api_mock.set_leverage = MagicMock(side_effect=ccxt.NetworkError())\n+    exchange._lev_prep('BTC/USDT:USDT', 3.2, 'buy')\n+    api_mock.fetch_leverage.call_count == 1\n \n+    api_mock.fetch_leverage = MagicMock(side_effect=ccxt.NetworkError())\n     ccxt_exceptionhandlers(\n         mocker,\n         default_conf,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/exchange/test_okx.py",
                "testStepsFull": "pipenv run  pytest"
            },
            {
                "id": 8714,
                "created_at": "2023-05-31T05:28:50Z",
                "closed_at": "2023-05-31T09:50:12Z",
                "title": "reload markets frequently",
                "labels": "Bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "1f543666f427d664098e39a4605df5174fbd2345",
                        "commit_date": "2023-05-31T09:46:31Z",
                        "parents": "b666c418bbb7d9903ca20676f12f39574753062a",
                        "stat": {
                            "total": 5,
                            "additions": 18,
                            "deletions": 13,
                            "files": [
                                {
                                    "sha": "88022e19ceec3a2c8ef4a9e08780852a965d80a0",
                                    "filename": "freqtrade/exchange/exchange.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/1f543666f427d664098e39a4605df5174fbd2345/freqtrade%2Fexchange%2Fexchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/1f543666f427d664098e39a4605df5174fbd2345/freqtrade%2Fexchange%2Fexchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fexchange.py?ref=1f543666f427d664098e39a4605df5174fbd2345",
                                    "patch": "@@ -191,7 +191,7 @@ def __init__(self, config: Config, *, exchange_config: Optional[ExchangeConfig]\n \n         # Converts the interval provided in minutes in config to seconds\n         self.markets_refresh_interval: int = exchange_conf.get(\n-            \"markets_refresh_interval\", 60) * 60\n+            \"markets_refresh_interval\", 60) * 60 * 1000\n \n         if self.trading_mode != TradingMode.SPOT and load_leverage_tiers:\n             self.fill_leverage_tiers()"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "f022a0905d881d7d71c3478bb58bd15802bc3847",
                                    "filename": "tests/exchange/test_exchange.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 4,
                                    "changes": 16,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/1f543666f427d664098e39a4605df5174fbd2345/tests%2Fexchange%2Ftest_exchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/1f543666f427d664098e39a4605df5174fbd2345/tests%2Fexchange%2Ftest_exchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_exchange.py?ref=1f543666f427d664098e39a4605df5174fbd2345",
                                    "patch": "@@ -633,34 +633,42 @@ def test__load_markets(default_conf, mocker, caplog):\n     assert ex.markets == expected_return\n \n \n-def test_reload_markets(default_conf, mocker, caplog):\n+def test_reload_markets(default_conf, mocker, caplog, time_machine):\n     caplog.set_level(logging.DEBUG)\n     initial_markets = {'ETH/BTC': {}}\n     updated_markets = {'ETH/BTC': {}, \"LTC/BTC\": {}}\n-\n+    start_dt = dt_now()\n+    time_machine.move_to(start_dt, tick=False)\n     api_mock = MagicMock()\n     api_mock.load_markets = MagicMock(return_value=initial_markets)\n     default_conf['exchange']['markets_refresh_interval'] = 10\n     exchange = get_patched_exchange(mocker, default_conf, api_mock, id=\"binance\",\n                                     mock_markets=False)\n     exchange._load_async_markets = MagicMock()\n-    exchange._last_markets_refresh = dt_ts()\n+    assert exchange._last_markets_refresh == dt_ts()\n \n     assert exchange.markets == initial_markets\n \n+    time_machine.move_to(start_dt + timedelta(minutes=8), tick=False)\n     # less than 10 minutes have passed, no reload\n     exchange.reload_markets()\n     assert exchange.markets == initial_markets\n     assert exchange._load_async_markets.call_count == 0\n \n     api_mock.load_markets = MagicMock(return_value=updated_markets)\n     # more than 10 minutes have passed, reload is executed\n-    exchange._last_markets_refresh = dt_ts(dt_now() - timedelta(minutes=15))\n+    time_machine.move_to(start_dt + timedelta(minutes=11), tick=False)\n     exchange.reload_markets()\n     assert exchange.markets == updated_markets\n     assert exchange._load_async_markets.call_count == 1\n     assert log_has('Performing scheduled market reload..', caplog)\n \n+    # Not called again\n+    exchange._load_async_markets.reset_mock()\n+\n+    exchange.reload_markets()\n+    assert exchange._load_async_markets.call_count == 0\n+\n \n def test_reload_markets_exception(default_conf, mocker, caplog):\n     caplog.set_level(logging.DEBUG)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run pytest -- tests/exchange/test_exchange.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 8562,
                "created_at": "2023-04-27T12:29:10Z",
                "closed_at": "2023-04-27T16:27:35Z",
                "title": "webhook entry doesnt work ",
                "labels": "Bug,RPC",
                "text_based": false,
                "commits": [
                    {
                        "hash": "daf564b62fb012633e457fbc1448455b1cf48716",
                        "commit_date": "2023-04-27T16:27:09Z",
                        "parents": "1d9933412a45b892a4a3bfde30cead83bce242b6",
                        "stat": {
                            "total": 14,
                            "additions": 33,
                            "deletions": 19,
                            "files": [
                                {
                                    "sha": "80690ec0cfbaa7105ed03fef8a1319141b4f1755",
                                    "filename": "freqtrade/rpc/webhook.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 4,
                                    "changes": 8,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/daf564b62fb012633e457fbc1448455b1cf48716/freqtrade%2Frpc%2Fwebhook.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/daf564b62fb012633e457fbc1448455b1cf48716/freqtrade%2Frpc%2Fwebhook.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Frpc%2Fwebhook.py?ref=daf564b62fb012633e457fbc1448455b1cf48716",
                                    "patch": "@@ -44,8 +44,11 @@ def cleanup(self) -> None:\n \n     def _get_value_dict(self, msg: RPCSendMsg) -> Optional[Dict[str, Any]]:\n         whconfig = self._config['webhook']\n+        if msg['type'].value in whconfig:\n+            # Explicit types should have priority\n+            valuedict = whconfig.get(msg['type'].value)\n         # Deprecated 2022.10 - only keep generic method.\n-        if msg['type'] in [RPCMessageType.ENTRY]:\n+        elif msg['type'] in [RPCMessageType.ENTRY]:\n             valuedict = whconfig.get('webhookentry')\n         elif msg['type'] in [RPCMessageType.ENTRY_CANCEL]:\n             valuedict = whconfig.get('webhookentrycancel')\n@@ -62,9 +65,6 @@ def _get_value_dict(self, msg: RPCSendMsg) -> Optional[Dict[str, Any]]:\n                              RPCMessageType.EXCEPTION,\n                              RPCMessageType.WARNING):\n             valuedict = whconfig.get('webhookstatus')\n-        elif msg['type'].value in whconfig:\n-            # Allow all types ...\n-            valuedict = whconfig.get(msg['type'].value)\n         elif msg['type'] in (\n                 RPCMessageType.PROTECTION_TRIGGER,\n                 RPCMessageType.PROTECTION_TRIGGER_GLOBAL,"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "d0a0f5b1ec0f6b380f25c64ef219e39beca17426",
                                    "filename": "tests/rpc/test_rpc_webhook.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 10,
                                    "changes": 25,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/daf564b62fb012633e457fbc1448455b1cf48716/tests%2Frpc%2Ftest_rpc_webhook.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/daf564b62fb012633e457fbc1448455b1cf48716/tests%2Frpc%2Ftest_rpc_webhook.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Frpc%2Ftest_rpc_webhook.py?ref=daf564b62fb012633e457fbc1448455b1cf48716",
                                    "patch": "@@ -17,6 +17,10 @@ def get_webhook_dict() -> dict:\n         \"enabled\": True,\n         \"url\": \"https://maker.ifttt.com/trigger/freqtrade_test/with/key/c764udvJ5jfSlswVRukZZ2/\",\n         \"webhookentry\": {\n+            # Intentionally broken, as \"entry\" should have priority.\n+            \"value1\": \"Buying {pair55555}\",\n+        },\n+        \"entry\": {\n             \"value1\": \"Buying {pair}\",\n             \"value2\": \"limit {limit:8f}\",\n             \"value3\": \"{stake_amount:8f} {stake_currency}\",\n@@ -89,15 +93,15 @@ def test_send_msg_webhook(default_conf, mocker):\n     webhook.send_msg(msg=msg)\n     assert msg_mock.call_count == 1\n     assert (msg_mock.call_args[0][0][\"value1\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value1\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value1\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value2\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value2\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value2\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value3\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value3\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value3\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value4\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value4\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value4\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value5\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value5\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value5\"].format(**msg))\n     # Test short\n     msg_mock.reset_mock()\n \n@@ -116,15 +120,15 @@ def test_send_msg_webhook(default_conf, mocker):\n     webhook.send_msg(msg=msg)\n     assert msg_mock.call_count == 1\n     assert (msg_mock.call_args[0][0][\"value1\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value1\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value1\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value2\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value2\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value2\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value3\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value3\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value3\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value4\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value4\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value4\"].format(**msg))\n     assert (msg_mock.call_args[0][0][\"value5\"] ==\n-            default_conf[\"webhook\"][\"webhookentry\"][\"value5\"].format(**msg))\n+            default_conf[\"webhook\"][\"entry\"][\"value5\"].format(**msg))\n     # Test buy cancel\n     msg_mock.reset_mock()\n \n@@ -328,6 +332,7 @@ def test_send_msg_webhook(default_conf, mocker):\n \n def test_exception_send_msg(default_conf, mocker, caplog):\n     default_conf[\"webhook\"] = get_webhook_dict()\n+    del default_conf[\"webhook\"][\"entry\"]\n     del default_conf[\"webhook\"][\"webhookentry\"]\n \n     webhook = Webhook(RPC(get_patched_freqtradebot(mocker, default_conf)), default_conf)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/rpc/test_rpc_webhook.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 8300,
                "created_at": "2023-03-09T22:40:38Z",
                "closed_at": "2023-03-22T18:49:31Z",
                "title": "In stoploss limit order, stop price should be more than limit price",
                "labels": "Bug,Stoploss",
                "text_based": false,
                "commits": [
                    {
                        "hash": "150c5510c74390641fd723f6a70b5f67f53ec9cf",
                        "commit_date": "2023-03-22T18:46:07Z",
                        "parents": "8cf3e9f91b50fd7d615822e87a51aa997548dd59",
                        "stat": {
                            "total": 14,
                            "additions": 36,
                            "deletions": 22,
                            "files": [
                                {
                                    "sha": "104eaa221e28fbfe8ad00783fb3b74ab9fbe9595",
                                    "filename": "freqtrade/exchange/exchange.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/150c5510c74390641fd723f6a70b5f67f53ec9cf/freqtrade%2Fexchange%2Fexchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/150c5510c74390641fd723f6a70b5f67f53ec9cf/freqtrade%2Fexchange%2Fexchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fexchange.py?ref=150c5510c74390641fd723f6a70b5f67f53ec9cf",
                                    "patch": "@@ -1135,7 +1135,11 @@ def _get_stop_limit_rate(self, stop_price: float, order_types: Dict, side: str)\n                           \"sell\" else (stop_price >= limit_rate))\n         # Ensure rate is less than stop price\n         if bad_stop_price:\n-            raise OperationalException(\n+            # This can for example happen if the stop / liquidation price is set to 0\n+            # Which is possible if a market-order closes right away.\n+            # The InvalidOrderException will bubble up to exit_positions, where it will be\n+            # handled gracefully.\n+            raise InvalidOrderException(\n                 \"In stoploss limit order, stop price should be more than limit price. \"\n                 f\"Stop price: {stop_price}, Limit price: {limit_rate}, \"\n                 f\"Limit Price pct: {limit_price_pct}\""
                                },
                                {
                                    "sha": "00bfc1ee22d7de997c78973372dcc6e6211c2bbc",
                                    "filename": "freqtrade/freqtradebot.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 5,
                                    "changes": 14,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/150c5510c74390641fd723f6a70b5f67f53ec9cf/freqtrade%2Ffreqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/150c5510c74390641fd723f6a70b5f67f53ec9cf/freqtrade%2Ffreqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Ffreqtradebot.py?ref=150c5510c74390641fd723f6a70b5f67f53ec9cf",
                                    "patch": "@@ -1021,12 +1021,16 @@ def exit_positions(self, trades: List[Trade]) -> int:\n         trades_closed = 0\n         for trade in trades:\n             try:\n+                try:\n+                    if (self.strategy.order_types.get('stoploss_on_exchange') and\n+                            self.handle_stoploss_on_exchange(trade)):\n+                        trades_closed += 1\n+                        Trade.commit()\n+                        continue\n \n-                if (self.strategy.order_types.get('stoploss_on_exchange') and\n-                        self.handle_stoploss_on_exchange(trade)):\n-                    trades_closed += 1\n-                    Trade.commit()\n-                    continue\n+                except InvalidOrderException as exception:\n+                    logger.warning(\n+                        f'Unable to handle stoploss on exchange for {trade.pair}: {exception}')\n                 # Check if we can sell our current pair\n                 if trade.open_order_id is None and trade.is_open and self.handle_trade(trade):\n                     trades_closed += 1"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "273860e158f376dc39fc3a3b4c1cd7abb9dec246",
                                    "filename": "tests/exchange/test_binance.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_binance.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_binance.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_binance.py?ref=150c5510c74390641fd723f6a70b5f67f53ec9cf",
                                    "patch": "@@ -52,7 +52,7 @@ def test_create_stoploss_order_binance(default_conf, mocker, limitratio, expecte\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'binance')\n \n-    with pytest.raises(OperationalException):\n+    with pytest.raises(InvalidOrderException):\n         order = exchange.create_stoploss(\n             pair='ETH/BTC',\n             amount=1,\n@@ -131,7 +131,7 @@ def test_create_stoploss_order_dry_run_binance(default_conf, mocker):\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'binance')\n \n-    with pytest.raises(OperationalException):\n+    with pytest.raises(InvalidOrderException):\n         order = exchange.create_stoploss(\n             pair='ETH/BTC',\n             amount=1,"
                                },
                                {
                                    "sha": "85d2ced9d4e0d5143e99094b07c60efda1d444fc",
                                    "filename": "tests/exchange/test_huobi.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_huobi.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_huobi.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_huobi.py?ref=150c5510c74390641fd723f6a70b5f67f53ec9cf",
                                    "patch": "@@ -4,7 +4,7 @@\n import ccxt\n import pytest\n \n-from freqtrade.exceptions import DependencyException, InvalidOrderException, OperationalException\n+from freqtrade.exceptions import DependencyException, InvalidOrderException\n from tests.conftest import EXMS, get_patched_exchange\n from tests.exchange.test_exchange import ccxt_exceptionhandlers\n \n@@ -31,7 +31,7 @@ def test_create_stoploss_order_huobi(default_conf, mocker, limitratio, expected,\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'huobi')\n \n-    with pytest.raises(OperationalException):\n+    with pytest.raises(InvalidOrderException):\n         order = exchange.create_stoploss(pair='ETH/BTC', amount=1, stop_price=190,\n                                          order_types={'stoploss_on_exchange_limit_ratio': 1.05},\n                                          side=side,\n@@ -84,7 +84,7 @@ def test_create_stoploss_order_dry_run_huobi(default_conf, mocker):\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'huobi')\n \n-    with pytest.raises(OperationalException):\n+    with pytest.raises(InvalidOrderException):\n         order = exchange.create_stoploss(pair='ETH/BTC', amount=1, stop_price=190,\n                                          order_types={'stoploss_on_exchange_limit_ratio': 1.05},\n                                          side='sell', leverage=1.0)"
                                },
                                {
                                    "sha": "07f3fb6a36119386c6437ce8833b52ef1f4b576e",
                                    "filename": "tests/exchange/test_kucoin.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 3,
                                    "changes": 6,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_kucoin.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/150c5510c74390641fd723f6a70b5f67f53ec9cf/tests%2Fexchange%2Ftest_kucoin.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_kucoin.py?ref=150c5510c74390641fd723f6a70b5f67f53ec9cf",
                                    "patch": "@@ -4,7 +4,7 @@\n import ccxt\n import pytest\n \n-from freqtrade.exceptions import DependencyException, InvalidOrderException, OperationalException\n+from freqtrade.exceptions import DependencyException, InvalidOrderException\n from tests.conftest import EXMS, get_patched_exchange\n from tests.exchange.test_exchange import ccxt_exceptionhandlers\n \n@@ -31,7 +31,7 @@ def test_create_stoploss_order_kucoin(default_conf, mocker, limitratio, expected\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'kucoin')\n     if order_type == 'limit':\n-        with pytest.raises(OperationalException):\n+        with pytest.raises(InvalidOrderException):\n             order = exchange.create_stoploss(pair='ETH/BTC', amount=1, stop_price=190,\n                                              order_types={\n                                                  'stoploss': order_type,\n@@ -92,7 +92,7 @@ def test_stoploss_order_dry_run_kucoin(default_conf, mocker):\n \n     exchange = get_patched_exchange(mocker, default_conf, api_mock, 'kucoin')\n \n-    with pytest.raises(OperationalException):\n+    with pytest.raises(InvalidOrderException):\n         order = exchange.create_stoploss(pair='ETH/BTC', amount=1, stop_price=190,\n                                          order_types={'stoploss': 'limit',\n                                                       'stoploss_on_exchange_limit_ratio': 1.05},"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/exchange/test_kucoin.py\npipenv run pytest -- tests/exchange/test_huobi.py\npipenv run pytest -- tests/exchange/test_binance.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 8229,
                "created_at": "2023-02-26T10:38:32Z",
                "closed_at": "2023-02-26T15:17:45Z",
                "title": "Order {trade.open_order_id} for {trade.pair} not cancelled, as the filled amount of {filled_val} would result in an unexitable trade.",
                "labels": "Bug,unsupported exchange",
                "text_based": false,
                "commits": [
                    {
                        "hash": "5b0bc5bbc54d7122f677a8ff9b615238e7940622",
                        "commit_date": "2023-02-26T15:17:41Z",
                        "parents": "6f7ab97fc363daa90a9c1767498a80192f69edef",
                        "stat": {
                            "total": 7,
                            "additions": 16,
                            "deletions": 9,
                            "files": [
                                {
                                    "sha": "20e558513d0a78cc558a0b478f7fbf78a833fdfc",
                                    "filename": "freqtrade/exchange/kucoin.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 2,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/5b0bc5bbc54d7122f677a8ff9b615238e7940622/freqtrade%2Fexchange%2Fkucoin.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/5b0bc5bbc54d7122f677a8ff9b615238e7940622/freqtrade%2Fexchange%2Fkucoin.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fkucoin.py?ref=5b0bc5bbc54d7122f677a8ff9b615238e7940622",
                                    "patch": "@@ -64,6 +64,7 @@ def create_order(\n         # ccxt returns status = 'closed' at the moment - which is information ccxt invented.\n         # Since we rely on status heavily, we must set it to 'open' here.\n         # ref: https://github.com/ccxt/ccxt/pull/16674, (https://github.com/ccxt/ccxt/pull/16553)\n-        res['type'] = ordertype\n-        res['status'] = 'open'\n+        if not self._config['dry_run']:\n+            res['type'] = ordertype\n+            res['status'] = 'open'\n         return res"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7ccd32155a12284a123c0b075ca6cf55766a52fb",
                                    "filename": "tests/exchange/test_exchange.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 5,
                                    "changes": 11,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/5b0bc5bbc54d7122f677a8ff9b615238e7940622/tests%2Fexchange%2Ftest_exchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/5b0bc5bbc54d7122f677a8ff9b615238e7940622/tests%2Fexchange%2Ftest_exchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_exchange.py?ref=5b0bc5bbc54d7122f677a8ff9b615238e7940622",
                                    "patch": "@@ -27,7 +27,7 @@\n \n \n # Make sure to always keep one exchange here which is NOT subclassed!!\n-EXCHANGES = ['bittrex', 'binance', 'kraken', 'gate', 'bybit']\n+EXCHANGES = ['bittrex', 'binance', 'kraken', 'gate', 'kucoin', 'bybit']\n \n get_entry_rate_data = [\n     ('other', 20, 19, 10, 0.0, 20),  # Full ask side\n@@ -1269,7 +1269,7 @@ def test_create_dry_run_order_limit_fill(default_conf, mocker, side, price, fill\n                           fetch_l2_order_book=order_book_l2_usd,\n                           )\n \n-    order = exchange.create_dry_run_order(\n+    order = exchange.create_order(\n         pair='LTC/USDT',\n         ordertype='limit',\n         side=side,\n@@ -1332,7 +1332,7 @@ def test_create_dry_run_order_market_fill(default_conf, mocker, side, rate, amou\n                           fetch_l2_order_book=order_book_l2_usd,\n                           )\n \n-    order = exchange.create_dry_run_order(\n+    order = exchange.create_order(\n         pair='LTC/USDT',\n         ordertype='market',\n         side=side,\n@@ -1425,9 +1425,10 @@ def test_create_order(default_conf, mocker, side, ordertype, rate, marketprice,\n     assert order['amount'] == 0.01\n \n \n-def test_buy_dry_run(default_conf, mocker):\n+@pytest.mark.parametrize(\"exchange_name\", EXCHANGES)\n+def test_buy_dry_run(default_conf, mocker, exchange_name):\n     default_conf['dry_run'] = True\n-    exchange = get_patched_exchange(mocker, default_conf)\n+    exchange = get_patched_exchange(mocker, default_conf, id=exchange_name)\n \n     order = exchange.create_order(pair='ETH/BTC', ordertype='limit', side=\"buy\",\n                                   amount=1, rate=200, leverage=1.0,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8 \nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/exchange/test_exchange.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 8082,
                "created_at": "2023-01-31T09:11:08Z",
                "closed_at": "2023-01-31T12:26:52Z",
                "title": "Consumer/Producer - Error on producer side when calling dataframe.iloc",
                "labels": "Bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "1dc3c58775daa2593b6dcb920858556971380b68",
                        "commit_date": "2023-01-31T11:04:56Z",
                        "parents": "410324ac1926563f58f9de7fe229b9751ba2d90e",
                        "stat": {
                            "total": 5,
                            "additions": 17,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "60a0589524716da0e6591906f7efa15f0f587b9d",
                                    "filename": "freqtrade/data/dataprovider.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 5,
                                    "changes": 12,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/1dc3c58775daa2593b6dcb920858556971380b68/freqtrade%2Fdata%2Fdataprovider.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/1dc3c58775daa2593b6dcb920858556971380b68/freqtrade%2Fdata%2Fdataprovider.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fdata%2Fdataprovider.py?ref=1dc3c58775daa2593b6dcb920858556971380b68",
                                    "patch": "@@ -9,7 +9,7 @@\n from datetime import datetime, timezone\n from typing import Any, Dict, List, Optional, Tuple\n \n-from pandas import DataFrame, to_timedelta\n+from pandas import DataFrame, Timedelta, Timestamp, to_timedelta\n \n from freqtrade.configuration import TimeRange\n from freqtrade.constants import (FULL_DATAFRAME_THRESHOLD, Config, ListPairsWithTimeframes,\n@@ -206,9 +206,11 @@ def _add_external_df(\n         existing_df, _ = self.__producer_pairs_df[producer_name][pair_key]\n \n         # CHECK FOR MISSING CANDLES\n-        timeframe_delta = to_timedelta(timeframe)  # Convert the timeframe to a timedelta for pandas\n-        local_last = existing_df.iloc[-1]['date']  # We want the last date from our copy\n-        incoming_first = dataframe.iloc[0]['date']  # We want the first date from the incoming\n+        # Convert the timeframe to a timedelta for pandas\n+        timeframe_delta: Timedelta = to_timedelta(timeframe)\n+        local_last: Timestamp = existing_df.iloc[-1]['date']  # We want the last date from our copy\n+        # We want the first date from the incoming\n+        incoming_first: Timestamp = dataframe.iloc[0]['date']\n \n         # Remove existing candles that are newer than the incoming first candle\n         existing_df1 = existing_df[existing_df['date'] < incoming_first]\n@@ -221,7 +223,7 @@ def _add_external_df(\n         # we missed some candles between our data and the incoming\n         # so return False and candle_difference.\n         if candle_difference > 1:\n-            return (False, candle_difference)\n+            return (False, int(candle_difference))\n         if existing_df1.empty:\n             appended_df = dataframe\n         else:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c6b1dcc5a8b99a53f09b263d947f026c73431426",
                                    "filename": "tests/data/test_dataprovider.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/1dc3c58775daa2593b6dcb920858556971380b68/tests%2Fdata%2Ftest_dataprovider.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/1dc3c58775daa2593b6dcb920858556971380b68/tests%2Fdata%2Ftest_dataprovider.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fdata%2Ftest_dataprovider.py?ref=1dc3c58775daa2593b6dcb920858556971380b68",
                                    "patch": "@@ -437,6 +437,7 @@ def test_dp__add_external_df(default_conf_usdt):\n     # Add the same dataframe again - dataframe size shall not change.\n     res = dp._add_external_df('ETH/USDT', df, last_analyzed, timeframe, CandleType.SPOT)\n     assert res[0] is True\n+    assert isinstance(res[1], int)\n     assert res[1] == 0\n     df, _ = dp.get_producer_df('ETH/USDT', timeframe, CandleType.SPOT)\n     assert len(df) == 24\n@@ -446,6 +447,7 @@ def test_dp__add_external_df(default_conf_usdt):\n \n     res = dp._add_external_df('ETH/USDT', df2, last_analyzed, timeframe, CandleType.SPOT)\n     assert res[0] is True\n+    assert isinstance(res[1], int)\n     assert res[1] == 0\n     df, _ = dp.get_producer_df('ETH/USDT', timeframe, CandleType.SPOT)\n     assert len(df) == 48\n@@ -455,6 +457,7 @@ def test_dp__add_external_df(default_conf_usdt):\n \n     res = dp._add_external_df('ETH/USDT', df3, last_analyzed, timeframe, CandleType.SPOT)\n     assert res[0] is True\n+    assert isinstance(res[1], int)\n     assert res[1] == 0\n     df, _ = dp.get_producer_df('ETH/USDT', timeframe, CandleType.SPOT)\n     # New length = 48 + 12 (since we have a 12 hour offset).\n@@ -478,6 +481,7 @@ def test_dp__add_external_df(default_conf_usdt):\n     res = dp._add_external_df('ETH/USDT', df4, last_analyzed, timeframe, CandleType.SPOT)\n     assert res[0] is False\n     # 36 hours - from 2022-01-03 12:00:00+00:00 to 2022-01-05 00:00:00+00:00\n+    assert isinstance(res[1], int)\n     assert res[1] == 36\n     df, _ = dp.get_producer_df('ETH/USDT', timeframe, CandleType.SPOT)\n     # New length = 61 + 1\n@@ -488,4 +492,5 @@ def test_dp__add_external_df(default_conf_usdt):\n     res = dp._add_external_df('ETH/USDT', df4, last_analyzed, timeframe, CandleType.SPOT)\n     assert res[0] is False\n     # 36 hours - from 2022-01-03 12:00:00+00:00 to 2022-01-05 00:00:00+00:00\n+    assert isinstance(res[1], int)\n     assert res[1] == 0"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8 \nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/data/test_dataprovider.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7766,
                "created_at": "2022-11-19T19:29:23Z",
                "closed_at": "2022-11-25T15:13:55Z",
                "title": "ERROR - Could not cancel stoploss order",
                "labels": "Bug,Stoploss",
                "text_based": false,
                "commits": [
                    {
                        "hash": "0f97ef0d7bb0616e1e8bbfe679d4af047b97325b",
                        "commit_date": "2022-11-25T15:08:33Z",
                        "parents": "1b3e62bcbc0f6a06ecb2b8871be7f8880ef3b82d",
                        "stat": {
                            "total": 2,
                            "additions": 6,
                            "deletions": 4,
                            "files": [
                                {
                                    "sha": "34d18b3d8703665fc5bdbfdc554844002ae69689",
                                    "filename": "freqtrade/freqtradebot.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 2,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/0f97ef0d7bb0616e1e8bbfe679d4af047b97325b/freqtrade%2Ffreqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/0f97ef0d7bb0616e1e8bbfe679d4af047b97325b/freqtrade%2Ffreqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Ffreqtradebot.py?ref=0f97ef0d7bb0616e1e8bbfe679d4af047b97325b",
                                    "patch": "@@ -827,6 +827,8 @@ def cancel_stoploss_on_exchange(self, trade: Trade) -> Trade:\n                 co = self.exchange.cancel_stoploss_order_with_result(\n                     trade.stoploss_order_id, trade.pair, trade.amount)\n                 trade.update_order(co)\n+                # Reset stoploss order id.\n+                trade.stoploss_order_id = None\n             except InvalidOrderException:\n                 logger.exception(f\"Could not cancel stoploss order {trade.stoploss_order_id}\")\n         return trade\n@@ -1011,7 +1013,7 @@ def exit_positions(self, trades: List[Trade]) -> int:\n \n     def handle_trade(self, trade: Trade) -> bool:\n         \"\"\"\n-        Sells/exits_short the current pair if the threshold is reached and updates the trade record.\n+        Exits the current pair if the threshold is reached and updates the trade record.\n         :return: True if trade has been sold/exited_short, False otherwise\n         \"\"\"\n         if not trade.is_open:\n@@ -1168,7 +1170,6 @@ def handle_stoploss_on_exchange(self, trade: Trade) -> bool:\n             if self.create_stoploss_order(trade=trade, stop_price=trade.stoploss_or_liquidation):\n                 return False\n             else:\n-                trade.stoploss_order_id = None\n                 logger.warning('Stoploss order was cancelled, but unable to recreate one.')\n \n         # Finally we check if stoploss on exchange should be moved up because of trailing."
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b71b5b387bebc1091ead97f9c36890d44da84c6f",
                                    "filename": "tests/test_freqtradebot.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/0f97ef0d7bb0616e1e8bbfe679d4af047b97325b/tests%2Ftest_freqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/0f97ef0d7bb0616e1e8bbfe679d4af047b97325b/tests%2Ftest_freqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Ftest_freqtradebot.py?ref=0f97ef0d7bb0616e1e8bbfe679d4af047b97325b",
                                    "patch": "@@ -1498,6 +1498,7 @@ def test_handle_stoploss_on_exchange_trailing(\n         })\n     )\n     assert freqtrade.handle_trade(trade) is True\n+    assert trade.stoploss_order_id is None\n \n \n @pytest.mark.parametrize(\"is_short\", [False, True])"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/test_freqtradebot.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7765,
                "created_at": "2022-11-19T08:50:54Z",
                "closed_at": "2022-11-20T08:31:42Z",
                "title": "when staking multiple unfilled entries, only the first entry gets autocancelled after entry timeout, rest doesn't get autocancelled",
                "labels": "Bug,RPC",
                "text_based": false,
                "commits": [
                    {
                        "hash": "12b471c64b7d87a888224ad9de034e604b10fd4f",
                        "commit_date": "2022-11-20T08:28:14Z",
                        "parents": "4de9a4661852e1d65e4754a5e57836ff5911c582",
                        "stat": {
                            "total": 0,
                            "additions": 8,
                            "deletions": 8,
                            "files": [
                                {
                                    "sha": "1d3f36844e9e561c41b457fadf68374c0e1cdacd",
                                    "filename": "freqtrade/rpc/rpc.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/12b471c64b7d87a888224ad9de034e604b10fd4f/freqtrade%2Frpc%2Frpc.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/12b471c64b7d87a888224ad9de034e604b10fd4f/freqtrade%2Frpc%2Frpc.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Frpc%2Frpc.py?ref=12b471c64b7d87a888224ad9de034e604b10fd4f",
                                    "patch": "@@ -774,6 +774,9 @@ def _rpc_force_entry(self, pair: str, price: Optional[float], *,\n             is_short = trade.is_short\n             if not self._freqtrade.strategy.position_adjustment_enable:\n                 raise RPCException(f'position for {pair} already open - id: {trade.id}')\n+            if trade.open_order_id is not None:\n+                raise RPCException(f'position for {pair} already open - id: {trade.id} '\n+                                   f'and has open order {trade.open_order_id}')\n         else:\n             if Trade.get_open_trade_count() >= self._config['max_open_trades']:\n                 raise RPCException(\"Maximum number of trades is reached.\")"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "8828b6f337135de5a6aac84acff27b96958bc88a",
                                    "filename": "tests/rpc/test_rpc.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/12b471c64b7d87a888224ad9de034e604b10fd4f/tests%2Frpc%2Ftest_rpc.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/12b471c64b7d87a888224ad9de034e604b10fd4f/tests%2Frpc%2Ftest_rpc.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Frpc%2Ftest_rpc.py?ref=12b471c64b7d87a888224ad9de034e604b10fd4f",
                                    "patch": "@@ -1066,6 +1066,11 @@ def test_rpc_force_entry(mocker, default_conf, ticker, fee, limit_buy_order_open\n     trade = rpc._rpc_force_entry(pair, 0.0001, order_type='limit', stake_amount=0.05)\n     assert trade.stake_amount == 0.05\n     assert trade.buy_tag == 'force_entry'\n+    assert trade.open_order_id == 'mocked_limit_buy'\n+\n+    freqtradebot.strategy.position_adjustment_enable = True\n+    with pytest.raises(RPCException, match=r'position for LTC/BTC already open.*open order.*'):\n+        rpc._rpc_force_entry(pair, 0.0001, order_type='limit', stake_amount=0.05)\n \n     # Test not buying\n     pair = 'XRP/BTC'"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/rpc/test_rpc.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7489,
                "created_at": "2022-09-27T13:26:06Z",
                "closed_at": "2022-09-28T10:07:26Z",
                "title": "max_open_trades Does not work when we use forceenter",
                "labels": "Bug,RPC",
                "text_based": false,
                "commits": [
                    {
                        "hash": "fb3d4083384dceff02ff65fca64021664c66a0dc",
                        "commit_date": "2022-09-28T09:32:07Z",
                        "parents": "e9abe3cb68d00b09a795d64e704930f5600aef91",
                        "stat": {
                            "total": 0,
                            "additions": 8,
                            "deletions": 8,
                            "files": [
                                {
                                    "sha": "143b11911ab9bb96319ee1ebbadd199fadd76c37",
                                    "filename": "freqtrade/rpc/rpc.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 0,
                                    "changes": 3,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/fb3d4083384dceff02ff65fca64021664c66a0dc/freqtrade%2Frpc%2Frpc.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/fb3d4083384dceff02ff65fca64021664c66a0dc/freqtrade%2Frpc%2Frpc.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Frpc%2Frpc.py?ref=fb3d4083384dceff02ff65fca64021664c66a0dc",
                                    "patch": "@@ -773,6 +773,9 @@ def _rpc_force_entry(self, pair: str, price: Optional[float], *,\n             is_short = trade.is_short\n             if not self._freqtrade.strategy.position_adjustment_enable:\n                 raise RPCException(f'position for {pair} already open - id: {trade.id}')\n+        else:\n+            if Trade.get_open_trade_count() >= self._config['max_open_trades']:\n+                raise RPCException(\"Maximum number of trades is reached.\")\n \n         if not stake_amount:\n             # gen stake amount"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "54a4cbe9ad82f6f4b270318fb21b9f4e3736d6b3",
                                    "filename": "tests/rpc/test_rpc.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/fb3d4083384dceff02ff65fca64021664c66a0dc/tests%2Frpc%2Ftest_rpc.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/fb3d4083384dceff02ff65fca64021664c66a0dc/tests%2Frpc%2Ftest_rpc.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Frpc%2Ftest_rpc.py?ref=fb3d4083384dceff02ff65fca64021664c66a0dc",
                                    "patch": "@@ -1030,6 +1030,7 @@ def test_rpc_count(mocker, default_conf, ticker, fee) -> None:\n \n def test_rpc_force_entry(mocker, default_conf, ticker, fee, limit_buy_order_open) -> None:\n     default_conf['force_entry_enable'] = True\n+    default_conf['max_open_trades'] = 0\n     mocker.patch('freqtrade.rpc.telegram.Telegram', MagicMock())\n     buy_mm = MagicMock(return_value=limit_buy_order_open)\n     mocker.patch.multiple(\n@@ -1044,6 +1045,10 @@ def test_rpc_force_entry(mocker, default_conf, ticker, fee, limit_buy_order_open\n     patch_get_signal(freqtradebot)\n     rpc = RPC(freqtradebot)\n     pair = 'ETH/BTC'\n+    with pytest.raises(RPCException, match='Maximum number of trades is reached.'):\n+        rpc._rpc_force_entry(pair, None)\n+    freqtradebot.config['max_open_trades'] = 5\n+\n     trade = rpc._rpc_force_entry(pair, None)\n     assert isinstance(trade, Trade)\n     assert trade.pair == pair"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.9\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\nsed -i 's/ccxt==1.93.98/ccxt==1.93.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/rpc/test_rpc.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7447,
                "created_at": "2022-09-20T18:07:28Z",
                "closed_at": "2022-09-21T04:54:06Z",
                "title": "Range Stability filter: Pairs under the minimum range are accepted when setting both minimum and maximum ranges ",
                "labels": "Bug,Pairlist",
                "text_based": false,
                "commits": [
                    {
                        "hash": "02f2096fc35d12859c8921c51d2857168d23ee05",
                        "commit_date": "2022-09-21T04:53:07Z",
                        "parents": "8f41f943b44dbda75e38546ee5c7a5d0ab88d553",
                        "stat": {
                            "total": 7,
                            "additions": 14,
                            "deletions": 7,
                            "files": [
                                {
                                    "sha": "1bc7ad48f32d370597482ab1efcc86248fe49f83",
                                    "filename": "freqtrade/plugins/pairlist/rangestabilityfilter.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 7,
                                    "changes": 10,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/02f2096fc35d12859c8921c51d2857168d23ee05/freqtrade%2Fplugins%2Fpairlist%2Frangestabilityfilter.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/02f2096fc35d12859c8921c51d2857168d23ee05/freqtrade%2Fplugins%2Fpairlist%2Frangestabilityfilter.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fplugins%2Fpairlist%2Frangestabilityfilter.py?ref=02f2096fc35d12859c8921c51d2857168d23ee05",
                                    "patch": "@@ -100,23 +100,19 @@ def _validate_pair_loc(self, pair: str, daily_candles: Optional[DataFrame]) -> b\n         if cached_res is not None:\n             return cached_res\n \n-        result = False\n+        result = True\n         if daily_candles is not None and not daily_candles.empty:\n             highest_high = daily_candles['high'].max()\n             lowest_low = daily_candles['low'].min()\n             pct_change = ((highest_high - lowest_low) / lowest_low) if lowest_low > 0 else 0\n-            if pct_change >= self._min_rate_of_change:\n-                result = True\n-            else:\n+            if pct_change < self._min_rate_of_change:\n                 self.log_once(f\"Removed {pair} from whitelist, because rate of change \"\n                               f\"over {self._days} {plural(self._days, 'day')} is {pct_change:.3f}, \"\n                               f\"which is below the threshold of {self._min_rate_of_change}.\",\n                               logger.info)\n                 result = False\n             if self._max_rate_of_change:\n-                if pct_change <= self._max_rate_of_change:\n-                    result = True\n-                else:\n+                if pct_change > self._max_rate_of_change:\n                     self.log_once(\n                         f\"Removed {pair} from whitelist, because rate of change \"\n                         f\"over {self._days} {plural(self._days, 'day')} is {pct_change:.3f}, \""
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "53875125177aff0cb753071ba0487ae315f99b7a",
                                    "filename": "tests/plugins/test_pairlist.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/02f2096fc35d12859c8921c51d2857168d23ee05/tests%2Fplugins%2Ftest_pairlist.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/02f2096fc35d12859c8921c51d2857168d23ee05/tests%2Fplugins%2Ftest_pairlist.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fplugins%2Ftest_pairlist.py?ref=02f2096fc35d12859c8921c51d2857168d23ee05",
                                    "patch": "@@ -467,6 +467,10 @@ def test_VolumePairList_refresh_empty(mocker, markets_empty, whitelist_conf):\n       {\"method\": \"RangeStabilityFilter\", \"lookback_days\": 10,\n        \"max_rate_of_change\": 0.01, \"refresh_period\": 1440}],\n      \"BTC\", []),  # All removed because of max_rate_of_change being 0.017\n+    ([{\"method\": \"StaticPairList\"},\n+      {\"method\": \"RangeStabilityFilter\", \"lookback_days\": 10,\n+        \"min_rate_of_change\": 0.018, \"max_rate_of_change\": 0.02, \"refresh_period\": 1440}],\n+     \"BTC\", []),  # All removed - limits are above the highest change_rate\n     ([{\"method\": \"StaticPairList\"},\n       {\"method\": \"VolatilityFilter\", \"lookback_days\": 3,\n        \"min_volatility\": 0.002, \"max_volatility\": 0.004, \"refresh_period\": 1440}],"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.9 \nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\nsed -i 's/ccxt==1.93.66/ccxt==1.93.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/plugins/test_pairlist.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7424,
                "created_at": "2022-09-16T10:45:09Z",
                "closed_at": "2022-09-17T15:11:13Z",
                "title": "custom_stoploss is not updated at next \"stoploss_on_exchange_interval\"",
                "labels": "Bug,Stoploss",
                "text_based": false,
                "commits": [
                    {
                        "hash": "063511826c3c4ed87e1247552e7a676cc0db4afe",
                        "commit_date": "2022-09-17T15:11:00Z",
                        "parents": "9f266cbcb2232e7d3fc9219caaef4ea21aa85c48",
                        "stat": {
                            "total": 8,
                            "additions": 25,
                            "deletions": 17,
                            "files": [
                                {
                                    "sha": "3eaec5c98bc1f52bcceab3efeb08425496eafc2a",
                                    "filename": "freqtrade/freqtradebot.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 3,
                                    "changes": 7,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/063511826c3c4ed87e1247552e7a676cc0db4afe/freqtrade%2Ffreqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/063511826c3c4ed87e1247552e7a676cc0db4afe/freqtrade%2Ffreqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Ffreqtradebot.py?ref=063511826c3c4ed87e1247552e7a676cc0db4afe",
                                    "patch": "@@ -1072,6 +1072,7 @@ def create_stoploss_order(self, trade: Trade, stop_price: float) -> bool:\n             order_obj = Order.parse_from_ccxt_object(stoploss_order, trade.pair, 'stoploss')\n             trade.orders.append(order_obj)\n             trade.stoploss_order_id = str(stoploss_order['id'])\n+            trade.stoploss_last_update = datetime.now(timezone.utc)\n             return True\n         except InsufficientFundsError as e:\n             logger.warning(f\"Unable to place stoploss order {e}.\")\n@@ -1145,10 +1146,9 @@ def handle_stoploss_on_exchange(self, trade: Trade) -> bool:\n             if self.create_stoploss_order(trade=trade, stop_price=stop_price):\n                 # The above will return False if the placement failed and the trade was force-sold.\n                 # in which case the trade will be closed - which we must check below.\n-                trade.stoploss_last_update = datetime.utcnow()\n                 return False\n \n-        # If stoploss order is canceled for some reason we add it\n+        # If stoploss order is canceled for some reason we add it again\n         if (trade.is_open\n                 and stoploss_order\n                 and stoploss_order['status'] in ('canceled', 'cancelled')):\n@@ -1186,7 +1186,8 @@ def handle_trailing_stoploss_on_exchange(self, trade: Trade, order: Dict) -> Non\n         if self.exchange.stoploss_adjust(stoploss_norm, order, side=trade.exit_side):\n             # we check if the update is necessary\n             update_beat = self.strategy.order_types.get('stoploss_on_exchange_interval', 60)\n-            if (datetime.utcnow() - trade.stoploss_last_update).total_seconds() >= update_beat:\n+            upd_req = datetime.now(timezone.utc) - timedelta(seconds=update_beat)\n+            if trade.stoploss_last_update_utc and upd_req >= trade.stoploss_last_update_utc:\n                 # cancelling the current stoploss on exchange first\n                 logger.info(f\"Cancelling current stoploss on exchange for pair {trade.pair} \"\n                             f\"(orderid:{order['id']}) in order to add another one ...\")"
                                },
                                {
                                    "sha": "6e421f33e9887b328cc1c4fc0092c436df8f897a",
                                    "filename": "freqtrade/persistence/trade_model.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 1,
                                    "changes": 7,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/063511826c3c4ed87e1247552e7a676cc0db4afe/freqtrade%2Fpersistence%2Ftrade_model.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/063511826c3c4ed87e1247552e7a676cc0db4afe/freqtrade%2Fpersistence%2Ftrade_model.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fpersistence%2Ftrade_model.py?ref=063511826c3c4ed87e1247552e7a676cc0db4afe",
                                    "patch": "@@ -376,6 +376,12 @@ def date_last_filled_utc(self) -> datetime:\n     def open_date_utc(self):\n         return self.open_date.replace(tzinfo=timezone.utc)\n \n+    @property\n+    def stoploss_last_update_utc(self):\n+        if self.stoploss_last_update:\n+            return self.stoploss_last_update.replace(tzinfo=timezone.utc)\n+        return None\n+\n     @property\n     def close_date_utc(self):\n         return self.close_date.replace(tzinfo=timezone.utc)\n@@ -560,7 +566,6 @@ def __set_stop_loss(self, stop_loss: float, percent: float):\n         self.stop_loss = stop_loss_norm\n \n         self.stop_loss_pct = -1 * abs(percent)\n-        self.stoploss_last_update = datetime.utcnow()\n \n     def adjust_stop_loss(self, current_price: float, stoploss: float,\n                          initial: bool = False, refresh: bool = False) -> None:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c1152ac090c8fb781dad48fd9a9383b781594201",
                                    "filename": "tests/test_freqtradebot.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 4,
                                    "changes": 11,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/063511826c3c4ed87e1247552e7a676cc0db4afe/tests%2Ftest_freqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/063511826c3c4ed87e1247552e7a676cc0db4afe/tests%2Ftest_freqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Ftest_freqtradebot.py?ref=063511826c3c4ed87e1247552e7a676cc0db4afe",
                                    "patch": "@@ -1427,6 +1427,7 @@ def test_handle_stoploss_on_exchange_trailing(\n     trade.is_open = True\n     trade.open_order_id = None\n     trade.stoploss_order_id = 100\n+    trade.stoploss_last_update = arrow.utcnow().shift(minutes=-20).datetime\n \n     stoploss_order_hanging = MagicMock(return_value={\n         'id': 100,\n@@ -1456,7 +1457,7 @@ def test_handle_stoploss_on_exchange_trailing(\n     )\n \n     cancel_order_mock = MagicMock()\n-    stoploss_order_mock = MagicMock(return_value={'id': 13434334})\n+    stoploss_order_mock = MagicMock(return_value={'id': 'so1'})\n     mocker.patch('freqtrade.exchange.Binance.cancel_stoploss_order', cancel_order_mock)\n     mocker.patch('freqtrade.exchange.Binance.stoploss', stoploss_order_mock)\n \n@@ -1569,6 +1570,7 @@ def test_handle_stoploss_on_exchange_trailing_error(\n     assert stoploss.call_count == 1\n \n     # Fail creating stoploss order\n+    trade.stoploss_last_update = arrow.utcnow().shift(minutes=-601).datetime\n     caplog.clear()\n     cancel_mock = mocker.patch(\"freqtrade.exchange.Binance.cancel_stoploss_order\", MagicMock())\n     mocker.patch(\"freqtrade.exchange.Binance.stoploss\", side_effect=ExchangeError())\n@@ -1657,6 +1659,7 @@ def test_handle_stoploss_on_exchange_custom_stop(\n     trade.is_open = True\n     trade.open_order_id = None\n     trade.stoploss_order_id = 100\n+    trade.stoploss_last_update = arrow.utcnow().shift(minutes=-601).datetime\n \n     stoploss_order_hanging = MagicMock(return_value={\n         'id': 100,\n@@ -1685,7 +1688,7 @@ def test_handle_stoploss_on_exchange_custom_stop(\n     )\n \n     cancel_order_mock = MagicMock()\n-    stoploss_order_mock = MagicMock(return_value={'id': 13434334})\n+    stoploss_order_mock = MagicMock(return_value={'id': 'so1'})\n     mocker.patch('freqtrade.exchange.Binance.cancel_stoploss_order', cancel_order_mock)\n     mocker.patch('freqtrade.exchange.Binance.stoploss', stoploss_order_mock)\n \n@@ -1727,8 +1730,7 @@ def test_handle_stoploss_on_exchange_custom_stop(\n     assert freqtrade.handle_trade(trade) is True\n \n \n-def test_tsl_on_exchange_compatible_with_edge(mocker, edge_conf, fee, caplog,\n-                                              limit_order) -> None:\n+def test_tsl_on_exchange_compatible_with_edge(mocker, edge_conf, fee, limit_order) -> None:\n \n     enter_order = limit_order['buy']\n     exit_order = limit_order['sell']\n@@ -1784,6 +1786,7 @@ def test_tsl_on_exchange_compatible_with_edge(mocker, edge_conf, fee, caplog,\n     trade.is_open = True\n     trade.open_order_id = None\n     trade.stoploss_order_id = 100\n+    trade.stoploss_last_update = arrow.utcnow()\n \n     stoploss_order_hanging = MagicMock(return_value={\n         'id': 100,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\nsed -i 's/ccxt==1.93.35/ccxt==1.93.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/test_freqtradebot.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7368,
                "created_at": "2022-09-07T03:20:34Z",
                "closed_at": "2022-09-07T16:21:14Z",
                "title": "Entry message does not show amount when it is okx.",
                "labels": "Bug,RPC",
                "text_based": false,
                "commits": [
                    {
                        "hash": "322f00e3e8c7c183e350e84e4c740e0d34130c68",
                        "commit_date": "2022-09-07T16:19:43Z",
                        "parents": "c08c82bc40710f198446022be355e34b0af28114",
                        "stat": {
                            "total": 1,
                            "additions": 3,
                            "deletions": 2,
                            "files": [
                                {
                                    "sha": "ec32cae0e71cb483467051037e21075d20dff76b",
                                    "filename": "freqtrade/freqtradebot.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/322f00e3e8c7c183e350e84e4c740e0d34130c68/freqtrade%2Ffreqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/322f00e3e8c7c183e350e84e4c740e0d34130c68/freqtrade%2Ffreqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Ffreqtradebot.py?ref=322f00e3e8c7c183e350e84e4c740e0d34130c68",
                                    "patch": "@@ -919,7 +919,7 @@ def _notify_enter(self, trade: Trade, order: Order, order_type: Optional[str] =\n             'stake_amount': trade.stake_amount,\n             'stake_currency': self.config['stake_currency'],\n             'fiat_currency': self.config.get('fiat_display_currency', None),\n-            'amount': order.safe_amount_after_fee,\n+            'amount': order.safe_amount_after_fee if fill else order.amount,\n             'open_date': trade.open_date or datetime.utcnow(),\n             'current_rate': current_rate,\n             'sub_trade': sub_trade,"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "30b0b75b6631b1ff9b9b2dea4c5175d96571919b",
                                    "filename": "tests/test_freqtradebot.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/322f00e3e8c7c183e350e84e4c740e0d34130c68/tests%2Ftest_freqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/322f00e3e8c7c183e350e84e4c740e0d34130c68/tests%2Ftest_freqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Ftest_freqtradebot.py?ref=322f00e3e8c7c183e350e84e4c740e0d34130c68",
                                    "patch": "@@ -3655,6 +3655,7 @@ def test_may_execute_trade_exit_after_stoploss_on_exchange_hit(\n     assert trade.exit_reason == ExitType.STOPLOSS_ON_EXCHANGE.value\n     assert rpc_mock.call_count == 3\n     assert rpc_mock.call_args_list[0][0][0]['type'] == RPCMessageType.ENTRY\n+    assert rpc_mock.call_args_list[0][0][0]['amount'] > 20\n     assert rpc_mock.call_args_list[1][0][0]['type'] == RPCMessageType.ENTRY_FILL\n     assert rpc_mock.call_args_list[2][0][0]['type'] == RPCMessageType.EXIT_FILL\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run  python -m pip install --upgrade pip\nsed -i 's/ccxt==1.93.3/ccxt==1.93.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/test_freqtradebot.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7294,
                "created_at": "2022-08-25T18:37:24Z",
                "closed_at": "2022-08-26T18:24:19Z",
                "title": "Certain dry-run trades are exiting because of liquidation",
                "labels": "Bug,Non-spot",
                "text_based": false,
                "commits": [
                    {
                        "hash": "01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                        "commit_date": "2022-08-26T18:14:24Z",
                        "parents": "753d1b2aad18eacfafea3f718a66747418657d4f",
                        "stat": {
                            "total": 26,
                            "additions": 66,
                            "deletions": 40,
                            "files": [
                                {
                                    "sha": "a5e9fd37c7990f4fe3196e70884964abe28b6dee",
                                    "filename": "freqtrade/exchange/binance.py",
                                    "status": "modified",
                                    "additions": 12,
                                    "deletions": 9,
                                    "changes": 21,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Fexchange%2Fbinance.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Fexchange%2Fbinance.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fbinance.py?ref=01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                                    "patch": "@@ -137,23 +137,27 @@ def dry_run_liquidation_price(\n         pair: str,\n         open_rate: float,   # Entry price of position\n         is_short: bool,\n-        position: float,  # Absolute value of position size\n+        amount: float,\n+        stake_amount: float,\n         wallet_balance: float,  # Or margin balance\n         mm_ex_1: float = 0.0,  # (Binance) Cross only\n         upnl_ex_1: float = 0.0,  # (Binance) Cross only\n     ) -> Optional[float]:\n         \"\"\"\n+        Important: Must be fetching data from cached values as this is used by backtesting!\n         MARGIN: https://www.binance.com/en/support/faq/f6b010588e55413aa58b7d63ee0125ed\n         PERPETUAL: https://www.binance.com/en/support/faq/b3c689c1f50a44cabb3a84e663b81d93\n \n         :param exchange_name:\n-        :param open_rate: (EP1) Entry price of position\n+        :param open_rate: Entry price of position\n         :param is_short: True if the trade is a short, false otherwise\n-        :param position: Absolute value of position size (in base currency)\n-        :param wallet_balance: (WB)\n+        :param amount: Absolute value of position size incl. leverage (in base currency)\n+        :param stake_amount: Stake amount - Collateral in settle currency.\n+        :param trading_mode: SPOT, MARGIN, FUTURES, etc.\n+        :param margin_mode: Either ISOLATED or CROSS\n+        :param wallet_balance: Amount of margin_mode in the wallet being used to trade\n             Cross-Margin Mode: crossWalletBalance\n             Isolated-Margin Mode: isolatedWalletBalance\n-        :param maintenance_amt:\n \n         # * Only required for Cross\n         :param mm_ex_1: (TMM)\n@@ -165,12 +169,11 @@ def dry_run_liquidation_price(\n         \"\"\"\n \n         side_1 = -1 if is_short else 1\n-        position = abs(position)\n         cross_vars = upnl_ex_1 - mm_ex_1 if self.margin_mode == MarginMode.CROSS else 0.0\n \n         # mm_ratio: Binance's formula specifies maintenance margin rate which is mm_ratio * 100%\n         # maintenance_amt: (CUM) Maintenance Amount of position\n-        mm_ratio, maintenance_amt = self.get_maintenance_ratio_and_amt(pair, position)\n+        mm_ratio, maintenance_amt = self.get_maintenance_ratio_and_amt(pair, stake_amount)\n \n         if (maintenance_amt is None):\n             raise OperationalException(\n@@ -182,9 +185,9 @@ def dry_run_liquidation_price(\n             return (\n                 (\n                     (wallet_balance + cross_vars + maintenance_amt) -\n-                    (side_1 * position * open_rate)\n+                    (side_1 * amount * open_rate)\n                 ) / (\n-                    (position * mm_ratio) - (side_1 * position)\n+                    (amount * mm_ratio) - (side_1 * amount)\n                 )\n             )\n         else:"
                                },
                                {
                                    "sha": "4386f47f627dae21c68a5b8373c1079ae4cf43b6",
                                    "filename": "freqtrade/exchange/exchange.py",
                                    "status": "modified",
                                    "additions": 15,
                                    "deletions": 11,
                                    "changes": 26,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Fexchange%2Fexchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Fexchange%2Fexchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fexchange%2Fexchange.py?ref=01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                                    "patch": "@@ -2437,6 +2437,7 @@ def get_liquidation_price(\n             pair: str,\n             open_rate: float,\n             amount: float,  # quote currency, includes leverage\n+            stake_amount: float,\n             leverage: float,\n             is_short: bool\n     ) -> Optional[float]:\n@@ -2446,13 +2447,13 @@ def get_liquidation_price(\n         elif (\n             self.trading_mode == TradingMode.FUTURES\n         ):\n-            wallet_balance = (amount * open_rate) / leverage\n             isolated_liq = self.get_or_calculate_liquidation_price(\n                 pair=pair,\n                 open_rate=open_rate,\n                 is_short=is_short,\n-                position=amount,\n-                wallet_balance=wallet_balance,\n+                amount=amount,\n+                stake_amount=stake_amount,\n+                wallet_balance=stake_amount,  # In isolated mode, stake-amount = wallet size\n                 mm_ex_1=0.0,\n                 upnl_ex_1=0.0,\n             )\n@@ -2627,14 +2628,14 @@ def get_or_calculate_liquidation_price(\n         # Dry-run\n         open_rate: float,   # Entry price of position\n         is_short: bool,\n-        position: float,  # Absolute value of position size\n+        amount: float,  # Absolute value of position size\n+        stake_amount: float,\n         wallet_balance: float,  # Or margin balance\n         mm_ex_1: float = 0.0,  # (Binance) Cross only\n         upnl_ex_1: float = 0.0,  # (Binance) Cross only\n     ) -> Optional[float]:\n         \"\"\"\n         Set's the margin mode on the exchange to cross or isolated for a specific pair\n-        :param pair: base/quote currency pair (e.g. \"ADA/USDT\")\n         \"\"\"\n         if self.trading_mode == TradingMode.SPOT:\n             return None\n@@ -2648,7 +2649,8 @@ def get_or_calculate_liquidation_price(\n                 pair=pair,\n                 open_rate=open_rate,\n                 is_short=is_short,\n-                position=position,\n+                amount=amount,\n+                stake_amount=stake_amount,\n                 wallet_balance=wallet_balance,\n                 mm_ex_1=mm_ex_1,\n                 upnl_ex_1=upnl_ex_1\n@@ -2677,22 +2679,24 @@ def dry_run_liquidation_price(\n         pair: str,\n         open_rate: float,   # Entry price of position\n         is_short: bool,\n-        position: float,  # Absolute value of position size\n+        amount: float,\n+        stake_amount: float,\n         wallet_balance: float,  # Or margin balance\n         mm_ex_1: float = 0.0,  # (Binance) Cross only\n         upnl_ex_1: float = 0.0,  # (Binance) Cross only\n     ) -> Optional[float]:\n         \"\"\"\n+        Important: Must be fetching data from cached values as this is used by backtesting!\n         PERPETUAL:\n          gateio: https://www.gate.io/help/futures/perpetual/22160/calculation-of-liquidation-price\n          okex: https://www.okex.com/support/hc/en-us/articles/\n             360053909592-VI-Introduction-to-the-isolated-mode-of-Single-Multi-currency-Portfolio-margin\n-        Important: Must be fetching data from cached values as this is used by backtesting!\n \n         :param exchange_name:\n         :param open_rate: Entry price of position\n         :param is_short: True if the trade is a short, false otherwise\n-        :param position: Absolute value of position size incl. leverage (in base currency)\n+        :param amount: Absolute value of position size incl. leverage (in base currency)\n+        :param stake_amount: Stake amount - Collateral in settle currency.\n         :param trading_mode: SPOT, MARGIN, FUTURES, etc.\n         :param margin_mode: Either ISOLATED or CROSS\n         :param wallet_balance: Amount of margin_mode in the wallet being used to trade\n@@ -2706,15 +2710,15 @@ def dry_run_liquidation_price(\n \n         market = self.markets[pair]\n         taker_fee_rate = market['taker']\n-        mm_ratio, _ = self.get_maintenance_ratio_and_amt(pair, position)\n+        mm_ratio, _ = self.get_maintenance_ratio_and_amt(pair, stake_amount)\n \n         if self.trading_mode == TradingMode.FUTURES and self.margin_mode == MarginMode.ISOLATED:\n \n             if market['inverse']:\n                 raise OperationalException(\n                     \"Freqtrade does not yet support inverse contracts\")\n \n-            value = wallet_balance / position\n+            value = wallet_balance / amount\n \n             mm_ratio_taker = (mm_ratio + taker_fee_rate)\n             if is_short:"
                                },
                                {
                                    "sha": "f4731220c9eaab188eb3cb2db7be3b682acc50c7",
                                    "filename": "freqtrade/freqtradebot.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Ffreqtradebot.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Ffreqtradebot.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Ffreqtradebot.py?ref=01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                                    "patch": "@@ -1734,6 +1734,7 @@ def update_trade_state(self, trade: Trade, order_id: str, action_order: Dict[str\n                     leverage=trade.leverage,\n                     pair=trade.pair,\n                     amount=trade.amount,\n+                    stake_amount=trade.stake_amount,\n                     open_rate=trade.open_rate,\n                     is_short=trade.is_short\n                 ))"
                                },
                                {
                                    "sha": "ff30dbc2a71b1dfa49597249d81d6c0db1de0705",
                                    "filename": "freqtrade/optimize/backtesting.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Foptimize%2Fbacktesting.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/freqtrade%2Foptimize%2Fbacktesting.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Foptimize%2Fbacktesting.py?ref=01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                                    "patch": "@@ -876,6 +876,7 @@ def _enter_trade(self, pair: str, row: Tuple, direction: LongShort,\n                 pair=pair,\n                 open_rate=propose_rate,\n                 amount=amount,\n+                stake_amount=trade.stake_amount,\n                 leverage=leverage,\n                 is_short=is_short,\n             ))"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "09328466863e19459beba9e8c35f27b401e7852b",
                                    "filename": "tests/exchange/test_exchange.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 6,
                                    "changes": 17,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/tests%2Fexchange%2Ftest_exchange.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/01126c43f78bf932c5dbeab7e5517cbab4fe52d6/tests%2Fexchange%2Ftest_exchange.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fexchange%2Ftest_exchange.py?ref=01126c43f78bf932c5dbeab7e5517cbab4fe52d6",
                                    "patch": "@@ -4132,7 +4132,8 @@ def test_get_or_calculate_liquidation_price(mocker, default_conf):\n         pair='NEAR/USDT:USDT',\n         open_rate=18.884,\n         is_short=False,\n-        position=0.8,\n+        amount=0.8,\n+        stake_amount=18.884 * 0.8,\n         wallet_balance=0.8,\n     )\n     assert liq_price == 17.47\n@@ -4143,7 +4144,8 @@ def test_get_or_calculate_liquidation_price(mocker, default_conf):\n         pair='NEAR/USDT:USDT',\n         open_rate=18.884,\n         is_short=False,\n-        position=0.8,\n+        amount=0.8,\n+        stake_amount=18.884 * 0.8,\n         wallet_balance=0.8,\n     )\n     assert liq_price == 17.540699999999998\n@@ -4543,7 +4545,8 @@ def test_liquidation_price_is_none(\n         pair='DOGE/USDT',\n         open_rate=open_rate,\n         is_short=is_short,\n-        position=71200.81144,\n+        amount=71200.81144,\n+        stake_amount=open_rate * 71200.81144,\n         wallet_balance=-56354.57,\n         mm_ex_1=0.10,\n         upnl_ex_1=0.0\n@@ -4552,7 +4555,7 @@ def test_liquidation_price_is_none(\n \n @pytest.mark.parametrize(\n     'exchange_name, is_short, trading_mode, margin_mode, wallet_balance, '\n-    'mm_ex_1, upnl_ex_1, maintenance_amt, position, open_rate, '\n+    'mm_ex_1, upnl_ex_1, maintenance_amt, amount, open_rate, '\n     'mm_ratio, expected',\n     [\n         (\"binance\", False, 'futures', 'isolated', 1535443.01, 0.0,\n@@ -4566,7 +4569,7 @@ def test_liquidation_price_is_none(\n     ])\n def test_liquidation_price(\n     mocker, default_conf, exchange_name, open_rate, is_short, trading_mode,\n-    margin_mode, wallet_balance, mm_ex_1, upnl_ex_1, maintenance_amt, position, mm_ratio, expected\n+    margin_mode, wallet_balance, mm_ex_1, upnl_ex_1, maintenance_amt, amount, mm_ratio, expected\n ):\n     default_conf['trading_mode'] = trading_mode\n     default_conf['margin_mode'] = margin_mode\n@@ -4580,7 +4583,8 @@ def test_liquidation_price(\n         wallet_balance=wallet_balance,\n         mm_ex_1=mm_ex_1,\n         upnl_ex_1=upnl_ex_1,\n-        position=position,\n+        amount=amount,\n+        stake_amount=open_rate * amount,\n     ), 2), expected)\n \n \n@@ -5111,6 +5115,7 @@ def test_get_liquidation_price(\n         pair='ETH/USDT:USDT',\n         open_rate=open_rate,\n         amount=amount,\n+        stake_amount=amount * open_rate / leverage,\n         leverage=leverage,\n         is_short=is_short,\n     )"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\nsed -i 's/ccxt==1.92.52/ccxt==1.92.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/exchange/test_exchange.py",
                "testStepsFull": "pipenv run pytest"
            },
            {
                "id": 7264,
                "created_at": "2022-08-20T17:15:04Z",
                "closed_at": "2022-08-22T09:46:39Z",
                "title": "\"No such file or directory\" after a hyperopt run doesn't show the results",
                "labels": "Bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "f55d5ffd8c65dc0cc76ab2255e21551a406f5a66",
                        "commit_date": "2022-08-22T09:20:14Z",
                        "parents": "914b6247e454c0fe432c1c76276b4481e5d98589",
                        "stat": {
                            "total": 1,
                            "additions": 9,
                            "deletions": 8,
                            "files": [
                                {
                                    "sha": "b99e7a94b81e5ed6a9069be41707a229780ec3b3",
                                    "filename": "freqtrade/resolvers/iresolver.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 1,
                                    "changes": 5,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/f55d5ffd8c65dc0cc76ab2255e21551a406f5a66/freqtrade%2Fresolvers%2Firesolver.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/f55d5ffd8c65dc0cc76ab2255e21551a406f5a66/freqtrade%2Fresolvers%2Firesolver.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/freqtrade%2Fresolvers%2Firesolver.py?ref=f55d5ffd8c65dc0cc76ab2255e21551a406f5a66",
                                    "patch": "@@ -193,7 +193,10 @@ def search_all_objects(cls, directory: Path, enum_failed: bool,\n         :return: List of dicts containing 'name', 'class' and 'location' entries\n         \"\"\"\n         logger.debug(f\"Searching for {cls.object_type.__name__} '{directory}'\")\n-        objects = []\n+        objects: List[Dict[str, Any]] = []\n+        if not directory.is_dir():\n+            logger.info(f\"'{directory}' is not a directory, skipping.\")\n+            return objects\n         for entry in directory.iterdir():\n             if (\n                 recursive and entry.is_dir()"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b794cdc997a4c0c7dcecb58d900152bfb4668066",
                                    "filename": "tests/strategy/test_strategy_loading.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/freqtrade/freqtrade/blob/f55d5ffd8c65dc0cc76ab2255e21551a406f5a66/tests%2Fstrategy%2Ftest_strategy_loading.py",
                                    "raw_url": "https://github.com/freqtrade/freqtrade/raw/f55d5ffd8c65dc0cc76ab2255e21551a406f5a66/tests%2Fstrategy%2Ftest_strategy_loading.py",
                                    "contents_url": "https://api.github.com/repos/freqtrade/freqtrade/contents/tests%2Fstrategy%2Ftest_strategy_loading.py?ref=f55d5ffd8c65dc0cc76ab2255e21551a406f5a66",
                                    "patch": "@@ -48,6 +48,10 @@ def test_search_all_strategies_with_failed():\n     assert len([x for x in strategies if x['class'] is not None]) == 9\n     assert len([x for x in strategies if x['class'] is None]) == 1\n \n+    directory = Path(__file__).parent / \"strats_nonexistingdir\"\n+    strategies = StrategyResolver.search_all_objects(directory, enum_failed=True)\n+    assert len(strategies) == 0\n+\n \n def test_load_strategy(default_conf, result):\n     default_conf.update({'strategy': 'SampleStrategy',"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip\nsed -i 's/ccxt==1.92.52/ccxt==1.92.1/g' requirements.txt\npipenv run pip install setuptools==65.5.0\npipenv run pip install wheel==0.38.4\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e .",
                "testSteps": "pipenv run pytest -- tests/strategy/test_strategy_loading.py",
                "testStepsFull": "pipenv run pytest"
            }
        ],
        "installSteps": "pipenv --python 3.8\nsudo wget -c http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\ntar -xzf ta-lib-0.4.0-src.tar.gz\ncd ta-lib && pipenev run ./configure --prefix=/usr && pipenv run make && pipenv run sudo make install \npipenv install TA-Lib\npipenv run python -m pip install --upgrade pip wheel\nexport LD_LIBRARY_PATH=${HOME}/dependencies/lib:$LD_LIBRARY_PATH\nexport TA_LIBRARY_PATH=${HOME}/dependencies/lib\nexport TA_INCLUDE_PATH=${HOME}/dependencies/include\npipenv run pip install -r requirements-dev.txt\npipenv run pip install -e ."
    },
    {
        "_id": "64b99a131d0358d36f0c0728",
        "username": "google",
        "repository": "jax",
        "issues": [
            {
                "id": 15886,
                "created_at": "2023-05-05T16:41:07Z",
                "closed_at": "2023-05-05T17:48:39Z",
                "title": "Assertion error from inside _pjit_batcher_for_sharding",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "1629c6c76bc4476ec214eeb9df5e50eef40f57a7",
                        "commit_date": "2023-05-05T17:48:33Z",
                        "parents": "d992080bfa115ff41a42004b40e86f4866519c40",
                        "stat": {
                            "total": 11,
                            "additions": 23,
                            "deletions": 12,
                            "files": [
                                {
                                    "sha": "680adc7275a1d042cfdbc15b3e17a804d99f4c6c",
                                    "filename": "jax/_src/pjit.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/1629c6c76bc4476ec214eeb9df5e50eef40f57a7/jax%2F_src%2Fpjit.py",
                                    "raw_url": "https://github.com/google/jax/raw/1629c6c76bc4476ec214eeb9df5e50eef40f57a7/jax%2F_src%2Fpjit.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fpjit.py?ref=1629c6c76bc4476ec214eeb9df5e50eef40f57a7",
                                    "patch": "@@ -1389,6 +1389,8 @@ def _pjit_batcher_for_sharding(\n     return new_gs\n   else:\n     assert isinstance(s, GSPMDSharding)\n+    if isinstance(getattr(s, '_original_sharding', None), NamedSharding):\n+      mesh = s._original_sharding.mesh  # type: ignore\n     assert mesh is not None and not mesh.empty\n     parsed_pspec = parse_flatten_op_sharding(s._op_sharding, mesh)[0]  # type: ignore\n     parsed_pspec = parsed_pspec.insert_axis_partitions(dim, val)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "e36d02641120caf5f70e8f45d9a7ecde4a77c479",
                                    "filename": "tests/pjit_test.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 11,
                                    "changes": 21,
                                    "blob_url": "https://github.com/google/jax/blob/1629c6c76bc4476ec214eeb9df5e50eef40f57a7/tests%2Fpjit_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/1629c6c76bc4476ec214eeb9df5e50eef40f57a7/tests%2Fpjit_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fpjit_test.py?ref=1629c6c76bc4476ec214eeb9df5e50eef40f57a7",
                                    "patch": "@@ -2916,23 +2916,22 @@ def test_with_sharding_constraint_spmd_axis_name(self):\n     x = jnp.arange(math.prod(shape)).reshape(shape)\n \n     def f(inp):\n-      return with_sharding_constraint(inp, P('data', None, None))\n+      sharding = NamedSharding(mesh, P('data', None, None))\n+      return with_sharding_constraint(inp, sharding)\n \n-    with mesh:\n-      out = jax.vmap(jax.jit(f), spmd_axis_name='mdl')(x)\n-      ns, _ = op_shardings.get_num_ways_dim_sharded(\n-          out.sharding._to_xla_op_sharding(out.ndim))\n-      self.assertListEqual(ns, [2, 2, 1, 1])\n+    out = jax.vmap(jax.jit(f), spmd_axis_name='mdl')(x)\n+    ns, _ = op_shardings.get_num_ways_dim_sharded(\n+        out.sharding._to_xla_op_sharding(out.ndim))\n+    self.assertListEqual(ns, [2, 2, 1, 1])\n \n     def apply_with_scan(x):\n       x, _ = jax.lax.scan(lambda x, _: (f(x), None), x, None, length=1)\n       return x\n \n-    with mesh:\n-      out2 = jax.vmap(apply_with_scan, spmd_axis_name='mdl')(x)\n-      ns2, _ = op_shardings.get_num_ways_dim_sharded(\n-          out2.sharding._to_xla_op_sharding(out2.ndim))\n-      self.assertListEqual(ns2, [2, 2, 1, 1])\n+    out2 = jax.vmap(apply_with_scan, spmd_axis_name='mdl')(x)\n+    ns2, _ = op_shardings.get_num_ways_dim_sharded(\n+        out2.sharding._to_xla_op_sharding(out2.ndim))\n+    self.assertListEqual(ns2, [2, 2, 1, 1])\n \n   def test_device_put_sharding_nondivisible_sharding_error(self):\n     mesh = jtu.create_global_mesh((2,), ('x',))"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/pjit_test.py"
            },
            {
                "id": 15663,
                "created_at": "2023-04-19T17:13:39Z",
                "closed_at": "2023-04-19T23:30:29Z",
                "title": "Source location info is incorrectly shared via caching",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                        "commit_date": "2023-04-19T17:56:53Z",
                        "parents": "c84446488827cc4dba894a8bc50c981227eb5ec1",
                        "stat": {
                            "total": 4,
                            "additions": 75,
                            "deletions": 71,
                            "files": [
                                {
                                    "sha": "fc21494055baee02ba86a6e675c1e2103b5ba110",
                                    "filename": "jax/_src/core.py",
                                    "status": "modified",
                                    "additions": 3,
                                    "deletions": 2,
                                    "changes": 5,
                                    "blob_url": "https://github.com/google/jax/blob/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Fcore.py",
                                    "raw_url": "https://github.com/google/jax/raw/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Fcore.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fcore.py?ref=a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                                    "patch": "@@ -426,7 +426,7 @@ def traverse_jaxpr_params(f, params):\n           if type(p) in (Jaxpr, ClosedJaxpr)}\n \n \n-def eval_jaxpr(jaxpr: Jaxpr, consts, *args):\n+def eval_jaxpr(jaxpr: Jaxpr, consts, *args, propagate_source_info=True):\n   def read(v: Atom) -> Any:\n     return v.val if isinstance(v, Literal) else env[v]\n \n@@ -441,7 +441,8 @@ def write(v: Var, val: Any) -> None:\n   for eqn in jaxpr.eqns:\n     subfuns, bind_params = eqn.primitive.get_bind_params(eqn.params)\n     name_stack = source_info_util.current_name_stack() + eqn.source_info.name_stack\n-    with source_info_util.user_context(eqn.source_info.traceback, name_stack=name_stack):\n+    traceback = eqn.source_info.traceback if propagate_source_info else None\n+    with source_info_util.user_context(traceback, name_stack=name_stack):\n       ans = eqn.primitive.bind(*subfuns, *map(read, eqn.invars), **bind_params)\n     if eqn.primitive.multiple_results:\n       map(write, eqn.outvars, ans)"
                                },
                                {
                                    "sha": "5cabfd08bdb6ff78a5133b58efe6e29153eefdca",
                                    "filename": "jax/_src/interpreters/partial_eval.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/google/jax/blob/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Finterpreters%2Fpartial_eval.py",
                                    "raw_url": "https://github.com/google/jax/raw/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Finterpreters%2Fpartial_eval.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Finterpreters%2Fpartial_eval.py?ref=a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                                    "patch": "@@ -1879,7 +1879,8 @@ def process_call(self, call_primitive, f, explicit_tracers, params):\n       dbg = debug_info_final(f, call_primitive.name)\n       jaxpr, out_type, consts = trace_to_subjaxpr_dynamic2(f, self.main, debug_info=dbg)\n     if params.get('inline', False):\n-      return core.eval_jaxpr(jaxpr, consts, *in_tracers)\n+      return core.eval_jaxpr(jaxpr, consts, *in_tracers,\n+                             propagate_source_info=False)\n     source_info = source_info_util.current()\n     out_tracers = []\n     for aval, _ in out_type:"
                                },
                                {
                                    "sha": "932393ddfa53c9f41ccddbca678658f8f7e5c70d",
                                    "filename": "jax/_src/pjit.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/google/jax/blob/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Fpjit.py",
                                    "raw_url": "https://github.com/google/jax/raw/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/jax%2F_src%2Fpjit.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fpjit.py?ref=a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                                    "patch": "@@ -1233,7 +1233,8 @@ def pjit_staging_rule(trace, *args, **params):\n       all(is_unspecified(i) for i in params[\"in_shardings\"]) and\n       all(is_unspecified(o) for o in params[\"out_shardings\"])):\n     jaxpr = params['jaxpr']\n-    return core.eval_jaxpr(jaxpr.jaxpr, jaxpr.consts, *args)\n+    return core.eval_jaxpr(jaxpr.jaxpr, jaxpr.consts, *args,\n+                           propagate_source_info=False)\n   elif config.jax_dynamic_shapes:\n     source_info = source_info_util.current()\n     out_tracers = []"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b1cc858aa5ec9a73ae1cfb3b674180fe9453edb9",
                                    "filename": "tests/BUILD",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 0,
                                    "changes": 9,
                                    "blob_url": "https://github.com/google/jax/blob/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/tests%2FBUILD",
                                    "raw_url": "https://github.com/google/jax/raw/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/tests%2FBUILD",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2FBUILD?ref=a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                                    "patch": "@@ -1060,6 +1060,15 @@ jax_test(\n     ],\n )\n \n+py_test(\n+    name = \"source_info_test\",\n+    srcs = [\"source_info_test.py\"],\n+    deps = [\n+        \"//jax\",\n+        \"//jax:test_util\",\n+    ],\n+)\n+\n exports_files(\n     [\n         \"api_test.py\","
                                },
                                {
                                    "sha": "1f0b5392e13887215377ae79f1339e07181d5ad1",
                                    "filename": "tests/source_info_test.py",
                                    "status": "added",
                                    "additions": 55,
                                    "deletions": 0,
                                    "changes": 55,
                                    "blob_url": "https://github.com/google/jax/blob/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/tests%2Fsource_info_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/a3b262c37972ec4333c4f110c0cd1d8e7fed858a/tests%2Fsource_info_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fsource_info_test.py?ref=a3b262c37972ec4333c4f110c0cd1d8e7fed858a",
                                    "patch": "@@ -0,0 +1,55 @@\n+# Copyright 2023 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from functools import partial\n+import inspect\n+\n+from absl.testing import absltest\n+\n+import jax\n+from jax import lax\n+from jax.config import config\n+from jax._src import source_info_util\n+from jax._src import test_util as jtu\n+\n+config.parse_flags_with_absl()\n+\n+\n+class SourceInfoTest(jtu.JaxTestCase):\n+\n+  def test_inline_jit_location_uses_callee_location(self):\n+\n+    # 'f' should be inlined into both 'g' and 'h', using the source line\n+    # information of the call site. In particular, the source line information\n+    # of 'h' should not refer to the source information of 'g'.\n+    @partial(jax.jit, inline=True)\n+    def f(x): return lax.add(x, 3)\n+\n+    def g(x): return lax.add(f(x), 4)\n+\n+    def h(x): return lax.add(f(x), 5)\n+\n+    for fn in (g, h):\n+      lines, fn_startline = inspect.getsourcelines(fn)\n+      fn_endline = fn_startline + len(lines)\n+      jaxpr = jax.make_jaxpr(fn)(2)\n+      for eqn in jaxpr.eqns:\n+        frame = source_info_util.user_frame(eqn.source_info)\n+        assert frame is not None, eqn\n+        self.assertLessEqual(fn_startline, frame.start_line)\n+        self.assertLessEqual(frame.end_line, fn_endline)\n+\n+\n+if __name__ == '__main__':\n+  absltest.main(testLoader=jtu.JaxTestLoader())"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/source_info_test.py"
            },
            {
                "id": 15400,
                "created_at": "2023-04-05T03:30:13Z",
                "closed_at": "2023-04-05T18:55:53Z",
                "title": "TypeError: Cannot determine dtype of Zero(ShapedArray(complex128[]))",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "ac4942d7f7e30b81525c88aa310db7d23add7b80",
                        "commit_date": "2023-04-05T03:45:21Z",
                        "parents": "bf50551e0fed037c359f1feee863864d3ec7ef50",
                        "stat": {
                            "total": 1,
                            "additions": 11,
                            "deletions": 10,
                            "files": [
                                {
                                    "sha": "d73ef2b12f8d78cb4d771005267bdf5fcffb7127",
                                    "filename": "jax/_src/lax/lax.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 1,
                                    "changes": 5,
                                    "blob_url": "https://github.com/google/jax/blob/ac4942d7f7e30b81525c88aa310db7d23add7b80/jax%2F_src%2Flax%2Flax.py",
                                    "raw_url": "https://github.com/google/jax/raw/ac4942d7f7e30b81525c88aa310db7d23add7b80/jax%2F_src%2Flax%2Flax.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Flax%2Flax.py?ref=ac4942d7f7e30b81525c88aa310db7d23add7b80",
                                    "patch": "@@ -355,6 +355,7 @@ def complex(x: ArrayLike, y: ArrayLike) -> Array:\n \n def conj(x: ArrayLike) -> Array:\n   r\"\"\"Elementwise complex conjugate function: :math:`\\overline{x}`.\"\"\"\n+  # TODO(mattjj): remove input_dtype, not needed anymore\n   return conj_p.bind(x, input_dtype=_dtype(x))\n \n def abs(x: ArrayLike) -> Array:\n@@ -1896,7 +1897,9 @@ def _conj_impl(x, **kw):\n \n def _conj_transpose_rule(t, x, *, input_dtype):\n   assert ad.is_undefined_primal(x)\n-  if dtypes.issubdtype(input_dtype, np.complexfloating):\n+  if type(t) is ad_util.Zero:\n+    return [ad_util.Zero(x.aval)]\n+  elif dtypes.issubdtype(input_dtype, np.complexfloating):\n     return [conj(t)]\n   else:\n     return [real(t)]"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "06a6b28ba14d08660e2a33311d719a9144bb6268",
                                    "filename": "tests/api_test.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 0,
                                    "changes": 6,
                                    "blob_url": "https://github.com/google/jax/blob/ac4942d7f7e30b81525c88aa310db7d23add7b80/tests%2Fapi_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/ac4942d7f7e30b81525c88aa310db7d23add7b80/tests%2Fapi_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fapi_test.py?ref=ac4942d7f7e30b81525c88aa310db7d23add7b80",
                                    "patch": "@@ -4214,6 +4214,12 @@ def outer_fn(x):\n     self.assertEqual(inner_count, 1)\n     self.assertEqual(outer_count, 1)\n \n+  def test_grad_conj_symbolic_zeros(self):\n+    # https://github.com/google/jax/issues/15400\n+    f = lambda x: jax.jit(lambda x, y: (x, y))(x, jax.lax.conj(x))[0]\n+    out = jax.grad(f)(3.0)  # doesn't crash\n+    self.assertAllClose(out, 1., check_dtypes=False)\n+\n \n class RematTest(jtu.JaxTestCase):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/api_test.py"
            },
            {
                "id": 15385,
                "created_at": "2023-04-04T06:36:51Z",
                "closed_at": "2023-04-10T14:31:56Z",
                "title": "jnp.arange is broken inside shmap unless using jit",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "e04409f088026ca2fe43bf427dfe9e6ad7218040",
                        "commit_date": "2023-04-09T06:03:49Z",
                        "parents": "90d58f45729fa6a7755ea99349997c2070d6e9c3",
                        "stat": {
                            "total": 1,
                            "additions": 19,
                            "deletions": 18,
                            "files": [
                                {
                                    "sha": "cd6c36343452aa87e9d07778a4ba4137a18b98d9",
                                    "filename": "jax/_src/dispatch.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/e04409f088026ca2fe43bf427dfe9e6ad7218040/jax%2F_src%2Fdispatch.py",
                                    "raw_url": "https://github.com/google/jax/raw/e04409f088026ca2fe43bf427dfe9e6ad7218040/jax%2F_src%2Fdispatch.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fdispatch.py?ref=e04409f088026ca2fe43bf427dfe9e6ad7218040",
                                    "patch": "@@ -320,7 +320,7 @@ def _names_to_pspec(names):\n         ndmin = max(names) + 1 if names else 0\n         return PartitionSpec(*(names.get(i) for i in range(ndmin)))\n       yield from ((NamedSharding(eqn.params['mesh'], _names_to_pspec(names)), source_info)\n-                  for names in eqn.params['in_names'])\n+                  for names in [*eqn.params['in_names'], *eqn.params['out_names']])\n   for subjaxpr in core.subjaxprs(jaxpr):\n     yield from jaxpr_shardings(subjaxpr)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "7b9a4118e8e3d2c99efdc4df45e9a7d2a3d8c728",
                                    "filename": "tests/shard_map_test.py",
                                    "status": "modified",
                                    "additions": 17,
                                    "deletions": 0,
                                    "changes": 17,
                                    "blob_url": "https://github.com/google/jax/blob/e04409f088026ca2fe43bf427dfe9e6ad7218040/tests%2Fshard_map_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/e04409f088026ca2fe43bf427dfe9e6ad7218040/tests%2Fshard_map_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fshard_map_test.py?ref=e04409f088026ca2fe43bf427dfe9e6ad7218040",
                                    "patch": "@@ -684,6 +684,23 @@ def foo(x):\n     with self.assertRaisesRegex(NotImplementedError, 'axis_index'):\n       g(x)\n \n+  def test_jaxpr_shardings_with_no_outputs(self):\n+    # https://github.com/google/jax/issues/15385\n+    mesh = jtu.create_global_mesh((4,), ('i',))\n+\n+    @jax.jit\n+    @partial(shard_map, mesh=mesh, in_specs=(), out_specs=P('i'))\n+    def f():\n+      return jax.lax.iota(jnp.dtype('int32'), 4)\n+    f()  # don't crash\n+\n+    @partial(shard_map, mesh=mesh, in_specs=(P('i'),), out_specs=P('i'))\n+    def g(a_block):\n+      i = jnp.arange(a_block.shape[0])\n+      return i + a_block\n+\n+    g(np.arange(32))  # don't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/shard_map_test.py"
            },
            {
                "id": 15190,
                "created_at": "2023-03-24T01:47:30Z",
                "closed_at": "2023-03-24T03:48:38Z",
                "title": "ndarray.round() stopped working without an argument.",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "793387e496975fc8b438aed541d809774621d29e",
                        "commit_date": "2023-03-24T03:16:23Z",
                        "parents": "4cb3b011a0a7ba9d47049544d51da09dec48db72",
                        "stat": {
                            "total": 1,
                            "additions": 6,
                            "deletions": 5,
                            "files": [
                                {
                                    "sha": "576236d0e144a53f86128e1f05dd6edfb2b020ea",
                                    "filename": "jax/_src/numpy/array_methods.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/793387e496975fc8b438aed541d809774621d29e/jax%2F_src%2Fnumpy%2Farray_methods.py",
                                    "raw_url": "https://github.com/google/jax/raw/793387e496975fc8b438aed541d809774621d29e/jax%2F_src%2Fnumpy%2Farray_methods.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fnumpy%2Farray_methods.py?ref=793387e496975fc8b438aed541d809774621d29e",
                                    "patch": "@@ -725,7 +725,7 @@ def max(self, values, *, indices_are_sorted=False, unique_indices=False,\n   \"ravel\": lax_numpy.ravel,\n   \"repeat\": lax_numpy.repeat,\n   \"reshape\": _reshape,\n-  \"round\": round,\n+  \"round\": lax_numpy.round,\n   \"searchsorted\": lax_numpy.searchsorted,\n   \"sort\": lax_numpy.sort,\n   \"squeeze\": lax_numpy.squeeze,"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c68162d704c037f553f0250ba728e839f7471d5e",
                                    "filename": "tests/lax_numpy_test.py",
                                    "status": "modified",
                                    "additions": 4,
                                    "deletions": 0,
                                    "changes": 4,
                                    "blob_url": "https://github.com/google/jax/blob/793387e496975fc8b438aed541d809774621d29e/tests%2Flax_numpy_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/793387e496975fc8b438aed541d809774621d29e/tests%2Flax_numpy_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_numpy_test.py?ref=793387e496975fc8b438aed541d809774621d29e",
                                    "patch": "@@ -858,6 +858,10 @@ def testOperatorRound(self, jit):\n                         jround(jnp.array(1.234, jnp.float32)),\n                         check_dtypes=False)\n \n+  def testRoundMethod(self):\n+    # https://github.com/google/jax/issues/15190\n+    (jnp.arange(3.) / 5.).round()  # doesn't crash\n+\n   @jtu.sample_product(shape=[(5,), (5, 2)])\n   def testOperatorReversed(self, shape):\n     rng = jtu.rand_default(self.rng())"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_numpy_test.py"
            },
            {
                "id": 14833,
                "created_at": "2023-03-07T21:40:38Z",
                "closed_at": "2023-04-12T22:12:01Z",
                "title": "custom JVP with symbolic zeros can unintentionally err under jit",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "03e72e3b77cc32aaa740f340674154af4e81f693",
                        "commit_date": "2023-04-12T22:11:55Z",
                        "parents": "62e2860040302cbef6fceb0fea205619658066f2",
                        "stat": {
                            "total": 25,
                            "additions": 112,
                            "deletions": 87,
                            "files": [
                                {
                                    "sha": "1d99f31c38db6c48d274a10e645ef6b064c4679a",
                                    "filename": "jax/BUILD",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/google/jax/blob/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2FBUILD",
                                    "raw_url": "https://github.com/google/jax/raw/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2FBUILD",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2FBUILD?ref=03e72e3b77cc32aaa740f340674154af4e81f693",
                                    "patch": "@@ -371,6 +371,7 @@ pytype_strict_library(\n     name = \"partial_eval\",\n     srcs = [\"_src/interpreters/partial_eval.py\"],\n     deps = [\n+        \":ad_util\",\n         \":api_util\",\n         \":config\",\n         \":core\","
                                },
                                {
                                    "sha": "5e52208e5a8ced67b569780612a15f9bebea9242",
                                    "filename": "jax/_src/checkify.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 11,
                                    "changes": 12,
                                    "blob_url": "https://github.com/google/jax/blob/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Fcheckify.py",
                                    "raw_url": "https://github.com/google/jax/raw/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Fcheckify.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fcheckify.py?ref=03e72e3b77cc32aaa740f340674154af4e81f693",
                                    "patch": "@@ -907,17 +907,7 @@ def custom_jvp_call_rule(in_err, enabled_errors, *in_vals, num_consts,\n                         call_jaxpr.consts, enabled_errors, err_tree))\n   partial_checkify, f_metadata = _flatten_and_get_error_metadata_thunk(\n       partial_checkify)\n-\n-  # Construct the default jvp function, without checkify-ing.\n-  @lu.wrap_init\n-  def jvp(*xs):\n-    # TODO(lenamartens, sharadmv): why not checkify here?\n-    jvp_jaxpr, jvp_consts = jvp_jaxpr_thunk()\n-    n, ragged = divmod(len(xs), 2)\n-    assert not ragged\n-    primals, tangents = xs[num_consts:n], xs[n+num_consts:]\n-    return core.eval_jaxpr(jvp_jaxpr, jvp_consts, *primals, *tangents)\n-\n+  jvp = custom_derivatives.lift_jvp(num_consts, jvp_jaxpr_thunk)\n   jvp, jvp_out_tree = flatten_fun_output(jvp)\n   all_outs = custom_derivatives.custom_jvp_call_p.bind(\n       partial_checkify, jvp, *err_vals, *in_vals, **params)"
                                },
                                {
                                    "sha": "909d79e7e6f71a082edef9a426d500f00278c0b8",
                                    "filename": "jax/_src/custom_derivatives.py",
                                    "status": "modified",
                                    "additions": 24,
                                    "deletions": 12,
                                    "changes": 36,
                                    "blob_url": "https://github.com/google/jax/blob/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Fcustom_derivatives.py",
                                    "raw_url": "https://github.com/google/jax/raw/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Fcustom_derivatives.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fcustom_derivatives.py?ref=03e72e3b77cc32aaa740f340674154af4e81f693",
                                    "patch": "@@ -267,7 +267,7 @@ def _add_args_(extra_args, *args, **kwargs):\n   all_args = (extra_args + args)\n   yield (yield all_args, kwargs)\n \n-@lu.transformation_with_aux\n+@partial(lu.transformation_with_aux, use_eq_store=True)\n def _flatten_jvp(primal_name, jvp_name, in_tree, maybe_out_type, *args):\n   primals_in, tangents_in = split_list(args, [len(args) // 2])\n   py_primals = tree_unflatten(in_tree, primals_in)\n@@ -327,7 +327,8 @@ def _flatten_jvp(primal_name, jvp_name, in_tree, maybe_out_type, *args):\n       for x in primals_out]\n   tangent_avals_out = [\n       raise_to_shaped(core.get_aval(t), weak_type=False).strip_named_shape()\n-      if type(t) is not SymbolicZero else t.aval for t in tangents_out]\n+      if type(t) is not SymbolicZero else t.aval.strip_weak_type()\n+      for t in tangents_out]\n   if primal_avals_out != tangent_avals_out:\n     if len(primal_avals_out) == 1:\n       (av1,), (av2,) = primal_avals_out, tangent_avals_out\n@@ -372,18 +373,29 @@ def get_bind_params(self, params):\n     num_consts = new_params.pop('num_consts')\n     jvp_jaxpr_thunk = new_params.pop('jvp_jaxpr_thunk')\n     fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr))\n-\n-    @lu.wrap_init\n-    def jvp(*xs):\n-      jvp_jaxpr, jvp_consts = jvp_jaxpr_thunk()\n-      n, ragged = divmod(len(xs), 2)\n-      assert not ragged\n-      primals, tangents = xs[num_consts:n], xs[n+num_consts:]\n-      return core.eval_jaxpr(jvp_jaxpr, jvp_consts, *primals, *tangents)\n-\n+    jvp = lift_jvp(num_consts, jvp_jaxpr_thunk)\n     return [fun, jvp], new_params\n \n-@lu.transformation_with_aux\n+def lift_jvp(num_consts: int, jvp_jaxpr_thunk: Callable) -> lu.WrappedFun:\n+  @lu.wrap_init\n+  def jvp(*xs):\n+    n, ragged = divmod(len(xs), 2)\n+    assert not ragged\n+    primals, tangents = xs[num_consts:n], xs[n+num_consts:]\n+    zeros = [type(t) is SymbolicZero for t in tangents]\n+    jvp_jaxpr, jvp_consts, out_zeros = jvp_jaxpr_thunk(*zeros)\n+    nonzero_tangents = [t for t in tangents if type(t) is not SymbolicZero]\n+    out = core.eval_jaxpr(jvp_jaxpr, jvp_consts, *primals, *nonzero_tangents)\n+    out_primals, nz_out_tangents = split_list(out, [len(out_zeros)])\n+    nz_out_tangents_ = iter(nz_out_tangents)\n+    out_tangents = [SymbolicZero(core.get_aval(p).at_least_vspace())\n+                    if z else next(nz_out_tangents_)\n+                    for p, z in zip(out_primals, out_zeros)]\n+    assert next(nz_out_tangents_, None) is None\n+    return [*out_primals, *out_tangents]\n+  return jvp\n+\n+@partial(lu.transformation_with_aux, use_eq_store=True)\n def process_env_traces(primitive, level: int, jvp_was_run: bool, *args):\n   outs = yield args, {}\n   todo = []"
                                },
                                {
                                    "sha": "7c1ea9d6330324d0480316bf8e10e8babd630c1c",
                                    "filename": "jax/_src/interpreters/partial_eval.py",
                                    "status": "modified",
                                    "additions": 23,
                                    "deletions": 2,
                                    "changes": 25,
                                    "blob_url": "https://github.com/google/jax/blob/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Finterpreters%2Fpartial_eval.py",
                                    "raw_url": "https://github.com/google/jax/raw/03e72e3b77cc32aaa740f340674154af4e81f693/jax%2F_src%2Finterpreters%2Fpartial_eval.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Finterpreters%2Fpartial_eval.py?ref=03e72e3b77cc32aaa740f340674154af4e81f693",
                                    "patch": "@@ -28,6 +28,7 @@\n \n from jax._src import linear_util as lu\n from jax._src.config import config\n+from jax._src import ad_util\n from jax._src import api_util\n from jax._src import core\n from jax._src import effects\n@@ -1950,8 +1951,15 @@ def process_custom_jvp_call(self, prim, fun, jvp, tracers, *, symbolic_zeros):\n       fun_jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, self.main, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n     main_ = ref(self.main)\n-    jvp_jaxpr_thunk = _memoize(\n-        lambda: trace_to_subjaxpr_dynamic(jvp, main_(), 2 * in_avals)[::2])\n+\n+    @_memoize\n+    def jvp_jaxpr_thunk(*in_zeros):\n+      nz_tangent_avals, zero_avals = partition_list(in_zeros, in_avals)\n+      jvp_, out_zeros = _jvp_jaxpr_zeros(jvp, in_zeros, tuple(zero_avals))\n+      in_avals_ = (*in_avals, *nz_tangent_avals)\n+      jaxpr, _, out_consts = trace_to_subjaxpr_dynamic(jvp_, main_(), in_avals_)\n+      return jaxpr, out_consts, out_zeros()\n+\n     out_tracers = [DynamicJaxprTracer(self, a) for a in out_avals]\n     invars = map(self.getvar, tracers)\n     constvars = map(self.getvar, map(self.instantiate_const, consts))\n@@ -2061,6 +2069,19 @@ def memoized(*args):\n     return out\n   return memoized\n \n+@lu.transformation_with_aux\n+def _jvp_jaxpr_zeros(in_zeros, zero_avals, *primal_tangent_avals):\n+  in_primals, nz_in_tangents = split_list(primal_tangent_avals, [len(in_zeros)])\n+  symbolic_zeros = map(ad_util.SymbolicZero, zero_avals)\n+  tangents = merge_lists(in_zeros, nz_in_tangents, symbolic_zeros)\n+  out = yield (*in_primals, *tangents), {}\n+  n, ragged = divmod(len(out), 2)\n+  assert not ragged\n+  out_primals, out_tangents = out[:n], out[n:]\n+  out_zeros = [type(t) is ad_util.SymbolicZero for t in out_tangents]\n+  out_nz_tangents, _ = partition_list(out_zeros, out_tangents)\n+  yield [*out_primals, *out_nz_tangents], out_zeros\n+\n # TODO(mattjj): remove this DebugInfo and helper functions, replace with\n # api_util.py versions\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "a11ac72153158304595a3ef37cf7ab46fd8f73e7",
                                    "filename": "tests/api_test.py",
                                    "status": "modified",
                                    "additions": 38,
                                    "deletions": 0,
                                    "changes": 38,
                                    "blob_url": "https://github.com/google/jax/blob/03e72e3b77cc32aaa740f340674154af4e81f693/tests%2Fapi_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/03e72e3b77cc32aaa740f340674154af4e81f693/tests%2Fapi_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fapi_test.py?ref=03e72e3b77cc32aaa740f340674154af4e81f693",
                                    "patch": "@@ -7486,6 +7486,44 @@ def f_jvp(primals, tangents):\n \n     jax.grad(lambda x, y: jax.vmap(f)(x, y).sum())(jnp.ones(3), jnp.ones(3))\n \n+  def test_symbolic_zeros_memoization_caching(self):\n+    # Tests multiple zero patterns for partial_eval._memoize, and also tests\n+    # that we're okay with stores being occupied with equal values.\n+\n+    @jax.custom_jvp\n+    def f(x, y):\n+      return x * y\n+\n+    @partial(f.defjvp, symbolic_zeros=True)\n+    def f_jvp(primals, tangents):\n+      x, y = primals\n+      x_dot, y_dot = tangents\n+      return f(x, y), y_dot\n+\n+    f_ = core.jaxpr_as_fun(jax.make_jaxpr(f)(2., 3.))\n+    _ = jax.linearize(f_, 2., 3.)\n+    _ = jax.linearize(lambda x: f_(x, 3.), 2.)  # don't crash!\n+\n+  def test_symbolic_zeros_under_jit(self):\n+    # https://github.com/google/jax/issues/14833\n+    Zero = jax.custom_derivatives.SymbolicZero\n+\n+    @jax.custom_jvp\n+    def f(x, y):\n+        return x * y\n+\n+    @partial(f.defjvp, symbolic_zeros=True)\n+    def fjvp(primals, tangents):\n+        x, y = primals\n+        tx, ty = tangents\n+        assert type(tx) is not Zero or type(ty) is not Zero\n+        return f(x, y), (\n+            ty if type(tx) is Zero else\n+            tx if type(ty) is Zero else\n+            tx + ty)\n+\n+    jax.jacfwd(jax.jit(f))(0.1, 0.2)  # don't crash\n+\n \n class CustomVJPTest(jtu.JaxTestCase):\n "
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/api_test.py"
            },
            {
                "id": 13857,
                "created_at": "2023-01-04T11:04:15Z",
                "closed_at": "2023-01-09T17:11:55Z",
                "title": "AttributeError: _src when importing jax",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "f317943f5671b0b1da3f91e71405b8f66e1c8fcc",
                        "commit_date": "2023-01-09T17:11:50Z",
                        "parents": "74601e59e15084ad4b7e0bcf045e8773b24f58bd",
                        "stat": {
                            "total": 1,
                            "additions": 28,
                            "deletions": 27,
                            "files": [
                                {
                                    "sha": "05c3f32b72f34d00a3d3f8566bdb799c91f1fb9f",
                                    "filename": "jax/__init__.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 1,
                                    "changes": 9,
                                    "blob_url": "https://github.com/google/jax/blob/f317943f5671b0b1da3f91e71405b8f66e1c8fcc/jax%2F__init__.py",
                                    "raw_url": "https://github.com/google/jax/raw/f317943f5671b0b1da3f91e71405b8f66e1c8fcc/jax%2F__init__.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F__init__.py?ref=f317943f5671b0b1da3f91e71405b8f66e1c8fcc",
                                    "patch": "@@ -163,4 +163,11 @@\n \n import jax.lib  # TODO(phawkins): remove this export.\n \n-del jax._src\n+if hasattr(jax, '_src'):\n+  del jax._src\n+else:\n+  from warnings import warn as _warn\n+  _warn(\"The jax module appears to have been reloaded within the python process. \"\n+        \"This is not well-supported and can cause unpredictable side-effects. \"\n+        \"For information see https://github.com/google/jax/issues/13857.\")\n+  del _warn"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c1a09194dc49ee13bceca68ec67eaeb9e952666d",
                                    "filename": "tests/api_test.py",
                                    "status": "modified",
                                    "additions": 19,
                                    "deletions": 0,
                                    "changes": 19,
                                    "blob_url": "https://github.com/google/jax/blob/f317943f5671b0b1da3f91e71405b8f66e1c8fcc/tests%2Fapi_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/f317943f5671b0b1da3f91e71405b8f66e1c8fcc/tests%2Fapi_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fapi_test.py?ref=f317943f5671b0b1da3f91e71405b8f66e1c8fcc",
                                    "patch": "@@ -4124,6 +4124,25 @@ def foo(x):\n     with self.assertRaisesRegex(TypeError, \"applied to foo\"):\n       f_vjp(1.0, 1.0)\n \n+  @unittest.skipIf(not sys.executable, \"test requires sys.executable\")\n+  @jtu.skip_on_devices(\"gpu\", \"tpu\")\n+  def test_jax_reload_warning(self):\n+    # Regression test for https://github.com/google/jax/issues/13857\n+    should_not_warn = \"import jax\"\n+    should_warn = (\n+      \"import jax;\"\n+      \"import importlib;\"\n+      \"importlib.reload(jax)\")\n+    expected = \"The jax module appears to have been reloaded within the python process\"\n+\n+    result = subprocess.run([sys.executable, '-c', should_not_warn],\n+                            check=True, capture_output=True)\n+    assert expected not in result.stderr.decode()\n+\n+    result = subprocess.run([sys.executable, '-c', should_warn],\n+                            check=True, capture_output=True)\n+    assert expected in result.stderr.decode()\n+\n \n @jtu.with_config(jax_experimental_subjaxpr_lowering_cache=True)\n class SubcallTraceCacheTest(jtu.JaxTestCase):"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/api_test.py"
            },
            {
                "id": 11965,
                "created_at": "2022-08-17T17:52:03Z",
                "closed_at": "2023-01-10T00:57:37Z",
                "title": "BUG: numpy and jax differ on modulus by zero",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "7788f0cf6bd013052ffb73aa91bee0752d909733",
                        "commit_date": "2023-01-10T00:57:32Z",
                        "parents": "849af498d1f2dfc89b2b70eb16766b60769a979e",
                        "stat": {
                            "total": 2,
                            "additions": 6,
                            "deletions": 4,
                            "files": [
                                {
                                    "sha": "cf43f886810f58ad580f37c0a8b195cceb0f400d",
                                    "filename": "jax/_src/numpy/ufuncs.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/7788f0cf6bd013052ffb73aa91bee0752d909733/jax%2F_src%2Fnumpy%2Fufuncs.py",
                                    "raw_url": "https://github.com/google/jax/raw/7788f0cf6bd013052ffb73aa91bee0752d909733/jax%2F_src%2Fnumpy%2Fufuncs.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fnumpy%2Fufuncs.py?ref=7788f0cf6bd013052ffb73aa91bee0752d909733",
                                    "patch": "@@ -550,6 +550,8 @@ def frexp(x: ArrayLike, /) -> Tuple[Array, Array]:\n def remainder(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n   x1, x2 = _promote_args_numeric(\"remainder\", x1, x2)\n   zero = _constant_like(x1, 0)\n+  if dtypes.issubdtype(x2.dtype, np.integer):\n+    x2 = _where(x2 == 0, lax_internal._ones(x2), x2)\n   trunc_mod = lax.rem(x1, x2)\n   trunc_mod_not_zero = lax.ne(trunc_mod, zero)\n   do_plus = lax.bitwise_and("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "5888cea1fcc9898a755dc47fa5a5e920b1c6607a",
                                    "filename": "tests/lax_numpy_operators_test.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 2,
                                    "changes": 4,
                                    "blob_url": "https://github.com/google/jax/blob/7788f0cf6bd013052ffb73aa91bee0752d909733/tests%2Flax_numpy_operators_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/7788f0cf6bd013052ffb73aa91bee0752d909733/tests%2Flax_numpy_operators_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_numpy_operators_test.py?ref=7788f0cf6bd013052ffb73aa91bee0752d909733",
                                    "patch": "@@ -240,9 +240,9 @@ def op_record(name, nargs, dtypes, shapes, rng_factory, diff_modes,\n     op_record(\"rad2deg\", 1, float_dtypes, all_shapes, jtu.rand_default, []),\n     op_record(\"ravel\", 1, all_dtypes, all_shapes, jtu.rand_default, [\"rev\"]),\n     op_record(\"real\", 1, number_dtypes, all_shapes, jtu.rand_some_inf, []),\n-    op_record(\"remainder\", 2, default_dtypes, all_shapes, jtu.rand_nonzero, [],\n+    op_record(\"remainder\", 2, default_dtypes, all_shapes, jtu.rand_some_zero, [],\n               tolerance={np.float16: 1e-2}),\n-    op_record(\"mod\", 2, default_dtypes, all_shapes, jtu.rand_nonzero, []),\n+    op_record(\"mod\", 2, default_dtypes, all_shapes, jtu.rand_some_zero, []),\n     op_record(\"modf\", 1, float_dtypes, all_shapes, jtu.rand_default, []),\n     op_record(\"modf\", 1, int_dtypes + unsigned_dtypes, all_shapes,\n               jtu.rand_default, [], check_dtypes=False),"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_numpy_operators_test.py"
            },
            {
                "id": 11010,
                "created_at": "2022-06-07T08:31:18Z",
                "closed_at": "2022-06-23T13:27:48Z",
                "title": "jax.random.uniform raises an `InconclusiveDimensionOperation` for very large shapes",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "4b03ebf4f5e774bad9865278f9f0f8b75948e647",
                        "commit_date": "2022-06-20T08:48:15Z",
                        "parents": "b50d77cc983e616beac6025f56a370eedfa3369e",
                        "stat": {
                            "total": 1,
                            "additions": 10,
                            "deletions": 9,
                            "files": [
                                {
                                    "sha": "8a7153ec7b62ef33c5b25deb90144dd18f4d763a",
                                    "filename": "jax/_src/prng.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 1,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/4b03ebf4f5e774bad9865278f9f0f8b75948e647/jax%2F_src%2Fprng.py",
                                    "raw_url": "https://github.com/google/jax/raw/4b03ebf4f5e774bad9865278f9f0f8b75948e647/jax%2F_src%2Fprng.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fprng.py?ref=4b03ebf4f5e774bad9865278f9f0f8b75948e647",
                                    "patch": "@@ -542,7 +542,7 @@ def threefry_random_bits(key: jnp.ndarray, bit_width, shape):\n         )\n       )\n     )\n-    bits = lax.reshape(bits, (np.uint32(max_count * 32 // bit_width),), (1, 0))\n+    bits = lax.reshape(bits, ((max_count * 32 // bit_width),), (1, 0))\n     bits = lax.convert_element_type(bits, dtype)[:size]\n   return lax.reshape(bits, shape)\n "
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "07e418c397d7cfb2642828a6c07e57e306997f94",
                                    "filename": "tests/random_test.py",
                                    "status": "modified",
                                    "additions": 8,
                                    "deletions": 0,
                                    "changes": 8,
                                    "blob_url": "https://github.com/google/jax/blob/4b03ebf4f5e774bad9865278f9f0f8b75948e647/tests%2Frandom_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/4b03ebf4f5e774bad9865278f9f0f8b75948e647/tests%2Frandom_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Frandom_test.py?ref=4b03ebf4f5e774bad9865278f9f0f8b75948e647",
                                    "patch": "@@ -1454,6 +1454,14 @@ def test_randint_out_of_range(self):\n     self.assertGreater((r == 0).sum(), 0)\n     self.assertGreater((r == 255).sum(), 0)\n \n+  def test_large_prng(self):\n+    # https://github.com/google/jax/issues/11010\n+    def f():\n+      return jax.random.uniform(jax.random.PRNGKey(3), (308000000, 128), dtype=jnp.bfloat16)\n+\n+    # just lower, don't run, takes too long\n+    jax.jit(f).lower()\n+\n \n threefry_seed = jax._src.prng.threefry_seed\n threefry_split = jax._src.prng.threefry_split"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/random_test.py"
            },
            {
                "id": 10834,
                "created_at": "2022-05-26T02:23:52Z",
                "closed_at": "2022-06-01T17:24:18Z",
                "title": "`lax.reduce_window` and its derivatives fail on scalar inputs",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "7306694a09727515281d1cc79666fb56d6307574",
                        "commit_date": "2022-06-01T17:24:13Z",
                        "parents": "e225317ff8d05aee2a315cc0fef744d23bd05df2",
                        "stat": {
                            "total": 5,
                            "additions": 34,
                            "deletions": 29,
                            "files": [
                                {
                                    "sha": "341f9a8af33305ff576c71a40831f67ef78e677c",
                                    "filename": "jax/_src/lax/windowed_reductions.py",
                                    "status": "modified",
                                    "additions": 9,
                                    "deletions": 5,
                                    "changes": 14,
                                    "blob_url": "https://github.com/google/jax/blob/7306694a09727515281d1cc79666fb56d6307574/jax%2F_src%2Flax%2Fwindowed_reductions.py",
                                    "raw_url": "https://github.com/google/jax/raw/7306694a09727515281d1cc79666fb56d6307574/jax%2F_src%2Flax%2Fwindowed_reductions.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Flax%2Fwindowed_reductions.py?ref=7306694a09727515281d1cc79666fb56d6307574",
                                    "patch": "@@ -304,7 +304,8 @@ def _generic_reduce_window_lower(ctx, *args, jaxpr, consts,\n       window_strides=mlir.dense_int_elements(window_strides),\n       base_dilations=mlir.dense_int_elements(base_dilation),\n       window_dilations=mlir.dense_int_elements(window_dilation),\n-      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64)))\n+      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n+                                          shape=(len(padding), 2)))\n   reducer = rw.regions[0].blocks.append(*(scalar_types + scalar_types))\n   with ir.InsertionPoint(reducer):\n     if jaxpr.effects:\n@@ -417,7 +418,7 @@ def reduce_window_shape_tuple(operand_shape, window_dimensions, window_strides,\n     operand_shape = lax._dilate_shape(operand_shape, base_dilation)\n   if window_dilation is not None:\n     window_dimensions = lax._dilate_shape(window_dimensions, window_dilation)\n-  pads_lo, pads_hi = zip(*padding)\n+  pads_lo, pads_hi = [(), ()] if len(padding) == 0 else zip(*padding)\n   operand_padded = core.sum_shapes(operand_shape, pads_lo, pads_hi)\n   return core.stride_shape(operand_padded, window_dimensions, window_strides)\n \n@@ -453,7 +454,8 @@ def _reduce_window_lower(\n       window_strides=mlir.dense_int_elements(window_strides),\n       base_dilations=mlir.dense_int_elements(base_dilation),\n       window_dilations=mlir.dense_int_elements(window_dilation),\n-      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64)))\n+      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n+                                          shape=(len(padding), 2)))\n   reducer = rw.regions[0].blocks.append(scalar_type, scalar_type)\n   with ir.InsertionPoint(reducer):\n     mhlo.ReturnOp(reduce_op(*reducer.arguments))\n@@ -498,7 +500,8 @@ def _select_and_scatter_lower(\n       init_value,\n       window_dimensions=mlir.dense_int_elements(window_dimensions),\n       window_strides=mlir.dense_int_elements(window_strides),\n-      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64)))\n+      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n+                                          shape=(len(padding), 2)))\n   select = op.select.blocks.append(scalar_type, scalar_type)\n   with ir.InsertionPoint(select):\n     if select_jaxpr.effects:\n@@ -743,7 +746,8 @@ def snd(t):\n       window_strides=mlir.dense_int_elements(window_strides),\n       base_dilations=mlir.dense_int_elements(base_dilation),\n       window_dilations=mlir.dense_int_elements(window_dilation),\n-      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64)))\n+      padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64),\n+                                          shape=(len(padding), 2)))\n   scalar_type = ir.RankedTensorType.get([], double_word_type)\n   reducer = rw.regions[0].blocks.append(scalar_type, scalar_type)\n   with ir.InsertionPoint(reducer):"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "70c7b895100f35744324a10d277056ceeee16ac7",
                                    "filename": "tests/lax_test.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/google/jax/blob/7306694a09727515281d1cc79666fb56d6307574/tests%2Flax_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/7306694a09727515281d1cc79666fb56d6307574/tests%2Flax_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_test.py?ref=7306694a09727515281d1cc79666fb56d6307574",
                                    "patch": "@@ -1756,6 +1756,26 @@ def testReduceWeakType(self, op_namespace, op, arr_weak_type, init_weak_type):\n     out_jit = jax.jit(fun)(arr, init)\n     self.assertEqual(dtypes.is_weakly_typed(out_jit), arr_weak_type and init_weak_type)\n \n+  def testReduceWindowScalar(self):\n+    rng = jtu.rand_small(self.rng())\n+    dtype = jnp.float32\n+    init_val = np.asarray(0, dtype=dtype)\n+    op = lax.add\n+\n+    def fun(operand, init_val):\n+      return lax.reduce_window(\n+          operand, init_val, op, window_dimensions=(), window_strides=(),\n+          padding=(), base_dilation=(), window_dilation=())\n+\n+    def reference_fun(operand, init_val):\n+      return lax_reference.reduce_window(\n+          operand, init_val, op, window_dimensions=(), window_strides=(),\n+          padding=(), base_dilation=())\n+\n+    args_maker = lambda: [rng((), dtype), init_val]\n+    self._CompileAndCheck(fun, args_maker)\n+    self._CheckAgainstNumpy(reference_fun, fun, args_maker)\n+\n   @parameterized.named_parameters(jtu.cases_from_list(\n       {\"testcase_name\": (\"_op={}_shape={}_dims={}_strides={}_padding={}\"\n                          \"_basedilation={}_windowdilation={}\")"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_test.py"
            },
            {
                "id": 10729,
                "created_at": "2022-05-16T18:52:29Z",
                "closed_at": "2022-05-17T13:27:57Z",
                "title": "Suspected bug in jax.lax.conv_general_dilated_patches",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "44f1e05a76def0adb9ab08d01e145597cb977010",
                        "commit_date": "2022-05-17T12:52:37Z",
                        "parents": "744f6b4ee802c7475285d5fa07666847a01b8705",
                        "stat": {
                            "total": 0,
                            "additions": 17,
                            "deletions": 17,
                            "files": [
                                {
                                    "sha": "a8738301a325ecf61418dc7919d7019093353d86",
                                    "filename": "jax/_src/lax/convolution.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/google/jax/blob/44f1e05a76def0adb9ab08d01e145597cb977010/jax%2F_src%2Flax%2Fconvolution.py",
                                    "raw_url": "https://github.com/google/jax/raw/44f1e05a76def0adb9ab08d01e145597cb977010/jax%2F_src%2Flax%2Fconvolution.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Flax%2Fconvolution.py?ref=44f1e05a76def0adb9ab08d01e145597cb977010",
                                    "patch": "@@ -14,6 +14,7 @@\n \n import builtins\n from functools import partial\n+import operator\n from typing import Any, List, NamedTuple, Optional, Sequence, Tuple, Union\n \n import numpy as np\n@@ -141,6 +142,15 @@ def conv_general_dilated(\n     padding = lax.padtype_to_pads(\n         np.take(lhs.shape, lhs_perm)[2:], effective_rhs_shape,  # type: ignore[index]\n         window_strides, padding)\n+  else:\n+    try:\n+      padding = tuple((operator.index(lo), operator.index(hi))\n+                      for lo, hi in padding)\n+    except (ValueError, TypeError) as e:\n+      raise ValueError(\n+        \"padding argument to conv_general_dilated should be a string or a \"\n+        f\"sequence of (low, high) pairs, got {padding}\") from e\n+\n   preferred_element_type = (\n       None if preferred_element_type is None else\n       dtypes.canonicalize_dtype(np.dtype(preferred_element_type)))"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "979d8e4f3ba737a5a4c1480d6bd5c9e264f4698a",
                                    "filename": "tests/lax_test.py",
                                    "status": "modified",
                                    "additions": 7,
                                    "deletions": 0,
                                    "changes": 7,
                                    "blob_url": "https://github.com/google/jax/blob/44f1e05a76def0adb9ab08d01e145597cb977010/tests%2Flax_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/44f1e05a76def0adb9ab08d01e145597cb977010/tests%2Flax_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_test.py?ref=44f1e05a76def0adb9ab08d01e145597cb977010",
                                    "patch": "@@ -1060,6 +1060,13 @@ def testConvTransposePaddingList(self):\n     c = lax.conv_general_dilated(a[None, None], b[None, None], (1,1), [(0,0),(0,0)], (1,1))\n     self.assertAllClose(c, 9 * jnp.ones((1, 1, 26, 26)))\n \n+  def testConvInvalidPadding(self):\n+    x = jnp.ones((1, 10, 10, 5), dtype=jnp.bfloat16)\n+    with self.assertRaisesRegex(ValueError,\n+                                r\"padding argument.*, got \\(3, 3\\)\"):\n+      jax.lax.conv_general_dilated_patches(x, (5, 5), window_strides=(1, 1),\n+                                           padding=(3, 3))\n+\n   @parameterized.named_parameters(jtu.cases_from_list(\n       {\"testcase_name\": \"_lhs_shape={}_rhs_shape={}_precision={}\".format(\n           jtu.format_shape_dtype_string(lhs_shape, dtype),"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_test.py"
            },
            {
                "id": 10315,
                "created_at": "2022-04-15T21:03:15Z",
                "closed_at": "2022-04-19T17:38:17Z",
                "title": "Low-level error in forward-mode AD of `lax.reduce_window_sum_p`",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "b8162b0b34264076d86905df47c830353da654ce",
                        "commit_date": "2022-04-19T16:36:31Z",
                        "parents": "40805e4f1843804baa1a900821128d57c6d4ab83",
                        "stat": {
                            "total": 1,
                            "additions": 18,
                            "deletions": 17,
                            "files": [
                                {
                                    "sha": "2170cf972ff7977613c629b563ea3037a8f6487f",
                                    "filename": "jax/core.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 1,
                                    "changes": 6,
                                    "blob_url": "https://github.com/google/jax/blob/b8162b0b34264076d86905df47c830353da654ce/jax%2Fcore.py",
                                    "raw_url": "https://github.com/google/jax/raw/b8162b0b34264076d86905df47c830353da654ce/jax%2Fcore.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2Fcore.py?ref=b8162b0b34264076d86905df47c830353da654ce",
                                    "patch": "@@ -1511,7 +1511,11 @@ def divide_shape_sizes(self, s1: Shape, s2: Shape) -> DimSize:\n     return sz1 // sz2\n \n   def stride(self, d: DimSize, window_size: DimSize, window_stride: DimSize) -> DimSize:\n-    \"\"\"(d - window_size) // window_stride + 1\"\"\"\n+    \"\"\"(d - window_size) // window_stride + 1.\n+\n+    If d == 0 or window_size > d, returns 0.\n+    \"\"\"\n+    if d == 0 or window_size > d: return 0\n     return (d - window_size) // window_stride + 1\n \n   def dilate(self, d: DimSize, dilation: int) -> DimSize:"
                                },
                                {
                                    "sha": "4465fea2bda7ac546efe83d9c937c3113f4eb6e3",
                                    "filename": "jax/experimental/jax2tf/shape_poly.py",
                                    "status": "modified",
                                    "additions": 1,
                                    "deletions": 0,
                                    "changes": 1,
                                    "blob_url": "https://github.com/google/jax/blob/b8162b0b34264076d86905df47c830353da654ce/jax%2Fexperimental%2Fjax2tf%2Fshape_poly.py",
                                    "raw_url": "https://github.com/google/jax/raw/b8162b0b34264076d86905df47c830353da654ce/jax%2Fexperimental%2Fjax2tf%2Fshape_poly.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2Fexperimental%2Fjax2tf%2Fshape_poly.py?ref=b8162b0b34264076d86905df47c830353da654ce",
                                    "patch": "@@ -442,6 +442,7 @@ def divide_shape_sizes(self, s1: Shape, s2: Shape) -> DimSize:\n   def stride(self, d: DimSize, window_size: DimSize, window_stride: DimSize) -> DimSize:\n     \"\"\"Implements `(d - window_size) // window_stride + 1`\"\"\"\n     try:\n+      # TODO(necula): check for d == 0 or window_size > d and return 0.\n       q, r = _ensure_poly(d - window_size).divmod(window_stride)\n       return q + 1\n     except InconclusiveDimensionOperation as e:"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "df3cd35086dfacd3fc8857ac2e8790e006ea911f",
                                    "filename": "tests/lax_test.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/google/jax/blob/b8162b0b34264076d86905df47c830353da654ce/tests%2Flax_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/b8162b0b34264076d86905df47c830353da654ce/tests%2Flax_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_test.py?ref=b8162b0b34264076d86905df47c830353da654ce",
                                    "patch": "@@ -1931,6 +1931,17 @@ def testReduceWindowShapeDilation(self, shape, window_dimensions,\n     # to https://www.tensorflow.org/xla/operation_semantics#reducewindow.\n     self.assertEqual(shape, result.shape)\n \n+  def testReduceWindowWithEmptyOutput(self):\n+    # https://github.com/google/jax/issues/10315\n+    shape = (5, 3, 2)\n+    operand, padding, strides = np.ones(shape), 'VALID', (1,) * len(shape)\n+    out = jax.eval_shape(lambda x: lax.reduce_window(x, 0., lax.add, padding=padding,\n+                         window_strides=strides,\n+                         window_dimensions=(3, 1, 1),\n+                         window_dilation=(3, 1, 1)), operand)\n+    self.assertEqual((0, 3, 2), out.shape)\n+\n+\n   @parameterized.named_parameters(jtu.cases_from_list(\n       {\"testcase_name\": \"_op={}_shape={}_axis={}_reverse={}\"\n        .format(op.__name__, jtu.format_shape_dtype_string(shape, dtype), axis,"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_test.py"
            },
            {
                "id": 9969,
                "created_at": "2022-03-19T15:13:10Z",
                "closed_at": "2022-03-21T18:29:01Z",
                "title": "jax.hessian uses positional argument `holomorphic` where it should be `has_aux` ",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "e5b3f0b537ae10ca9acceccc849b76254838156b",
                        "commit_date": "2022-03-21T16:52:57Z",
                        "parents": "1ffa285bd641a12385bf609b21bf72df1b13782e",
                        "stat": {
                            "total": 2,
                            "additions": 28,
                            "deletions": 26,
                            "files": [
                                {
                                    "sha": "f00831b3b5b215e92c4fe65aab6a3f047078e3e0",
                                    "filename": "jax/_src/api.py",
                                    "status": "modified",
                                    "additions": 6,
                                    "deletions": 2,
                                    "changes": 8,
                                    "blob_url": "https://github.com/google/jax/blob/e5b3f0b537ae10ca9acceccc849b76254838156b/jax%2F_src%2Fapi.py",
                                    "raw_url": "https://github.com/google/jax/raw/e5b3f0b537ae10ca9acceccc849b76254838156b/jax%2F_src%2Fapi.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fapi.py?ref=e5b3f0b537ae10ca9acceccc849b76254838156b",
                                    "patch": "@@ -1158,7 +1158,7 @@ def jacfun(*args, **kwargs):\n \n \n def hessian(fun: Callable, argnums: Union[int, Sequence[int]] = 0,\n-            holomorphic: bool = False) -> Callable:\n+            has_aux: bool = False, holomorphic: bool = False) -> Callable:\n   \"\"\"Hessian of ``fun`` as a dense array.\n \n   Args:\n@@ -1168,6 +1168,9 @@ def hessian(fun: Callable, argnums: Union[int, Sequence[int]] = 0,\n       containers thereof.\n     argnums: Optional, integer or sequence of integers. Specifies which\n       positional argument(s) to differentiate with respect to (default ``0``).\n+    has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\n+      first element is considered the output of the mathematical function to be\n+      differentiated and the second element is auxiliary data. Default False.\n     holomorphic: Optional, bool. Indicates whether ``fun`` is promised to be\n       holomorphic. Default False.\n \n@@ -1218,7 +1221,8 @@ def hessian(fun: Callable, argnums: Union[int, Sequence[int]] = 0,\n   ``(out1, out2, ..., in1, in2, ..., in1, in2, ...)``. To flatten pytrees into\n   1D vectors, consider using :py:func:`jax.flatten_util.flatten_pytree`.\n   \"\"\"\n-  return jacfwd(jacrev(fun, argnums, holomorphic), argnums, holomorphic)\n+  return jacfwd(jacrev(fun, argnums, has_aux=has_aux, holomorphic=holomorphic),\n+                argnums, has_aux=has_aux, holomorphic=holomorphic)\n \n def _std_basis(pytree):\n   leaves, _ = tree_flatten(pytree)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "8e4ef94e44069c92c4b610e543d894dacb33e364",
                                    "filename": "tests/api_test.py",
                                    "status": "modified",
                                    "additions": 20,
                                    "deletions": 0,
                                    "changes": 20,
                                    "blob_url": "https://github.com/google/jax/blob/e5b3f0b537ae10ca9acceccc849b76254838156b/tests%2Fapi_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/e5b3f0b537ae10ca9acceccc849b76254838156b/tests%2Fapi_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fapi_test.py?ref=e5b3f0b537ae10ca9acceccc849b76254838156b",
                                    "patch": "@@ -1217,6 +1217,26 @@ def test_hessian(self):\n     f = lambda x: jnp.dot(x, jnp.dot(A, x))\n     assert np.allclose(hessian(f)(x), A + A.T)\n \n+  @jtu.skip_on_devices(\"tpu\")\n+  def test_hessian_holomorphic(self):\n+    R = self.rng().randn\n+    A = R(4, 4)\n+    x = R(4) * (1 + 2j)\n+\n+    f = lambda x: jnp.dot(x, jnp.dot(A, x))\n+    assert np.allclose(hessian(f, holomorphic=True)(x), A + A.T)\n+\n+  @jtu.skip_on_devices(\"tpu\")\n+  def test_hessian_aux(self):\n+    R = self.rng().randn\n+    A = R(4, 4)\n+    x = R(4)\n+\n+    f = lambda x: (jnp.dot(x, jnp.dot(A, x)), x)\n+    h, aux = hessian(f, has_aux=True)(x)\n+    assert np.allclose(h, A + A.T)\n+    assert np.allclose(aux, x)\n+\n   def test_std_basis(self):\n     basis = api._std_basis(jnp.zeros(3))\n     assert getattr(basis, \"shape\", None) == (3, 3)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/api_test.py"
            },
            {
                "id": 9438,
                "created_at": "2022-02-03T20:34:13Z",
                "closed_at": "2022-02-14T17:44:19Z",
                "title": "TypeError: No constant handler for type: <class 'jax.core.Token'>",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "5a259925a05c8ff095bb57b0ddfc8b5353cf52dc",
                        "commit_date": "2022-02-14T17:09:29Z",
                        "parents": "fb821e94ad64fc73d856895d5f497d7139b9b5b5",
                        "stat": {
                            "total": 0,
                            "additions": 9,
                            "deletions": 9,
                            "files": [
                                {
                                    "sha": "771ad7d4e87706c523e5e67f1b10cc24e0d5241e",
                                    "filename": "jax/interpreters/mlir.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/jax%2Finterpreters%2Fmlir.py",
                                    "raw_url": "https://github.com/google/jax/raw/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/jax%2Finterpreters%2Fmlir.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2Finterpreters%2Fmlir.py?ref=5a259925a05c8ff095bb57b0ddfc8b5353cf52dc",
                                    "patch": "@@ -258,6 +258,8 @@ def _device_array_constant_handler(val, canonicalize_types):\n for t in device_array.device_array_types:\n   register_constant_handler(t, _device_array_constant_handler)\n \n+register_constant_handler(\n+  core.Token, lambda _, __: [mhlo.CreateTokenOp(mhlo.TokenType.get())])\n \n # Source locations\n "
                                },
                                {
                                    "sha": "60641895815b1535d2d586263ffd001c3374c19b",
                                    "filename": "jax/interpreters/xla.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 0,
                                    "changes": 2,
                                    "blob_url": "https://github.com/google/jax/blob/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/jax%2Finterpreters%2Fxla.py",
                                    "raw_url": "https://github.com/google/jax/raw/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/jax%2Finterpreters%2Fxla.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2Finterpreters%2Fxla.py?ref=5a259925a05c8ff095bb57b0ddfc8b5353cf52dc",
                                    "patch": "@@ -374,6 +374,8 @@ def _device_array_constant_handler(c, val, canonicalize_types=True):\n   register_constant_handler(t, _device_array_constant_handler)\n \n \n+register_constant_handler(core.Token, lambda c, _, __: [xops.CreateToken(c)])\n+\n # TODO(mattjj): try to remove this canonicalize_dtype stuff\n def canonicalize_dtype(x):\n   typ = type(x)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "c4d012b5a7c414f8e34fa3f8e9a5688a6d9a74c6",
                                    "filename": "tests/api_test.py",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/google/jax/blob/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/tests%2Fapi_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/5a259925a05c8ff095bb57b0ddfc8b5353cf52dc/tests%2Fapi_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fapi_test.py?ref=5a259925a05c8ff095bb57b0ddfc8b5353cf52dc",
                                    "patch": "@@ -2959,6 +2959,11 @@ def test_jit_returning_token(self):\n     x = jax.jit(jax.lax.create_token)(1.0)\n     self.assertIsInstance(x, jax.core.Token)\n \n+  def test_jit_capturing_token(self):\n+    tok = jax.core.token\n+    _, y = jax.jit(lambda x: (x + 2, tok))(7)\n+    self.assertIsInstance(y, jax.core.Token)\n+\n   def test_leak_checker_catches_a_jit_leak(self):\n     with jax.checking_leaks():\n       lst = []"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/api_test.py"
            },
            {
                "id": 9429,
                "created_at": "2022-02-03T13:54:25Z",
                "closed_at": "2022-02-03T16:16:22Z",
                "title": "jax.tree_util.Partial is not hash-stable in jax>=0.2.22",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "042c9bd7a5e42e8ceca7eea65b1858f39704be87",
                        "commit_date": "2022-02-03T15:44:13Z",
                        "parents": "4432f473133f4bb150bf11e2ce5b8302549bdb0d",
                        "stat": {
                            "total": 5,
                            "additions": 41,
                            "deletions": 36,
                            "files": [
                                {
                                    "sha": "3b5295f59a706a458bba29832a75093477e3f1b3",
                                    "filename": "jax/_src/tree_util.py",
                                    "status": "modified",
                                    "additions": 26,
                                    "deletions": 5,
                                    "changes": 31,
                                    "blob_url": "https://github.com/google/jax/blob/042c9bd7a5e42e8ceca7eea65b1858f39704be87/jax%2F_src%2Ftree_util.py",
                                    "raw_url": "https://github.com/google/jax/raw/042c9bd7a5e42e8ceca7eea65b1858f39704be87/jax%2F_src%2Ftree_util.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Ftree_util.py?ref=042c9bd7a5e42e8ceca7eea65b1858f39704be87",
                                    "patch": "@@ -267,6 +267,24 @@ def tree_all(tree):\n   lambda s, values: collections.defaultdict(s[0], safe_zip(s[1], values)))  # type: ignore[index]\n \n \n+\n+class _HashableCallableShim:\n+  \"\"\"Object that delegates __call__, __hash__, and __eq__ to another object.\"\"\"\n+  def __init__(self, fun):\n+    self.fun = fun\n+\n+  def __call__(self, *args, **kw):\n+    return self.fun(*args, **kw)\n+\n+  def __hash__(self):\n+    return hash(self.fun)\n+\n+  def __eq__(self, other):\n+    if isinstance(other, _HashableCallableShim):\n+      return self.fun == other.fun\n+    return self.fun == other\n+\n+\n class Partial(functools.partial):\n   \"\"\"A version of functools.partial that works in pytrees.\n \n@@ -318,16 +336,19 @@ class Partial(functools.partial):\n   def __new__(klass, func, *args, **kw):\n     # In Python 3.10+, if func is itself a functools.partial instance,\n     # functools.partial.__new__ would merge the arguments of this Partial\n-    # instance with the arguments of the func. We box func in another lambda to\n-    # avoid this optimization since it would change which arguments are\n-    # considered part of the pytree.\n+    # instance with the arguments of the func. We box func in a class that does\n+    # not (yet) have a `func` attribute to defeat this optimization, since we\n+    # care exactly which arguments are considered part of the pytree.\n     if isinstance(func, functools.partial):\n       original_func = func\n-      func = lambda *args, **kw: original_func(*args, **kw)\n+      func = _HashableCallableShim(original_func)\n+      out = super(Partial, klass).__new__(klass, func, *args, **kw)\n       func.func = original_func.func\n       func.args = original_func.args\n       func.keywords = original_func.keywords\n-    return super(Partial, klass).__new__(klass, func, *args, **kw)\n+      return out\n+    else:\n+      return super(Partial, klass).__new__(klass, func, *args, **kw)\n \n \n register_pytree_node("
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "b64b4758a836d5cf9c5b60895a9a281b40e15872",
                                    "filename": "tests/tree_util_test.py",
                                    "status": "modified",
                                    "additions": 10,
                                    "deletions": 0,
                                    "changes": 10,
                                    "blob_url": "https://github.com/google/jax/blob/042c9bd7a5e42e8ceca7eea65b1858f39704be87/tests%2Ftree_util_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/042c9bd7a5e42e8ceca7eea65b1858f39704be87/tests%2Ftree_util_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Ftree_util_test.py?ref=042c9bd7a5e42e8ceca7eea65b1858f39704be87",
                                    "patch": "@@ -200,6 +200,16 @@ def f(a, b, c): pass\n     h = tree_util.Partial(g, 3)\n     self.assertEqual(h.args, (3,))\n \n+  def testPartialFuncAttributeHasStableHash(self):\n+    # https://github.com/google/jax/issues/9429\n+    fun = functools.partial(print, 1)\n+    p1 = tree_util.Partial(fun, 2)\n+    p2 = tree_util.Partial(fun, 2)\n+    self.assertEqual(fun, p1.func)\n+    self.assertEqual(p1.func, fun)\n+    self.assertEqual(p1.func, p2.func)\n+    self.assertEqual(hash(p1.func), hash(p2.func))\n+\n   @parameterized.parameters(*(TREES + LEAVES))\n   def testRoundtripViaBuild(self, inputs):\n     xs, tree = _process_pytree(tuple, inputs)"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/tree_util_test.py"
            },
            {
                "id": 8688,
                "created_at": "2021-11-24T19:14:57Z",
                "closed_at": "2021-12-14T19:50:00Z",
                "title": "Unexpected assertion error for gradient of ppermute.",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "c555f5f0e44224c65b356b63a9d68983ec14e5fa",
                        "commit_date": "2021-12-14T18:42:05Z",
                        "parents": "006109030c61a71477bc3245fa44bd4039c1c320",
                        "stat": {
                            "total": 1,
                            "additions": 16,
                            "deletions": 15,
                            "files": [
                                {
                                    "sha": "6a123f721657acad40d64c0d8e25fc65202f16fa",
                                    "filename": "jax/_src/lax/parallel.py",
                                    "status": "modified",
                                    "additions": 2,
                                    "deletions": 1,
                                    "changes": 3,
                                    "blob_url": "https://github.com/google/jax/blob/c555f5f0e44224c65b356b63a9d68983ec14e5fa/jax%2F_src%2Flax%2Fparallel.py",
                                    "raw_url": "https://github.com/google/jax/raw/c555f5f0e44224c65b356b63a9d68983ec14e5fa/jax%2F_src%2Flax%2Fparallel.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Flax%2Fparallel.py?ref=c555f5f0e44224c65b356b63a9d68983ec14e5fa",
                                    "patch": "@@ -793,7 +793,8 @@ def _ppermute_batcher(axis_size, frame_name, _, vals_in, dims_in, axis_name, per\n     raise NotImplementedError(\"ppermute batcher only supports a single axis\")\n   assert axis_name[0] == frame_name, \"ppermute batcher called with a wrong axis!\"\n   assert len(perm) == axis_size, \"Permutation doesn't match the axis size!\"\n-  assert d is not batching.not_mapped\n+  if d is batching.not_mapped:\n+    return v, d\n   perm_indices = np.zeros(axis_size, dtype=int)\n   for src, dst in perm:\n     perm_indices[dst] = src"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "bb6f2a72d66aebe25ad7247070dd56fe15f30844",
                                    "filename": "tests/batching_test.py",
                                    "status": "modified",
                                    "additions": 13,
                                    "deletions": 0,
                                    "changes": 13,
                                    "blob_url": "https://github.com/google/jax/blob/c555f5f0e44224c65b356b63a9d68983ec14e5fa/tests%2Fbatching_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/c555f5f0e44224c65b356b63a9d68983ec14e5fa/tests%2Fbatching_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Fbatching_test.py?ref=c555f5f0e44224c65b356b63a9d68983ec14e5fa",
                                    "patch": "@@ -1267,6 +1267,19 @@ def f(x):\n     self.assertEqual(f(jnp.ones(3)).shape, (3,))\n     self.assertEqual(jax.vmap(f)(jnp.ones((2, 3))).shape, (2, 3))\n \n+  def testPpermuteBatcherTrivial(self):\n+    # https://github.com/google/jax/issues/8688\n+    def ppermute(input):\n+      return jax.lax.ppermute(input, axis_name=\"i\", perm=[[0, 1], [1, 0]])\n+\n+    grad_fn = jax.grad(ppermute)\n+\n+    vmapped_gradients_fn = jax.vmap(grad_fn, axis_name=\"i\")\n+\n+    vector = jax.numpy.array([1., 2.])\n+    ans = vmapped_gradients_fn(vector)  # doesn't crash\n+    self.assertAllClose(ans, jnp.ones(2), check_dtypes=False)\n+\n \n Array = Any\n ArrayElt = Any"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/batching_test.py"
            },
            {
                "id": 8634,
                "created_at": "2021-11-20T13:36:20Z",
                "closed_at": "2021-11-22T21:33:02Z",
                "title": "Incorrect gradient for segment sum",
                "labels": "bug",
                "text_based": false,
                "commits": [
                    {
                        "hash": "4679f455f976c823851312438d766f8ffecac0e8",
                        "commit_date": "2021-11-22T21:32:58Z",
                        "parents": "5415306257faede98f6c45f0ce67aeb1cd26b8ff",
                        "stat": {
                            "total": 16,
                            "additions": 69,
                            "deletions": 53,
                            "files": [
                                {
                                    "sha": "7de6df4994eccadf7cb878f2b666e96e376ce191",
                                    "filename": "CHANGELOG.md",
                                    "status": "modified",
                                    "additions": 5,
                                    "deletions": 0,
                                    "changes": 5,
                                    "blob_url": "https://github.com/google/jax/blob/4679f455f976c823851312438d766f8ffecac0e8/CHANGELOG.md",
                                    "raw_url": "https://github.com/google/jax/raw/4679f455f976c823851312438d766f8ffecac0e8/CHANGELOG.md",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/CHANGELOG.md?ref=4679f455f976c823851312438d766f8ffecac0e8",
                                    "patch": "@@ -13,6 +13,11 @@ PLEASE REMEMBER TO CHANGE THE '..main' WITH AN ACTUAL TAG in GITHUB LINK.\n ## jax 0.2.26 (Unreleased)\n * [GitHub\n   commits](https://github.com/google/jax/compare/jax-v0.2.25...main).\n+* Bug fixes:\n+  * Out-of-bounds indices to `jax.ops.segment_sum` will now be handled with\n+    `FILL_OR_DROP` semantics, as documented. This primarily afects the\n+    reverse-mode derivative, where gradients corresponding to out-of-bounds\n+    indices will now be returned as 0. (#8634).\n \n ## jaxlib 0.1.74 (Nov 17, 2021)\n * Enabled peer-to-peer copies between GPUs. Previously, GPU copies were bounced via"
                                },
                                {
                                    "sha": "e8ec92d0a70ddbb58d7876a49178cf9e35099ccf",
                                    "filename": "jax/_src/ops/scatter.py",
                                    "status": "modified",
                                    "additions": 37,
                                    "deletions": 16,
                                    "changes": 53,
                                    "blob_url": "https://github.com/google/jax/blob/4679f455f976c823851312438d766f8ffecac0e8/jax%2F_src%2Fops%2Fscatter.py",
                                    "raw_url": "https://github.com/google/jax/raw/4679f455f976c823851312438d766f8ffecac0e8/jax%2F_src%2Fops%2Fscatter.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/jax%2F_src%2Fops%2Fscatter.py?ref=4679f455f976c823851312438d766f8ffecac0e8",
                                    "patch": "@@ -412,8 +412,10 @@ def _segment_update(name: str,\n                     indices_are_sorted: bool = False,\n                     unique_indices: bool = False,\n                     bucket_size: Optional[int] = None,\n-                    reducer: Optional[Callable] = None) -> Array:\n+                    reducer: Optional[Callable] = None,\n+                    mode: Optional[lax.GatherScatterMode] = None) -> Array:\n   jnp._check_arraylike(name, data, segment_ids)\n+  mode = lax.GatherScatterMode.FILL_OR_DROP if mode is None else mode\n   data = jnp.asarray(data)\n   segment_ids = jnp.asarray(segment_ids)\n   dtype = data.dtype\n@@ -430,7 +432,7 @@ def _segment_update(name: str,\n   if num_buckets == 1:\n     return _scatter_update(\n       out, segment_ids, data, scatter_op, indices_are_sorted,\n-      unique_indices, normalize_indices=False)\n+      unique_indices, normalize_indices=False, mode=mode)\n \n   # Bucketize indices and perform segment_update on each bucket to improve\n   # numerical stability for operations like product and sum.\n@@ -450,7 +452,8 @@ def segment_sum(data: Array,\n                 num_segments: Optional[int] = None,\n                 indices_are_sorted: bool = False,\n                 unique_indices: bool = False,\n-                bucket_size: Optional[int] = None) -> Array:\n+                bucket_size: Optional[int] = None,\n+                mode: Optional[lax.GatherScatterMode] = None) -> Array:\n   \"\"\"Computes the sum within segments of an array.\n \n   Similar to TensorFlow's `segment_sum\n@@ -460,8 +463,7 @@ def segment_sum(data: Array,\n     data: an array with the values to be summed.\n     segment_ids: an array with integer dtype that indicates the segments of\n       `data` (along its leading axis) to be summed. Values can be repeated and\n-      need not be sorted. Values outside of the range [0, num_segments) are\n-      dropped and do not contribute to the sum.\n+      need not be sorted.\n     num_segments: optional, an int with nonnegative value indicating the number\n       of segments. The default is set to be the minimum number of segments that\n       would support all indices in ``segment_ids``, calculated as\n@@ -473,6 +475,9 @@ def segment_sum(data: Array,\n     bucket_size: size of bucket to group indices into. ``segment_sum`` is\n       performed on each bucket separately to improve numerical stability of\n       addition. Default ``None`` means no bucketing.\n+    mode: a :class:`jax.lax.GatherScatterMode` value describing how\n+      out-of-bounds indices should be handled. By default, values outside of the\n+      range [0, num_segments) are dropped and do not contribute to the sum.\n \n   Returns:\n     An array with shape :code:`(num_segments,) + data.shape[1:]` representing the\n@@ -492,16 +497,18 @@ def segment_sum(data: Array,\n     >>> jit(segment_sum, static_argnums=2)(data, segment_ids, 3)\n     DeviceArray([1, 5, 4], dtype=int32)\n   \"\"\"\n-  return _segment_update(\"segment_sum\", data, segment_ids, lax.scatter_add, num_segments,\n-                         indices_are_sorted, unique_indices, bucket_size, jnp.sum)\n+  return _segment_update(\n+      \"segment_sum\", data, segment_ids, lax.scatter_add, num_segments,\n+      indices_are_sorted, unique_indices, bucket_size, jnp.sum, mode=mode)\n \n \n def segment_prod(data: Array,\n                  segment_ids: Array,\n                  num_segments: Optional[int] = None,\n                  indices_are_sorted: bool = False,\n                  unique_indices: bool = False,\n-                 bucket_size: Optional[int] = None) -> Array:\n+                 bucket_size: Optional[int] = None,\n+                 mode: Optional[lax.GatherScatterMode] = None) -> Array:\n   \"\"\"Computes the product within segments of an array.\n \n   Similar to TensorFlow's `segment_prod\n@@ -524,6 +531,9 @@ def segment_prod(data: Array,\n     bucket_size: size of bucket to group indices into. ``segment_prod`` is\n       performed on each bucket separately to improve numerical stability of\n       addition. Default ``None`` means no bucketing.\n+    mode: a :class:`jax.lax.GatherScatterMode` value describing how\n+      out-of-bounds indices should be handled. By default, values outside of the\n+      range [0, num_segments) are dropped and do not contribute to the sum.\n \n   Returns:\n     An array with shape :code:`(num_segments,) + data.shape[1:]` representing the\n@@ -543,16 +553,18 @@ def segment_prod(data: Array,\n     >>> jit(segment_prod, static_argnums=2)(data, segment_ids, 3)\n     DeviceArray([ 0,  6, 20], dtype=int32)\n   \"\"\"\n-  return _segment_update(\"segment_prod\", data, segment_ids, lax.scatter_mul, num_segments,\n-                         indices_are_sorted, unique_indices, bucket_size, jnp.prod)\n+  return _segment_update(\n+      \"segment_prod\", data, segment_ids, lax.scatter_mul, num_segments,\n+      indices_are_sorted, unique_indices, bucket_size, jnp.prod, mode=mode)\n \n \n def segment_max(data: Array,\n                 segment_ids: Array,\n                 num_segments: Optional[int] = None,\n                 indices_are_sorted: bool = False,\n                 unique_indices: bool = False,\n-                bucket_size: Optional[int] = None) -> Array:\n+                bucket_size: Optional[int] = None,\n+                mode: Optional[lax.GatherScatterMode] = None) -> Array:\n   \"\"\"Computes the maximum within segments of an array.\n \n   Similar to TensorFlow's `segment_max\n@@ -574,6 +586,9 @@ def segment_max(data: Array,\n     unique_indices: whether `segment_ids` is known to be free of duplicates.\n     bucket_size: size of bucket to group indices into. ``segment_max`` is\n       performed on each bucket separately. Default ``None`` means no bucketing.\n+    mode: a :class:`jax.lax.GatherScatterMode` value describing how\n+      out-of-bounds indices should be handled. By default, values outside of the\n+      range [0, num_segments) are dropped and do not contribute to the sum.\n \n   Returns:\n     An array with shape :code:`(num_segments,) + data.shape[1:]` representing the\n@@ -593,16 +608,18 @@ def segment_max(data: Array,\n     >>> jit(segment_max, static_argnums=2)(data, segment_ids, 3)\n     DeviceArray([1, 3, 5], dtype=int32)\n   \"\"\"\n-  return _segment_update(\"segment_max\", data, segment_ids, lax.scatter_max, num_segments,\n-                         indices_are_sorted, unique_indices, bucket_size, jnp.max)\n+  return _segment_update(\n+      \"segment_max\", data, segment_ids, lax.scatter_max, num_segments,\n+      indices_are_sorted, unique_indices, bucket_size, jnp.max, mode=mode)\n \n \n def segment_min(data: Array,\n                 segment_ids: Array,\n                 num_segments: Optional[int] = None,\n                 indices_are_sorted: bool = False,\n                 unique_indices: bool = False,\n-                bucket_size: Optional[int] = None) -> Array:\n+                bucket_size: Optional[int] = None,\n+                mode: Optional[lax.GatherScatterMode] = None) -> Array:\n   \"\"\"Computes the minimum within segments of an array.\n \n   Similar to TensorFlow's `segment_min\n@@ -624,6 +641,9 @@ def segment_min(data: Array,\n     unique_indices: whether `segment_ids` is known to be free of duplicates.\n     bucket_size: size of bucket to group indices into. ``segment_min`` is\n       performed on each bucket separately. Default ``None`` means no bucketing.\n+    mode: a :class:`jax.lax.GatherScatterMode` value describing how\n+      out-of-bounds indices should be handled. By default, values outside of the\n+      range [0, num_segments) are dropped and do not contribute to the sum.\n \n   Returns:\n     An array with shape :code:`(num_segments,) + data.shape[1:]` representing the\n@@ -643,5 +663,6 @@ def segment_min(data: Array,\n     >>> jit(segment_min, static_argnums=2)(data, segment_ids, 3)\n     DeviceArray([0, 2, 4], dtype=int32)\n   \"\"\"\n-  return _segment_update(\"segment_min\", data, segment_ids, lax.scatter_min, num_segments,\n-                         indices_are_sorted, unique_indices, bucket_size, jnp.min)\n+  return _segment_update(\n+      \"segment_min\", data, segment_ids, lax.scatter_min, num_segments,\n+      indices_are_sorted, unique_indices, bucket_size, jnp.min, mode=mode)"
                                }
                            ],
                            "tests": [
                                {
                                    "sha": "9d34008d7ecafff830bcf8802a0e0e60c91ee1f9",
                                    "filename": "tests/lax_numpy_indexing_test.py",
                                    "status": "modified",
                                    "additions": 11,
                                    "deletions": 0,
                                    "changes": 11,
                                    "blob_url": "https://github.com/google/jax/blob/4679f455f976c823851312438d766f8ffecac0e8/tests%2Flax_numpy_indexing_test.py",
                                    "raw_url": "https://github.com/google/jax/raw/4679f455f976c823851312438d766f8ffecac0e8/tests%2Flax_numpy_indexing_test.py",
                                    "contents_url": "https://api.github.com/repos/google/jax/contents/tests%2Flax_numpy_indexing_test.py?ref=4679f455f976c823851312438d766f8ffecac0e8",
                                    "patch": "@@ -1155,6 +1155,17 @@ def testSegmentSum(self):\n     expected = jnp.array([0, 0, 0, 13, 2, 7])\n     self.assertAllClose(ans, expected, check_dtypes=False)\n \n+  def testSegmentSumOutOfBounds(self):\n+    def fn(data, segment_ids):\n+      return jax.ops.segment_sum(data, segment_ids, num_segments).sum()\n+\n+    data = np.array([0, 0], dtype=np.float32)\n+    num_segments = 2\n+    segment_ids = np.array([2, 3])\n+    val, grad = jax.value_and_grad(fn)(data, segment_ids)\n+    self.assertAllClose(val, np.array(0., np.float32))\n+    self.assertAllClose(grad, np.array([0., 0.], np.float32))\n+\n \n   @parameterized.named_parameters(itertools.chain.from_iterable(\n       jtu.cases_from_list({"
                                }
                            ]
                        }
                    }
                ],
                "manuallyChecked": true,
                "testSteps": "pipenv run python -m pytest tests/lax_numpy_indexing_test.py"
            }
        ],
        "installSteps": "pipenv --python 3.10\npipenv run pip install numpy wheel build\npipenv run python3 build/build.py\npipenv run pip install dist/*.whl\npipenv run pip install -e .\npipenv run pip install -r build/test-requirements.txt\nrm dist/*"
    }
]